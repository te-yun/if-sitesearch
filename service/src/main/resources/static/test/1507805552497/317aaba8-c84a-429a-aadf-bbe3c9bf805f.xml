<?xml version='1.0' encoding='UTF-8' ?>
<!--
  ~ Copyright 2017 IntraFind Software AG. All rights reserved.
  ~
  ~ Licensed under the Apache License, Version 2.0 (the "License");
  ~ you may not use this file except in compliance with the License.
  ~ You may obtain a copy of the License at
  ~
  ~     http://www.apache.org/licenses/LICENSE-2.0
  ~
  ~ Unless required by applicable law or agreed to in writing, software
  ~ distributed under the License is distributed on an "AS IS" BASIS,
  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  ~ See the License for the specific language governing permissions and
  ~ limitations under the License.
  -->

<documents count="50">
    <Document>
        <url>http://www.intrafind.de/unternehmen/glossar/Crawler</url>
        <id>DEC089BE34324C06E374E2549B11340F</id>
        <title>Crawler</title>
        <body>Crawler   Die Indexing Engine oder der Crawler beschafft die Daten und Dokumente aus den Datenquellen und
            führt sie anschließend  in einer Struktur entsprechend auf, die wiederum die Suche vereinfacht. Gleichzeitig
            erstellt die Crawling Engine Caches der Dokumente, damit eine Dokumentvorschau dargestellt werden kann.
            Dieser Index wird von der Query Engine durchlaufen, um die Treffer aufzulisten.  
        </body>
    </Document>
    <Document>
        <url>http://www.intrafind.de/blog</url>
        <id>BA441E1FCAE926D5999C6F90C77F3CA9</id>
        <title>Enterprise Search Blog</title>
        <body>ENTERPRISE SEARCH BLOG Auf dieser Seite finden Sie aktuelle Blogbeiträge der IntraFind Software AG.
            Erfahren Sie von unseren Experten Wissenswertes zu den Themen Enterprise Search, Tagging und Content
            Analytics.
        </body>
    </Document>
    <Document>
        <url>http://www.intrafind.de/news_en</url>
        <id>1C7264AF8FDE0D3C14BCC6EBA2CA5C36</id>
        <title>News of IntraFind Software AG</title>
        <body>IntraFind News</body>
    </Document>
    <Document>
        <url>http://www.intrafind.de/unternehmen/glossar/e-commerce</url>
        <id>EB6DB7984002AFACFAD6B3199128625F</id>
        <title>E-Commerce</title>
        <body>E-Commerce   E-Commerce = elektronischer Geschäftsverkehr, elektronischer Handel.   Geschäftsprozesse wie
            der Verkauf von Waren oder Dienstleistungen werden zunehmend automatisiert über elektronische
            Handelsplattformen (z.B. Online Shops oder Portale) im Internet abgewickelt. Eine leistungsfähige Suche,
            d.h. die Möglichkeit für Portalbesucher schnell und einfach die gewünschten Artikel und Inhalte auf dem
            Portal zu finden, kann sich entscheidend auf deren Verweildauer bzw. auf generierte Umsätze auswirken.  
            IntraFind bietet für Betreiber von E-Commerce-Plattformen umfassende Unterstützung bei der qualitativen
            Optimierung der Shop-Suche wie verbesserte Sprachfeatures durch den Einsatz von Textanalyse, geführte Suche
            und Auswertungsmöglichkeiten durch die Verwendung der Wissenslandkarte oder Hilfe bei der Datenerfassung und
            -veredelung. Mehr über die Branchenlösung E-Commerce zurück  
        </body>
    </Document>
    <Document>
        <url>
            http://www.intrafind.de/blog/schwierige-verwandtschaft-wie-suchmaschinen-mithilfe-von-wortfamilien-erweitert-werden-koennen
        </url>
        <id>114E1531F5C1D0193B9E6D292AB7A712</id>
        <title>Schwierige Verwandtschaft – wie Suchmaschinen mithilfe von Wortfamilien erweitert werden können</title>
        <body>Schwierige Verwandtschaft – wie Suchmaschinen mithilfe von Wortfamilien erweitert werden können Über den
            Mehrwert eines Stem-Thesaurus für optimale Suche und Recherche Was muss eine gute Suchmaschine können? Den
            Mindestanspruch erfüllen jene Suchfunktionen, die in einem Textverarbeitungsprogramm oder einem Editor
            üblicherweise mit dem Shortcut STRG F aufgerufen werden: Man tippt eine Buchstabenfolge ein und das
            geöffnete Dokument wird auf alle Vorkommen genau dieser Kombination durchsucht. Auf diese Art und Weise
            lassen sich eindeutig bestimmte Textstellen innerhalb eines Dokuments schnell und einfach finden.   Wörter
            bestehen allerdings nicht aus immer gleichen Buchstabenfolgen, sondern sind dynamische Gebilde: sie werden
            je nach grammatischer Person, Zeit, Kasus oder Geschlecht flektiert. Einem Nutzer, der sich umfangreicher
            über ein bestimmtes Thema oder über die Verwendung eines bestimmten Wortes informieren möchte, wäre eine
            solche Suchfunktion deshalb sicherlich zu wenig.   Ein Weg zur Erweiterung der Suche ist demzufolge die
            Lemmatisierung: ein Feature, das neben der Kompositazerlegung den Kern des Linguistik Plugins von IntraFind
            ausmacht und das auf dem firmeneigenem Vollformenlexikon basiert. So ist z.B. die Buchstabenfolge “dachtest”
            im Lexikon ihrer Grundform “denken” zugeordnet, ebenso wie alle anderen möglichen Formen von “denken”. Auf
            diese Weise kann bei einer Suche nach einer dieser Formen die gesamte Bandbreite gebeugter Wortvarianten
            (Flexionsparadigma) einbezogen werden.   Wörter sind dynamische Gebilde – nicht nur, weil sie flektiert
            werden können, sondern auch, weil sie durch sogenannte Wortbildungsprozesse zu neuen Wörtern zusammengeführt
            werden können. Zu diesen Wortneubildungen stehen sie dann in einer Art Verwandtschaftsverhältnis. Diese
            Verwandtschaftsbeziehung kann zwischen Wörtern aus ein und derselben Wortart bestehen, wie z.B. zwischen den
            Substantiven “Chemie” und “Chemiker” oder zwischen Wörtern verschiedener Wortarten wie “Chemie” und
            “chemisch” oder “kaufen” und “Käufer”.   Eine Gruppe von Wörtern, die in einer solchen Beziehung zueinander
            stehen, nennt man eine Wortfamilie. In vielen Situationen kann es nützlich sein, bei einer Suche nicht nur
            das Flexionsparadigma, sondern auch die Wortfamilie miteinzuschließen; denn wer nach “Chemie” sucht, für den
            könnten auch Suchergebnisse interessant sein, die “Chemiker”, “Chemikalie” und “chemisch” enthalten. Zu
            diesem Zweck entwickelte IntraFind einen Stem-Thesaurus, in dem die Verwandtschaftsrelation zwischen
            Mitgliedern einer Wortfamilie hinterlegt ist.   Der Thesaurus ist ein Stem-Thesaurus, weil das Kriterium für
            die Verwandtschaft ein gemeinsamer Wortstamm ist: ein linguistisch umstrittener Begriff, der sich aber in
            der Anwendung als derjenige Teil des Wortes definieren lässt, der einer Wortfamilie gemeinsam ist und aus
            dem durch Anhängen von Prä- oder Suffixen die einzelnen Wörter der Wortfamilie entstehen. Der Wortstamm für
            unsere “Chemie”-Familie wäre also “chem-”, woran dann die Endungen “-isch”, “-ie” und “-iker” gehängt werden
            können.   Die Untersuchung von Wortbildungsprozessen zeigt, dass es gewisse Regelmäßigkeiten bei der
            Ableitung von Wörtern aus dem Wortstamm gibt: z.B. entstehen Infinitive fast immer durch ein Anhängen von
            “-en” oder “-n”, Nomen sehr oft durch das Suffix “-er” oder “-ler”. Die Stämme “spiel-” und “wander-” werden
            im Infinitiv zu “spiel-en” und “wander-n”, als Nomen zu “Spiel-er” und “Wander-er”.   Regeln dieser Art
            lassen sich so implementieren, dass ein Teil des Thesaurus automatisch erstellt werden kann. Allerdings wird
            auch deutlich, dass es in der Sprache zu viele Ausnahmen gibt, als dass man sie einfach mit ein paar
            zusätzlichen Ausnahmeregeln korrigieren könnte. Während der “Spieler” einfach jemand ist, der spielt, ist
            die “Leber” nicht eine Person, die lebt.   Besonders schwierig wird die Verwandtschaft bei Vorsilben, welche
            die Bedeutung des Stammes sehr stark verändern, auch wenn sie durchaus noch mit ihm verwandt sind:
            “unterschreiben” hat schon etwas mit “schreiben” zu tun, aber es ist doch etwas anderes und sollte bei einer
            Suchanfrage nicht damit vermischt werden.   Wegen all dieser Ausnahmen ist es sinnvoll, Wortfamilien in
            einem Stem-Thesaurus zu speichern, der zwar mit automatischer Unterstützung, aber auch mit halbautomatisch
            erstellten Ausnahmelisten und menschlicher Endkontrolle erstellt wird. Der fertige Thesaurus enthält
            schließlich nur diejenigen Wortfamilien, innerhalb derer eine hinreichende semantische Verwandtschaft
            besteht, um sie als zusammengehörig anzusehen. Er steht dann als weitgehend feststehende, aber erweiterbare
            Ressource zur Verfügung, um das Spektrum von Suchfunktionen um die jeweilige Wortfamilie zu erweitern.  
            Solche halb-automatisch erzeugten, händisch kuratierten Stem-Thesauri sind wertvolle Ressourcen für die
            explorative Suche (siehe IntraFind Blogbeitrag &quot;Tagging - Mehrwerte durch Metadaten&quot;). Sie
            ermöglichen im Zusammenspiel mit den weiteren Features des iFinder5 elastic eine deutliche Erweiterung der
            Such- und Recherchemöglichkeiten des Benutzers.   Der Autor Pascal Zambito, B.Sc., Absolvent der
            Computerliguistik an der Ludwig-Maximilians-Universität München (LMU), hat im Rahmen seiner Bachelorarbeit
            im Auftrag der IntraFind Software AG einen Stem-Thesaurus für die deutsche Sprache entwickelt.
        </body>
    </Document>
    <Document>
        <url>
            http://www.intrafind.de/blog/10-jahre-automatische-textklassifikation-mit-topicfinder-ein-erfahrungsbericht-aus-der-praxis-374
        </url>
        <id>BAA7DB058FD1357DA5D880C742D348F9</id>
        <title>10 Jahre automatische Textklassifikation mit TopicFinder. Ein Erfahrungsbericht aus der Praxis.</title>
        <body>10 Jahre automatische Textklassifikation mit TopicFinder. Ein Erfahrungsbericht aus der Praxis.   Kürzlich
            stellte ich fest, dass 10 Jahre vergangen sind seit dem ersten Release des TopicFinders, unseres Produkts
            für die automatische Textklassifikation. Seitdem führten wir mit dem TopicFinder zahlreiche Kundenprojekte
            durch und die Weiterentwicklung des Produkts wurde maßgeblich durch die Praxiserfahrung getrieben. Ich
            denke, wir können uns durchaus zu Recht als Pioniere der Anwendung automatischer Textklassifikation
            bezeichnen. Das Jubiläum nehme ich zum Anlass für einen kleinen Blog-Beitrag.   Zunächst zur
            Begriffsklärung: Unter automatischer Textklassifikation versteht man die automatische Zuordnung von
            Dokumenten zu vordefinierten Themen auf Basis des Dokumenteninhalts. Ziel der Automatisierung ist fast nie,
            menschliche Experten zu ersetzen. Eher geht es darum, die großen Datenmengen, die durch menschliche Experten
            nicht mehr handhabbar sind, durch automatische Filterung in den Griff zu bekommen.   Das Anwendungsspektrum
            reicht von fachlich anspruchsvoller Klassifikation von juristischen Dokumenten (z.B. Gerichtsurteilen) oder
            Patenten (Internationale Patentklassifikation nach IPC oder ECLA) über die Filterung von Nachrichtentexten
            bis hin zur Produktklassifikation (z.B. nach Produktkatalogen ECLASS oder UNSPSC) oder zur
            Formularerkennung.   Grundlage für die Zuordnung von Dokumenten zu Themen bilden Klassifikationsregeln, die
            meist ein sehr ähnliches Format wie Queries in der Volltextsuche haben. Jedoch sind die
            Klassifikationsregeln für jedes Thema fest hinterlegt und werden eher selten geändert. Außerdem sind sie in
            der Regel sehr viel komplexer als normale Queries, um das jeweilige Thema möglichst umfassend abzudecken und
            trotzdem scharf genug von anderen Themen abzugrenzen.   Klassifikationsregeln manuell zu erstellen und zu
            pflegen ist sehr aufwändig. Deshalb setzten wir von Anfang an auf statistische Verfahren, die
            Klassifikationsregeln automatisch auf Basis von Beispieldokumenten (Trainingsdokumenten) für die jeweiligen
            Themen generieren. Statistische Verfahren sind objektiver als menschliche Experten, wodurch auch die
            Klassifikationsqualität steigt. Außerdem können die mit statistischen Verfahren erzeugten Regeln sehr viel
            komplexer werden, als von Menschen erstellte Regeln. Sie umfassen oft mehrere hundert Einzel- und
            Mehrwortbegriffe.     Was zeichnet nun den TopicFinder gegenüber anderen Produkten für die automatische
            Textklassifikation aus? Was haben wir aus der Praxis gelernt?   Der TopicFinder arbeitet nicht
            ausschließlich auf statistischer Basis. Als Vorverarbeitungsschritt setzen wir die aus unseren Enterprise
            Search-Produkten bekannten morphologischen Analyzer zur Normalisierung von Wörtern ein und wir nutzen
            Wortkategorien zur Erzeugung von Mehrwortbegriffen (Nominalphrasen). Durch diese Vorverarbeitung versorgen
            wir die nachfolgenden statistischen Verfahren mit zusätzlichem Wissen. Die Klassifikationsqualität steigt
            bzw. es werden weniger Beispieldokumente zum Lernen benötigt. Im TopicFinder verwenden wir Support Vektor
            Maschinen, die besten verfügbaren statistischen Verfahren. Durch Kreuzvalidierungsläufe wird vollautomatisch
            die Modellkomplexität optimiert. Gemäß Occams Rasiermesser generieren wir möglichst einfache Modell- /
            Klassifikationsregeln. Die Erfahrung zeigt, dass man der manuellen Klassifikation durch Experten nicht blind
            vertrauen darf. Die Beispieldaten, die man vom Kunden zum Training erhält, enthalten in den meisten Fällen
            auch falsche Themenzuordnungen. Sehr oft werden passende Themenzuordnungen für Beispieldokumente übersehen.
            Selbst wenn viel Wert auf eine hochwertige, konsistente Klassifikation gelegt wurde und die
            Beispieldokumente aus seit Jahren gepflegten Taxonomien stammen, so ist doch mindestens mit einer Fehlerrate
            von 5% zu rechnen. Der TopicFinder kann automatisch inkonsistente Themenzuordnungen in Beispieldaten
            entdecken, z.B. wenn sehr ähnliche Dokumente unterschiedliche Themenzuordnungen aufweisen. Auch den
            automatisch erzeugten Klassifikationsregeln darf man nicht blind vertrauen. Der TopicFinder bietet
            Unterstützung beim Testen der automatisch generierten Regeln, z.B. durch automatische Kreuzvalidierung. Ein
            Alleinstellungsmerkmal ist, dass die automatisch generierten Klassifikationsregeln lesbar sind und somit
            ihre Plausibilität durch Experten überprüft werden kann. Klassifikation mit dem TopicFinder ist also keine
            Blackbox. Mittels farbigem Highlighting machen wir auch für einzelne Dokumente sichtbar, warum sie einem
            Thema zugeordnet werden. TopicFinder next generation - wohin geht die Reise?   Dieses Jahr werden wir die
            Administrations- und Trainingsoberfläche des TopicFinder komplett überarbeiten. Der Import von
            Trainingsdaten wird flexibler durch Skripte steuerbar werden. Den Aufwand für die Erzeugung von manuell
            klassifizierten Trainingsdaten wollen wir durch die Einführung einer Active Learning-Komponente deutlich
            reduzieren und wir wollen mehr Unterstützung für Continuous Learning (automatische Verbesserung der
            Klassifikation durch Nutzung von User-Feedback) bieten.   Der Autor Dr. Christoph Goller verfügt über 15
            Jahre Erfahrung im Enterprise Search Markt. Er promovierte in Computerwissenschaften an der Technischen
            Universität in München und arbeitete im Anschluss in einigen wissenschaftlichen Projekten zu Themenbereich
            Künstliche Intelligenz, Machine Learning sowie Neuronale Netze.   Seit 2002 leitet er den Bereich Forschung
            bei IntraFind und verantwortet IntraFind&apos;s Kern-Suche- und Content Analyse Technologien. Zwischen 2003
            und 2007 arbeitete Christoph aktiv als Committer in Apache&apos;s Open Source Projekt Lucene.    
        </body>
    </Document>
    <Document>
        <url>http://www.intrafind.de/company/glossary/e-commerce</url>
        <id>2BFF0187B668958F0D0E398A99AC2B9B</id>
        <title>E-Commerce</title>
        <body>E-Commerce   Business processes such as the sale of goods or services are increasingly automated through
            electronic trading platforms (e.g. online shops or portals) conducted on the Internet. A powerful search,
            that is the opportunity for visitors to quickly and easily find their favorite items and content on the
            site, may have a significant impact on the length of their stay or on the generated volume of sales.    
            IntraFind offers comprehensive support for the qualitative optimization of shop search. This includes
            improved language features through the use of text analysis and guided search as well as analysis
            capabilities by using the knowledge map, or assistance with data collection and refinement. Get to know more
            about the industry solution E-commerce.  
        </body>
    </Document>
    <Document>
        <url>http://www.intrafind.de/blog/counting-counts-arguments-for-using-statistics-to-process-language-en</url>
        <id>34380822588A313E4E07748B90498261</id>
        <title>Counting counts – arguments for using statistics to process language</title>
        <body>Counting counts – arguments for using statistics to process language In my early student years I had a
            casual conversation with a computer science researcher. The researcher worked on home care robotics for the
            elderly. I got interested and wanted to know more about it. The objective was to have a robot learn how to
            cook, clean a toilet and wash the dishes. All by itself. And: &quot;at some point a machine will be able to
            understand text and learn stuff from the internet, so we concentrate on what happens after that&quot;. This
            struck me as a very bold assumption. Here is an analogy: thinking about what kind of special stretching
            exercises I should do once I beat Usain Bolt on the 100m sprint. I&apos;m not arguing that it&apos;s
            impossible (you don&apos;t know me!), and I won&apos;t deny the possibility of language understanding
            becoming a fact, but it was the hardest part in that system. And it got my attention.   Very soon I got my
            hands on a book on precisely this topic called &quot;Foundations of Statistical Natural Language Processing&quot;
            by Christopher Manning and Hinrich Schütze. It is a remarkably well written book, by authors that don&apos;t
            shy away from looking beyond the border of their field. The introductory chapter puts the approach of a
            quantitative, non-symbolic and non-logical Natural Language Processing (the authors&apos; definition of
            statistical NLP) into its historic and language-philosophical context.    In this post I want to revisit
            some aspects of this introduction and go a little deeper into what is only one short section on page 17,
            namely Ludwig Wittgenstein&apos;s argument of &quot;meaning is use&quot; (Philosophical Investigations,
            1953), and how it can be seen as a philosophical justification for statistical NLP, including machine
            learning. Empirical arguments – the statistical modelling of language works The successful use of
            statistical models in language is naturally a killer argument for using these methods. Naive Bayes
            classifiers, Latent Dirichlet Allocation models, Support Vector Machines, Deep Neural Nets, Hidden Markov
            Models, have all achieved remarkable results on several NLP tasks in the last years. But how did this all
            begin?   The American linguist George K. Zipf was one of the first (early 20th century) to study statistical
            properties of language. He found out, that if one orders the words of a language according to their
            frequency – giving the most frequent word the rank 1, the second rank 2, etc. – the product of frequency *
            rank will remain approximately constant for all words. In other words, the most common word occurs
            approximately twice as often as the second most common word, and three times as often as the third most
            frequent word, and so on. This remarkable property of natural languages is called Zipf&apos;s Law. Several
            well-known best practices in statistical NLP go back to this discovery, e.g. ignoring words in stopword
            lists. These are lists of the most common words in a corpus that occur that often that they don&apos;t carry
            any statistical importance for a particular task. Zipf also discovered that there is a negative correlation
            between a word&apos;s length and its frequency as well as a positive correlation between age and frequency.
              These simple, yet remarkable findings show that there is something intrinsically statistical in natural
            languages. It&apos;s not clear what explains these phenomena – maybe the principle of least effort that Zipf
            believes is a key characteristic of humans; maybe it&apos;s a greater mathematical law governing many things
            including language. But we shouldn&apos;t look for causality; it suffices to accept that these properties
            are measurable.   1948 an engineering breakthrough happened in the area of communication – the development
            of Information Theory by Claude Shannon. Shannon worked at the famous Bell Labs on communication systems and
            devised a mathematical theory of communication on noisy channels. He showed that regardless of the noise in
            a communication channel, it is possible to communicate error-free through it. This works up to a maximum
            bandwidth, defined by the actual bandwidth and the noise. Information Theory provides the pillars for all
            modern communication systems and models communication as a stochastic process (a statistical model). Key
            concepts in Information Theory as &quot;Entropy&quot; or &quot;Mutual Information&quot; have been found to
            relate closely to and be useful in the processing of natural languages. Information Theory provides, for
            example, the theoretical lower limit achievable by compressing a text in a particular language. The
            language-philosophical argument – Wittgenstein and “meaning is use” All these empirical proofs showing that
            the statistical modeling of language is actually not a farfetched idea give us more or less quantitative
            assurance that these methods have their entitlement to exist. Wittgenstein provides us with what could be
            called a qualitative argument that is based on how he understands the inner workings of natural languages.  
            Wittgenstein argues that for a very large class of words there are no strictly definable meanings, in a
            mathematical sense. The question &quot;what is the meaning of areté?&quot;, for example, is a question one
            cannot answer in a satisfying way (ask Meno! [https://en.wikipedia.org/wiki/Meno]). This argument seems
            strange at first, since we&apos;re used to asking precisely this question – even more so when learning a new
            language. Wittgenstein claims that the meaning of a word is given by its use in the language.      From §43:
            &quot;Man kann für eine große Klasse von Fällen der Benützung des Wortes &quot;Bedeutung&quot; – wenn auch
            nicht für alle Fälle seiner Benützung – dieses Wort so erklären: Die Bedeutung eines Wortes ist sein
            Gebrauch in der Sprache.&quot;   Take as an example the German word &quot;Spiel&quot; (§66-§71), which is a
            noun very close to the English &quot;game&quot;, but is used to describe also leisure activities small
            children do, as playing with a ball, or playing catch, etc. If one is asked what the meaning of &quot;Spiel&quot;
            is, one idea could be to try and find one single common feature permeating all kinds of &quot;Spiel&quot;.
            But what would the single common feature be between a professional game of chess and the kicking of a ball
            against a garage door? Are they all entertaining? Think about the game of Russian roulette! Oh, maybe all
            &quot;Spiele&quot; have a winner and a loser? Who wins and who loses in a game of solitaire? Maybe it&apos;s
            all about agility? But compare the agility necessary in chess with the one needed in soccer; and what
            agility do you need for a game of Bingo? One will quickly realize that all &quot;Spiele&quot; are part of an
            incredibly complex web of relations, but I defy anyone to find one single common character shared by all
            &quot;Spiele&quot;.   Wittgenstein calls this &quot;Familienähnlichkeiten&quot; (family resemblances). As
            members of a family share several characteristics, e.g. expressions, height, eye color, or temper, but never
            all of them at once, words in a family also share several features. Another very nice analogy is that of a
            thread. The thread is composed of thousands of fibers that interleave, some touching each other, most not
            touching each other at all, being very distant from another – but together composing one single object we
            call a thread. &quot;Spiel&quot; is one such thread, composed of many fibers (instances of a &quot;Spiel&quot;),
            some sharing portions of what makes them a &quot;Spiel&quot;, some not.   The only way to explain the &quot;meaning&quot;
            of &quot;Spiel&quot; is to actually enumerate all known uses of &quot;Spiel&quot; in the language. One could
            say that &quot;Spiel&quot; has a fuzzy definition – but does this make the word useless? I believe not. I
            even do believe that this fuzziness is more informative and meaningful than if there was a clear definition
            of &quot;Spiel&quot;.   Of course, this exercise can be expanded to several other words. The first time I
            read these aphorisms, they resonated with my experiences and I was very quickly very much convinced by them
            (&quot;Yes! This is how I understand language!&quot;). The beauty of this idea is that it provides a very
            convincing philosophical theory that justifies the approach of collecting examples of text (corpora) and
            trying to learn patterns from them, which is the way statistical NLP and machine learning work. Furthermore,
            Wittgenstein&apos;s argument of &quot;meaning is use&quot; implies that there is no other way to understand
            a large part of language; this is one fundamental nature of language, and this is how humans understand and
            use it.   And this is a damned good argument for doing our job the way we do it.   Der Autor Breno Faria,
            Head of Development, ist seit 2012 für die IntraFind Software AG tätig. Seit den späten 2000er Jahren
            beschäftigt er sich intensiv mit den Themen Content Analytics und Information Retrieval. 2015 übernahm er
            die Rolle des Entwicklungsleiters bei IntraFind.   Im Rahmen von Veranstaltungen, z.B. Berlin Buzzwords 2014
            oder &quot;IntraFind Enterprise Search Day 2015&quot;, referiert er regelmäßig über neue Technologien oder
            präsentiert innovative Lösungen aus IntraFind Kundenprojekten.
        </body>
    </Document>
    <Document>
        <url>http://www.intrafind.de/blog/suche-in-internet-extranet-und-intranet-drei-seiten-einer-medaille</url>
        <id>8F8FF07D55002428979140666E6167F9</id>
        <title>Suche in Internet, Extranet und Intranet. Drei Seiten einer Medaille.</title>
        <body>Suche in Internet, Extranet und Intranet. Drei Seiten einer Medaille.   Viele Unternehmen stehen bei der
            Einführung einer Suche vor der Herausforderung, verschiedene Bereiche in einer Suche zu vereinheitlichen:
            sowohl in organisatorischer (Standorte, Abteilungen, usw.) als auch in technischer Hinsicht (Datenquellen,
            Zugriffsrechte, etc.).   Daraus ergeben sich spezielle Anforderungen, die ich im Folgenden exemplarisch
            anhand der Suche in Internet, Extranet und Intranet etwas näher beleuchten werde. DATENQUELLEN Ohne Daten
            keine Suche: Die zu durchsuchenden Daten bilden die Grundlage für jede Suchlösung. Die erste Herausforderung
            besteht darin, die Daten so in das Suchsystem zu transferieren, dass diese später vernünftig durchsuchbar
            sind. Durchsuchbar meint in diesem Fall zum einen, dass die Inhalte vom Anwender gefunden werden können,
            aber auch dass die Inhalte zu einem erwarteten Zeitpunkt verfügbar sind.   Es gibt verschiedene Methoden,
            Inhalte aus Datenquellen wie Webseiten, CMS, DMS, File-System, SharePoint, Jive, Wikis, Datenbanken usw. in
            ein Suchsystem zu übernehmen.   Bei der Suche im Internet steht man häufig „nur“ vor der Aufgabe, ein System
            (CMS) durchsuchbar zu machen, und die darin gespeicherten Informationen sind in der Regel auch für alle
            Benutzer sichtbar.   Für eine Extranetsuche muss üblicherweise ebenfalls nur ein System berücksichtigt
            werden (CMS), aber hier gibt es bereits Benutzergruppen, die nur spezifische Inhalte sehen und natürlich
            auch nur diese durchsuchen dürfen.   Für die Suche im Intranet liegt meist die geballte Ladung an Varianten
            vor. Dort sind verschiedene Systeme, verschiedenste Dateiformate, granulare Berechtigungskonzepte und vieles
            mehr an der Tagesordnung.   CRAWLER   Die schnellste Art, aus Inhalten einen Suchindex zu erstellen, ist die
            Verwendung eines Crawlers. Dieser ist dem ersten Anschein nach prädestiniert für die Indizierung von
            Webseiten. Der Crawler ist auch die einzige Lösung, wenn man keinen Einfluss auf das zu durchsuchende System
            hat (z.B. die Webseite eines Mitbewerbers).   Allerdings ist ein Crawler die denkbar schlechteste Lösung, um
            Daten in einem Suchindex aktuell zu halten.   Der Crawler muss eine Webseite immer komplett durchforsten, um
            neue und geänderte Inhalte zu entdecken. Ein Crawler kann aufgrund der Beschaffenheit des Internets
            gelöschte Inhalte nicht direkt erkennen. Beim Prüfen einer bereits gecrawlten Seite erhält er vom jeweiligen
            Webserver nur die Meldung „Seite nicht gefunden“ zurück. Nun kann es aber sein, dass die Seite gar nicht
            gelöscht wurde, sondern aus technischen Gründen lediglich temporär nicht erreichbar ist. Daher muss ein
            Crawler eine nicht erreichbare (und eventuell gelöschte) Seite immer mehrmals prüfen, bevor diese auch als
            gelöscht erkannt und endgültig aus dem Suchergebnis entfernt wird.   Eine Suche für eine Webseite im
            Internet oder ein einfaches Extranet kann aber durchaus mit Hilfe eines Crawlers befüllt werden.  
            KONNEKTOREN   Konnektoren sind die erste Wahl, wenn Inhalte aus Quellsystemen extrahiert und zu einem Index
            aufgebaut werden sollen. Ein Konnektor läuft typischerweise innerhalb des Quellsystems und zapft dessen
            Workflows an.   Konnektoren kennen „ihr“ System, für das sie entwickelt wurden, und können daher die
            jeweiligen Features der Quellsysteme entsprechend einsetzen. Ein guter Konnektor erkennt Änderungen an
            seinem Quellsystem (sofern das Quellsystem dies zulässt) und kann direkt auf diese Änderung reagieren. Dies
            bedeutet, dass die Information über eine Neuanlage, eine Änderung oder eine Löschung direkt an das
            Suchsystem weitergeleitet werden kann. Somit ist der Suchindex beim Einsatz von Konnektoren im Gegensatz zum
            Crawling stets aktuell. SICHERHEIT Jeder Benutzer sollte im optimalen Fall nur die Dokumente in einem
            Suchergebnis sehen, die er auch sehen darf. Im einfachsten Fall dürfen alle Benutzer alle Dokumente sehen,
            suchen und finden.   Für einen Internetauftritt stellt sich die Frage nach der Sicherheit in der Regel
            nicht, da die Benutzer meist anonym sind und alle Inhalte auf der Webseite lesen dürfen.   Bei einem
            Extranet gibt es mindestens die Unterscheidung in öffentliche und nicht öffentliche Inhalte, es kann dort
            aber auch persönliche Inhalte geben.   Im Intranet sind Benutzerberechtigungen typischerweise am häufigsten
            und in den unterschiedlichsten Ausprägungen verbreitet.   Die Herausforderung für ein Suchsystem besteht
            darin, einem Benutzer genau die Dokumente anzuzeigen, die er auch sehen darf. Und das nicht nur bei der
            Verarbeitung der Suchanfrage, sondern auch bei der Ausführung von Funktionalitäten wie Autocomplete oder der
            Anwendung von Suchfiltern (Facetten)!   Early Binding   Um bei optimaler Performance die bestmögliche
            Treffermenge zu berechnen, ist ein so genanntes „Early Binding“ unerlässlich.   „Early Binding“ bedeutet,
            dass bereits bei der Indexierung die Berechtigungen der Dokumente mit im Suchsystem abgespeichert werden.
            Die Berechtigungen können dann zur Suchzeit sofort mit ausgewertet werden und garantieren somit eine sichere
            und schnelle Suche.   Late Binding   Unter „Late Binding“ versteht man das Prüfen und den Abgleich jedes
            Eintrags in der Suchergebnisliste mit dem Quellsystem.   Dabei wird versucht, anhand der
            Berechtigungsinformationen des aktuell suchenden Benutzers zu jedem Eintrag in der (technischen)
            Ergebnisliste im Hintergrund das entsprechende Dokument zu öffnen. Liefert das Quellsystem hierfür eine
            Fehlermeldung zurück (d.h. es ist keine Berechtigung vorhanden), wird das entsprechende Dokument in der
            Ergebnisliste des Benutzers nicht angezeigt.   Dieses Vorgehen ist zum einen langsam, zum anderen setzt es
            die Quellsysteme unter Last, da diese bei jeder Suchanfrage des Anwenders – auch bei bereits ausgeführten –
            immer die Inhaltsprüfungen beantworten müssen.   In der Praxis bedeutet dies: Im Optimalfall hat ein
            Benutzer das Recht, alle Inhalte zu sehen. Folglich wird bei einer Ergebnisliste von zehn Dokumenten genau
            zehnmal bei den Quellsystemen nachgefragt, ob ausreichende Berechtigungen vorliegen, und dem Benutzer werden
            diese Ergebnisse präsentiert. Hat ein Benutzer aber nur Rechte auf 10% der Inhalte, dann müssen schon 100
            Anfragen an die Quellsysteme gestellt werden, um eine gültige Ergebnisliste für den Benutzer zu erstellen.
            BENUTZERGRUPPEN Die Benutzergruppen hängen natürlich sehr eng mit dem Thema Security zusammen. Bestimmte
            Benutzer oder Gruppen sollen nur Zugriff auf Inhalte erhalten, für die sie auch berechtigt sind.   Für ein
            einfaches Internetportal (z.B. eine kostenfreie News-Seite) gibt es nur eine Benutzergruppe: Alle. Jeder
            Benutzer darf alle Inhalte auf dem Portal sehen und somit auch alles suchen und finden. Eine Unterscheidung
            findet nicht statt (zumindest nicht über eine User-Authentifizierung).   Im Extranet sieht dies schon anders
            aus. Dort kann der Zugang zu Inhalten über die Zugehörigkeit zu Benutzergruppen (z.B. in einem
            Partnerportal: Hat der Benutzer den Status „Partner“ ja/nein?) oder über die Rechte der einzelnen Benutzer
            reglementiert sein. Hier kann es dann also durchaus der Fall sein, dass nicht jeder Benutzer alle Inhalte
            sehen darf. Allerdings gibt es in den meisten Fällen nur ein Authentifizierungssystem, das beachtet werden
            muss. Daher ist eine Implementierung einmalig zu machen, um eine rechteabhängige Suche und Ergebnisanzeige
            zu gewährleisten.   Im Intranet sind die Hürden schon wesentlich höher. Hier gibt es häufig mehrere
            Authentifizierungssysteme, oft auch durch Unternehmen selbst „gestrickt“. Dies stellt für eine optimal
            eingestellte Suche mit Berücksichtigung aller Quellsysteme und deren Authentifizierungselemente eine große
            Herausforderung dar. Ein entsprechendes Mapping auf die einzelnen Benutzer und Gruppen muss erstellt und bei
            der Indexierung und Suche berücksichtigt werden. In der Regel gilt: Je größer das Unternehmen, desto größer
            auch die Anzahl an Benutzern, Gruppen und Quellsystemen und damit auch der Koordinationsaufwand für eine
            rechtegeprüfte Suche. FAZIT Alles in allem werden die Anforderungen an ein Suchsystem und die angegliederten
            Systeme (Crawler, Konnektoren, Security-Mapper usw.) vom Internet über das Extranet bis zum Intranet immer
            komplexer.   Für jedes Szenario gibt es diverse Stellschrauben und Komponenten, die zu beachten und für die
            Qualität der Suche ausschlaggebend sind. Eine „click-and-run“-Installation ist ein passabler Anfang, aber
            die Tücken stecken für eine gut funktionierende Suche im Detail.   Der Autor Mit 30 Jahren IT- und
            Programmiererfahrung beschäftigt sich Jörg Issel, Principal Solution Manager der IntraFind Software AG, seit
            1999 intensiv mit dem Thema Suche, insbesondere im heterogenen Unternehmensumfeld. &quot;Ich versuche immer,
            die für das Kundenproblem beste Lösung zu skizzieren und diese dann im Dialog mit den Kunden auch effizient
            umzusetzen. Die beste Software ist nutzlos, wenn sie die Probleme des Anwenders nicht löst.&quot;
        </body>
    </Document>
    <Document>
        <url>
            http://www.intrafind.de/blog/revolutioniert-die-semantische-suche-das-netz-trends-und-herausforderungen-in-der-forschung
        </url>
        <id>BA6B654B9347A6B415F7D5DF4C28E16D</id>
        <title>Revolutioniert die “Semantische Suche” das Netz? Trends und Herausforderungen in der Forschung.</title>
        <body>Revolutioniert die “Semantische Suche” das Netz? Trends und Herausforderungen in der Forschung.   Der
            Begriff „semantische Suche“ wird sehr unterschiedlich und teilweise inflationär genutzt. Eigentlich fällt
            schon jedes Verfahren darunter, das die Benutzeranfrage (Query) in irgendeiner Weise interpretiert und
            versucht, eine optimale Antwort (keine reine Volltextsuche) darauf zu geben. Verfahren wie die
            „semantisch-assoziative Suche“ der IntraFind Software AG liefern verwandte Begriffe auf Basis der
            indexierten Dokumente, die zur Verfeinerung oder Erweiterung einer Suche dienen können.   Die Suche nach dem
            Begriff „Clinton“ liefert z.B. die möglichen Spezialisierungen „Hillary Clinton“, „Bill Clinton“ oder
            „Chelsea Clinton“, ohne dass diese Personen in einer Ontologie hinterlegt wären, rein auf Basis der
            vorhandenen Textdokumente. Jedoch wird auch der Begriff „Weißes Haus“ als mögliche Erweiterung der Suche
            geliefert. Das Verfahren ist vergleichbar zu Clustering-Techniken. Eigentlich wird eine Tag Cloud zur
            aktuellen Suche geliefert.   Vorteil: Eine manuelle Pflege ontologischer Ressourcen ist nicht notwendig, da
            das Verfahren rein auf statistischer Basis, jedoch unter Berücksichtigung linguistischen Wissens
            (Wortkategorien, Noun Phrase-Erkennung) arbeitet. Einen ähnlichen Effekt kann man durch Einbeziehung eines
            manuellen Thesaurus erzielen. Auf dieser Basis kann sogar sprachübergreifend (crosslingual) gesucht werden.
              In letzter Zeit prägen Google und Siri den Begriff “semantische Suche”.   Benutzeranfragen werden
            interpretiert und anstelle einer Trefferliste werden wirkliche Antworten generiert, zumindest bei
            Faktenfragen wie der Frage nach einem chinesischen Restaurant in der Nähe oder nach dem Geburtsdatum der
            Bundeskanzlerin. Bei nicht eindeutig interpretierbaren Faktenfragen wird ein Artikel der Wikipedia zum
            Hauptsuchbegriff zurückgeliefert. Faktenfragen lassen sich schon mit relativ einfachen Verfahren (Wer, Wo,
            Wann, Wie groß, Wie viel, …) erkennen und interpretieren. Sie werden in den meisten Fällen durch Einträge
            aus Datenbanken (oder Triple Stores wie der dbpedia) beantwortet. Ohne diese strukturierte Information aus
            Datenbanken würde die „semantische Suche“ á la Google und Siri nicht funktionieren.   Besonders
            beeindruckend sind natürlich solche Ergebnisse, wenn gleichzeitig der aktuelle Ort des Fragestellers mit
            einbezogen wird oder die „semantische Suche“ mit einer Spracherkennung verbunden ist. D.h. die hinter der
            semantischen Suche von Google und Siri steckende Technologie ist kein Hexenwerk. Echtes Textverständnis auf
            Basis von intelligenten Verfahren kommt nicht zum Einsatz.   Da jedoch in der Praxis oft die strukturierten
            Daten fehlen, versuchen wir bei IntraFind bzgl. „semantischer Suche“ etwas weiterzugehen. Wir erkennen
            Entitäten wir Personen, Organisationen und Orte in Texten. Schon vor 4 Jahren haben wir eine semantische
            Suchmaschine gebaut, die Faktenfragen rein auf der Basis von Text beantworten kann. Wie oben geschildert,
            werden Faktenfragen auf Basis einfacher Muster erkannt.   So wird die Frage nach den Gründern von Microsoft
            („Wer hat Microsoft gegründet?“) übersetzt in eine Query nach Personen in der Nähe des Begriffs “Microsoft”
            und Synonymen des Wortes „gründen“. Damit lässt sich die Frage auch ohne eine Datenbank mit Faktenwissen und
            ohne aufwändig manuell gepflegte Ontologien beantworten.   Aus meiner Sicht sind derartige Ansätze
            notwendig, um beim Thema „semantische Suche“ weiterzukommen, denn für die meisten interessanten Fragen gibt
            es keine manuell gepflegte Datenbanken, sondern nur textuelle Information. Für einen wirklichen Erfolg der
            semantischen Suche brauchen wir Verfahren, die Faktenwissen aus Texten extrahieren. Dies ist die
            Herausforderung für die nächsten Jahre.   Der Autor Dr. Christoph Goller verfügt über 15 Jahre Erfahrung im
            Enterprise Search Markt. Er promovierte in Computerwissenschaften an der Technischen Universität in München
            und arbeitete im Anschluss in einigen wissenschaftlichen Projekten zu Themenbereich Künstliche Intelligenz,
            Machine Learning sowie Neuronale Netze.   Seit 2002 leitet er den Bereich Forschung bei IntraFind und
            verantwortet IntraFind&apos;s Kern-Suche- und Content Analyse Technologien. Zwischen 2003 und 2007 arbeitete
            Christoph aktiv als Committer in Apache&apos;s Open Source Projekt Lucene.    
        </body>
    </Document>
    <Document>
        <url>http://www.intrafind.de/blog/approximative-data-structures-for-natural-language-processing-en</url>
        <id>9E800D3572CB03DA3F2065692EC7C2E4</id>
        <title>Approximative data structures for natural language processing</title>
        <body>Approximative data structures for natural language processing   Some say software developers draw their
            motivation from minimizing or maximizing numbers in any given problem. That&apos;s a smug innuendo. From my
            experience, developers are always on the lookout for beautiful solutions, of which numbers are but a
            symptom. The usage of approximative data structures for language processing is one such example of a
            beautiful idea with nice numbers.   When dealing with natural language we&apos;re often confronted with the
            need of storing and retrieving language statistics, as e.g. n-Gram distributions, word counts, document
            frequency counts, etc. Inherent to the discrete nature of language, most of these statistics fall into a
            key-value schema, i.e. for a particular key (say, a word) we want to know a number. Let&apos;s focus on one
            particularly well known statistics, the document frequency (DF). The DF of a word is the number of documents
            in a corpus in which this word appears. The DF is used in the computation of the TFIDF, a simple yet proven
            heuristic for determining relevance.   To put things into perspective, the English Wikipedia has around 7Mio
            articles and the same order of magnitude of distinct words. If we were to use a hash map for storing the
            DFs, we would need approximately 100MB of memory just to store the words and counters (add at least another
            100MB for data structure overhead). Thats for one language and one kind of statistics. One way to achieve
            lower memory requirements is to abdicate precision. This is a small price to pay, since we&apos;re dealing
            with corpus statistics, an intrinsically noisy affair.   Count-Min Sketch [C&amp;M 2005] is a sub-linear
            space data structure for computing approximate frequencies. It allows us to trade-off exactitude for memory.
            By tuning its two only parameters ( ϵ and δ ) we can answer the question of how many times we have seen a
            word within ϵ ⋅ | w | (where | w | is the number of distinct words in a corpus) of the actual value with
            probability δ . The memory needed for this will be proportional to 1 ϵ ⋅ - log 2 ( 1 - δ ) . Count-Min
            Sketches (CMS) share some similarities with bloom filters by using pairwise independent hash functions and
            sharing memory among keys. Figure 1 A CMS holds a two dimensional array of counters. The depth of the array
            determines the confidence probability ( δ ) and also the amount of hash functions we will use. The width
            determines the mean error we accept for count estimates. The two operations defined on a CMS are: add(key,
            value) and estimate(key). On add(key, value) we add value to the counters in the buckets corresponding to
            the computed hash values for each row using the independent hash functions (fig. 1). On estimate(key) we
            return the minimum value out of all corresponding buckets (fig. 2). As a side note: an easy improvement on
            the error expectation in detriment of a little runtime overhead is to change add(key, value) to only update
            the minimal buckets. Figure 2 One easy observation is that a CMS suffers under saturation, meaning that the
            precision guarantees depend on the number of distinct entries (words in our case); i.e. the more distinct
            words are added, the larger the chance of hash collisions distorting the counts. In a CMS errors are always
            positive, since there is no operation decreasing counter values. Yet another observation is that the errors
            have bigger impact on rare entries. Following the latter observation, CMS errors have a smoothing effect on
            word counts, which in NLP is something often required. As a cherry on the cake: CMSs have associative
            properties and can be elegantly distributed.   At IntraFind we have been using CMSs and other approximation
            techniques (e.g. sampling) in a few nlp applications with great success. We see approximative data
            structures as a vital technology block in a big data realm. At the present moment we are working on a novel
            approximative data structure, which we call a converging map, to store mappings from normal forms of terms
            or term compounds to their most frequent surface form appearing in a corpus, without the need of storing
            neither the normal forms nor the less frequent surface forms.     Bibliography: [C&amp;M 2005]: Cormode, G.,
            &amp; Muthukrishnan, S. (2005). An improved data stream summary: the count-min sketch and its applications.
            Journal of Algorithms, 55(1), 58-75. Der Autor Breno Faria, Head of Development, ist seit 2012 für die
            IntraFind Software AG tätig. Seit den späten 2000er Jahren beschäftigt er sich intensiv mit den Themen
            Content Analytics und Information Retrieval. 2015 übernahm er die Rolle des Entwicklungsleiters bei
            IntraFind.   Im Rahmen von Veranstaltungen, z.B. Berlin Buzzwords 2014 oder &quot;IntraFind Enterprise
            Search Day 2015&quot;, referiert er regelmäßig über neue Technologien oder präsentiert innovative Lösungen
            aus IntraFind Kundenprojekten.
        </body>
    </Document>
    <Document>
        <url>
            http://www.intrafind.de/company/news/intrafind-offers-netapp-customers-ready-to-use-enterprise-search-solutions
        </url>
        <id>852C4077210784107D3CF8A4370319BE</id>
        <title>11/21/2016 - IntraFind Offers NetApp Customers Ready to Use Enterprise Search Solutions</title>
        <body>IntraFind Offers NetApp Customers Ready to Use Enterprise Search Solutions As the first enterprise search
            provider worldwide, NetApp Alliance Partner IntraFind Software AG offers with the iFinder5 elastic a
            NetApp-certified solution for intelligent full-text search in NetApp file services. NetApp storage solutions
            store large amounts of data in file systems. With the iFinder5 elastic, users can search documents in their
            file systems quicker, both full-text and related metadata. The advantages: The user no longer has to use the
            time-consuming, slow and resource-intensive explorer search and finds the desired information in split
            seconds. One of the central aspects for application in a company is the consideration of authorizations.
            Thanks to Secure Search, users can only access the documents they are authorized to access. The IntraFind
            NetApp connector, in combination with the fpolicy interface, processes large amounts of data quickly and
            securely. If a new document is created, deleted, changed, or authorization is changed, this change is
            processed directly and almost without a time lag in the iFinder. An elaborate crawling is not necessary for
            this. This way, the information and authorizations are always up-to-date and the search results always
            reflect the current status in real-time. Thanks to numerous connectors, such as Microsoft Exchange,
            Confluence, Microsoft SharePoint, ERP or CRM systems, the iFinder can also be easily converted into a
            company-wide search solution. The iFinder5 elastic thus forms the central information and knowledge
            repository of a company. The iFinder – tailored to the quality requirements of a customer – offers all the
            usual features for a professional, market-leading search solution such as faceting and filtering, preview
            with hit highlighting, intelligent autocomplete with typing error correction and the full range of a Boolean
            search. In addition, the iFinder stands out through numerous expandabilities such as semantic search and
            proper noun recognition. Thanks to excellent linguistics combined with deep learning, the iFinder can be
            expanded to an insight engine that can process natural-language (NLP) search queries. Ready to Use Solution
            for NetApp Customers IntraFind provides NetApp customers with the iFinder5 elastic a ready to use product
            based on Elasticsearch with an intuitive user interface. The software package is available either as license
            or subscription. Users get a flexible search solution that can either be used as a standalone solution for
            NetApp file services or extended to a comprehensive enterprise search solution. The iFinder can be
            seamlessly integrated into the existing IT infrastructure. The application is easily scalable: if the data
            grows, the iFinder5 elastic grows with it. The enterprise search solution is therefore suitable for small
            companies as well as large corporations. IntraFind provides a test package that allows you to quickly and
            easily test the search solution with your own documents. About IntraFind Software AG IntraFind develops
            products and solutions for easy searching, finding, and analyizing of structured and unstructured
            information across all available data sources of a company. Key aspects are full-text search and the
            complete range of text analysis and machine learning methods, natural language processing, combined with the
            possibilities of graphical databases for Big Data Analytics. Well-known customers are: AUDI AG, German
            Federal Armed Forces, IHK Berlin, Robert Bosch GmbH and Rohde &amp; Schwarz GmbH &amp; Co. KG. More
            information: www.intrafind.com For more information please contact IntraFind Software AG Christiane Stagge
            Landsberger Straße 368 D-80687 München E-mail: christiane.stagge@intrafind.de Internet:
            https://www.intrafind.com  
        </body>
    </Document>
    <Document>
        <url>http://www.intrafind.de/blog/contract-analyzer-vertraege-analysieren-mit-due-diligence-software</url>
        <id>4E0E831876D63BE6F69CA663968FE405</id>
        <title>Contract Analyzer: Verträge analysieren mit Due Diligence Software</title>
        <body>Contract Analyzer: Verträge analysieren mit Due Diligence Software Contract Analyzer: Verträge analysieren
            mit Due Diligence Software Es sollte eine der größten Übernahmen in der Firmengeschichte werden: Als Rolf M.
            den Vertrag unterzeichnete, schien das Geschäft perfekt. Mit dem Zukauf der anderen Firma wollte Rolf M. mit
            seinem Unternehmen in neue Marktfelder vordringen und das Produktportfolio erweitern. Doch eines Morgens kam
            mit einem einfachen Brief das böse Erwachen: Wie sich herausstellte, gehörten die Markenrechte des
            relevanten Produkts von der neu übernommenen Firma längst einem anderen Unternehmen. Das Unternehmen von
            Rolf M. konnte nicht mehr davon profitieren. Der Schaden ging in die Millionenhöhe. Mit Due Diligence
            Software Risiken erkennen Steht eine Firma oder Organisation zum Verkauf oder spielt ein Unternehmen mit dem
            Gedanken, ein anderes Unternehmen zu übernehmen, fallen nicht nur betriebswirtschaftliche Entscheidungen an.
            In sogenannten Due Diligence Verfahren wird geprüft, welchen Wert das Unternehmen hat und welche laufenden
            Verträge es gibt. Dabei kommen Fachjuristen oder Anwälte ins Spiel, die oftmals für spezielle Firmen für
            Merger &amp; Acquisitions (M &amp; A) tätig sind. Sie überprüfen, ob es zum Beispiel noch laufende
            Gerichtsverfahren gibt oder welche bestehenden Kauf- und Mitarbeiterverträge existieren. Bei solchen Due
            Diligence Verfahren geht es darum, Risiken aufzuspüren, die sich durch die Übernahmen ergeben könnten. Im
            Falle von Rolf M. waren dies Patent- und Markenrechtsvereinbarungen, die einige Jahre zuvor geschlossen
            wurden und von den Anwälten wohl übersehen wurden. Due Diligence- und M &amp; A-Verfahren finden oftmals
            unter sehr hohem Zeitdruck und begrenztem Budget statt. Fachjuristen müssen dabei seitenlange Verträge
            durcharbeiten. Bei einer 60-Millionen-Transaktion müssen Anwälte sich schätzungsweise durch etwa 40.000
            Dokumente durcharbeiten. Dabei fallen enorme Kosten an: Die Gesamtkosten, die weltweit durch Due Diligence
            verursacht werden, werden auf 90 Milliarden Dollar geschätzt. Durchschnittliche Stundensätze für
            Fachjuristen von 350 Euro sind dabei keine Seltenheit. Mit Artificial Intelligence Risiken und Kosten
            minimieren Die Software Contract Analyzer soll Anwälte künftig bei ihrer Arbeit unterstützen und Menschen
            wie Rolf M. rechtzeitig davor warnen, wenn sie unnötige Risiken eingehen. Die Anwendung basiert auf
            Artificial Intelligence und unterstützt Machine Learning Verfahren. Dadurch ist sie in der Lage, bestimmte
            Klauseln, die kritisch sind, in den Verträgen zu erkennen und farbig zu markieren. Der Fachjurist spart sich
            Zeit, weil ihm die Software das seitenlange Lesen abnimmt. Er kann sich hingegen auf den wesentlichen Teil
            seiner Arbeit konzentrieren, nämlich das Prüfen und Bearbeiten der einzelnen Klauseln. Für das Unternehmen
            minimieren sich wiederum die potenziellen Risiken und auch die Kosten, da das Prüfen der Verträge mit
            weniger Aufwand verrichtet werden kann. Über das zentrale Dashboard des Contract Analyzer sieht der
            Fachjurist auf einem Blick, welche Dokumente ihm zugeordnet sind. Die kritischen Stellen des Vertrags
            erkennt er anhand eines Red Flag Reports. Das Management sieht ebenfalls über das Dashboard, bei welchen
            Verträgen es Punkte gibt, die gegebenenfalls nachverhandelt werden müssen. Der Contract Analyzer ist
            komplett browsergesteuert. Egal ob Netzlaufwerke, Dokumentenmanagementsysteme oder Wikis – der Contract
            Analyzer verfügt über zahlreiche Konnektoren und kann an verschiedene Datenquellen angebunden werden. So
            kann er alle wesentlichen Verträge analysieren, egal an welchem Ort sie vorher gespeichert worden sind. Es
            ist außerdem möglich, gescannte Dokumente über eine integrierte OCR-Schnittstelle einzubinden und zu
            analysieren. Zusätzlich unterstützt der Contract Analyzer über 600 Dateiformate. Dem Nutzer wird dadurch
            eine Vorschauansicht auf das wesentliche Dokument ermöglicht, ohne dass die entsprechende Software, wie
            beispielsweise Microsoft Visio, auf seinem Gerät installiert sein muss. Über die Benutzeroberfläche kann der
            Fachjurist auch nach bestimmten Dokumenten suchen. Die Ergebnisse in der Trefferliste sind dabei
            rechtegeprüft. Das bedeutet, dass er nur die Dateien sieht, die er auch berechtigt ist zu sehen. Hat der
            Fachjurist die jeweiligen Dokumente geprüft, kann er sie freigeben und zu einem Report hinzufügen. Andere
            Projektverantwortliche können diesen Status wiederum über ihr eigenes Dashboard mitverfolgen. Mit dem
            Contract Analyzer kann eine Firma somit nicht nur erfolgreich Risiken minimieren, sondern auch effizienter
            arbeiten und Geld sparen. Haben wir Ihr Interesse geweckt? Dann nehmen Sie mit uns Kontakt auf und fragen
            uns nach dem Contract Analyzer. Mehr Informationen finden Sie auch unter AnalyzeLaw.com Über die Lösungen zu
            Artificial Intelligence von IntraFind finden Sie hier weitere Details.   Der Autor Robert Eberhard ist
            Diplom-Informatiker und kann auf über 20 Jahre Berufserfahrung zurückblicken. Seine Schwerpunkte sind
            Suchmaschinenoptimierung und IT-Projektmanagement.
        </body>
    </Document>
    <Document>
        <url>http://www.intrafind.de/blog/einfuehrung-einer-unternehmensweiten-suche-aller-anfang-ist-nicht-schwer</url>
        <id>99A05BD83E3DC4D90412445AC1AF1326</id>
        <title>Einführung einer unternehmensweiten Suche: Aller Anfang ist (nicht) schwer</title>
        <body>Einführung einer unternehmensweiten Suche: Aller Anfang ist (nicht) schwer   Die Einführung einer
            unternehmensweiten Suche ist und bleibt ein IT-Projekt, birgt somit eine gewisse Komplexität und sollte
            daher gut geplant sein.   Enterprise Search hat viele Facetten: Es soll die Suche und Darstellung von
            Zusammenhängen - z.B. im Bereich Produktdatenmanagement - ebenso unterstützen wie die Klärung von
            Fragestellungen im Vertragsmanagement, im Marketing, im Vertrieb und allen anderen Abteilungen. Der Nutzen
            einer Search-Lösung liegt schließlich darin, den Informationsschatz eines Unternehmens allumfassend in den
            Griff zu bekommen und – sofern freigegeben- möglichst vielen Mitarbeitern zur Verfügung zu stellen.   Aus
            jahrelanger Projekterfahrung heraus empfehle ich Ihnen jedoch, sich dabei nicht zu übernehmen („think big“),
            sondern mit einem iterativen Vorgehen zunächst Erfahrungen in einem kleineren Projekt zu sammeln („start
            small“). Denn schon dort werden Sie erste Hürden nehmen müssen, mit denen Sie auch später im größeren Rahmen
            konfrontiert sein werden.   Sind diese ersten Stolpersteine jedoch aus dem Weg geräumt, ist die Suche in
            Teilbereichen erst einmal eingeführt und der erste Druck aus den Fachabteilungen verschwunden, erhalten Sie
            genau das Schulterklopfen, aus dem die Motivation hervorgeht, auch komplexe Dinge anzugehen.   Also klein
            anfangen und dann wachsen! Starten Sie beispielsweise mit der Anbindung einer Datenquelle an die Suche und
            erweitern Sie sukzessive um weitere Datenquellen.   Besonders sinnvoll als erster Angriffspunkt: die
            File-Server. Hier schlummert in der Regel ein großer Teil des Wissens eines Unternehmens. Hier liegt der
            richtige strategische Ansatz, hier bietet sich viel Raum für Experimente und erste Erfahrungen. Der Aufwand
            ist überschaubar, das Effizienzsteigerungspotential enorm und Sie erhalten einen guten Eindruck über die
            Qualität der Suchmaschine und wie solch ein System von den Mitarbeitern angenommen und in den täglichen
            Arbeitsprozess integriert wird.   Tragen Sie sich mit dem Gedanken, eine übergreifende Suche in Ihrem
            Unternehmen einzuführen? Es werden unzählige Fragen auftauchen: Wie gehen wir vor? Wie viel Budget brauchen
            wir? Wie viele Projektmitglieder? Für welches Produkt entscheiden wir uns? Welche Datenquellen sollen an die
            Suche angebunden werden? Wie und wo verankern wir sie? Wie sehen Oberfläche und Trefferliste aus? Und, und,
            und.   Vergleichbar mit dem Bau eines Hauses, das man auch nicht ohne Architekt baut, fällt es mit ein
            bisschen Hilfe von extern meist leichter, sich in diesem Neuland zu bewegen.   Die wichtigste Frage lautet
            doch:   Was muss ich berücksichtigen, damit das Projekt „Unternehmensweite Suche“ ein Erfolg wird?   Für das
            Internet lässt sich das leicht beantworten. Die unterschiedlichsten kommerziellen Suchen haben alle eins
            gemeinsam: sie liefern als Ergebnis das, was der Benutzer im Rahmen seiner Aufgabenstellung sucht. Die
            Erwartungen der Nutzer werden somit erfüllt.   Übertragen auf Ihr Unternehmens-oder Behördenumfeld muss die
            Suchmaschine also sowohl bei generellen Suchanfragen (denken Sie an den Speiseplan der Kantine oder an
            Vorlagen für Urlaubsanträge etc.) als auch bei hochspezialisierten und sehr unterschiedlichen Recherchen
            („Welche Kundenbeschwerden gab es im letzten Quartal zu Produkt X und wurden deshalb Änderungen am Produkt
            vorgenommen?“) unterstützen und zufriedenstellende Ergebnisse liefern. Entwicklung, Produktion, Personal,
            Finanzen, die Art der Anfragen kann dabei unterschiedlicher nicht sein.   Aus der Sicht einer Privatperson
            betrachtet, sucht man kaum eine Wohnung, ein Auto oder einen konkreten Musiktitel über eine der großen
            Internetsuchmaschinen. Ich begebe mich stattdessen auf ein spezielles Immobilienportal, eine Autobörse oder
            auf das Portal eines Musikanbieters. Andererseits kann ich jedoch kaum erhoffen, von einem Immobilienportal
            einen Überblick über die besten, derzeit auf dem Markt befindlichen Fernsehgeräte und deren innovative
            Funktionen zu erhalten.   Doch genau diese beiden Typen von Anforderungen gilt es mit einer Suchmaschine für
            ein Unternehmen zu erfüllen. Die Alternative, eine Fülle an Suchtechnologien je Applikation, Abteilung oder
            Art der Frage zu beschaffen, ist keine Option, zumal hier für Betrieb und Wartung hohe Kosten anfallen
            würden, die solche Projekte schnell unwirtschaftlich machen.   Ein erfolgreich eigeführtes Suchsystem trifft
            die unterschiedlichen Bedürfnisse der Nutzer und liefert dabei vollständige und qualitativ hochwertige
            Suchergebnisse - gleichgültig, ob eine zentrale, allgemeine Suche (Speiseplan) oder eine spezialisierte
            Recherche ausgeführt wird. Die Art und Weise, wie die User an das System herangehen, ist dabei für alle
            Arten der Suche gleich.   Der Grundstock hierfür ist eine Lösung, in der alle relevanten Inhalte für alle
            Benutzer mit der Suchmaschine indiziert und in der Suche bereitgestellt werden (können); wie auch immer die
            Bereitstellung später aussieht. Dies bedeutet, dass erst einmal alle notwendigen Datenquellen der
            Organisation in der Suche erfasst und zur Verfügung gestellt werden müssen.   Denn nur dann bin ich der
            Lage, auf die verschiedenen Bedürfnisse der Benutzer zu reagieren. Ich kann beispielsweise verschiedene
            Datentöpfe miteinander kombinieren, ohne Qualitätseinbußen in der Suche hinnehmen zu müssen und kann
            sicherstellen, dass Benutzer nur eine Suchsyntax lernen müssen. Dank einer einheitlichen intuitiv
            bedienbaren Benutzeroberfläche besteht hier jedoch kein großer Schulungsbedarf.   Grundlegende Funktionen
            und Schnittstellen   Welche grundlegenden Funktionen muss eine Suche dafür haben?   Dazu gehören:   Fertige
            Such- und Ergebnisseiten, die anpassbar sind, um sie im Intranet oder Internet in einem Portal anzubieten;
            dies ermöglicht die schnelle Bereitstellung einer Suche für das Unternehmen, ohne sich gleich um eine
            komplexe Integration in bestehende Portale kümmern zu müssen. Die Möglichkeit, die Suche oder Komponenten
            davon in ein bestehendes Intranet oder Web einzubinden - der logische nächste Schritt, wenn bereits Portale
            im Unternehmen bestehen. Funktionen und Schnittstellen, um die Suche in bestehende Anwendungen zu
            integrieren. Hier wird es spannend, denn daran denkt man in der Regel erst später. Sogenannte „Konnektoren“,
            die es mir erlauben, die verschiedenen im Unternehmen vorhandenen Datenquellen in die Suche einzubinden. Die
            Möglichkeit, gegebenenfalls Suchanfragen an andere Suchmaschinen weiterzuleiten und deren Ergebnisse zu
            verarbeiten. Dies ist notwendig, wenn Informationen aus dem Internet einbezogen werden sollen Nicht zu
            vergessen, der entscheidende Erfolgsfaktor:   Die Fähigkeit, Zugriffsrechte bei der Indexierung, bei der
            Suche und auch in der Oberfläche zu berücksichtigen! Nicht jeder Benutzer darf und soll sämtliche, zu seiner
            Suchanfrage im Unternehmen vorhandenen und verfügbaren Informationen in einer Trefferliste sehen.   Oft
            vergessen: Navigationselemente und Steuerelemente einer guten Suchoberfläche werden häufig aus dem
            durchsuchten Inhalt gebildet. Über eine rechtegeprüfte Autovervollständigung ist hier zu steuern, welche
            Inhalte der Benutzer sehen darf und welche nicht.   Erfüllt die ausgewählte Suchmaschine all diese
            Kriterien, beginnt das nächste große Kapitel des Abenteuers Suchmaschineneinführung: wie stelle ich meinen
            Benutzern die Informationen zur Verfügung?   Darüber dann mehr im nächsten Blog-Beitrag.   Der Autor Nach
            dem BWL Studium war Rutger Lörch in verschiedenen Positionen bei namhaften Hardware- und Softwareanbietern
            wie Nixdorf, Digital und Oracle tätig. Seit 2008 unterstützt er beim Suchspezialisten IntraFind den
            Vertrieb. „Das Thema Enterprise Search ist deshalb so faszinierend, weil die Anforderungen der jeweiligen
            Interessenten sehr individuell sind. Ein sehr abwechslungsreiches Betätigungsfeld voller spannender
            Herausforderungen.“
        </body>
    </Document>
    <Document>
        <url>http://www.intrafind.de/blog/was-2017-wichtig-wird</url>
        <id>EBE5FBD5306083BC186140E9B2C9FC2A</id>
        <title>Was 2017 wichtig wird</title>
        <body>Was 2017 wichtig wird Die wichtigsten IT-Trends für Enterprise Search Lösungen Neues Jahr, neue Trends:
            Kaum waren die letzten Silvesterböller verschossen, trafen sich die ersten IT-Experten Anfang 2017 schon
            wieder zu Konferenzen und Messen wie beispielsweise zur CES in Las Vegas. Artificial Intelligence war dort
            eines der zentralen Themen. Außerdem wurden zahlreiche neue Gadgets vorgestellt, die im Laufe des Jahres
            wohl auch in den deutschen Märkten zu finden sein werden. An Prognosen über neue Produkte und Techniktrends
            mangelt es nicht. Laut dem Analystenhaus Gartner werden Themen wie Internet of Things, Machine Learning und
            Big Data auch 2017 die wichtigste Rolle in der IT-Branche spielen. Gartner sieht dabei die wesentlichsten
            Entwicklungsschübe bei den intelligenten Apps, Smart Cars und Chatbots. Machine Learning wird insbesondere
            den Betrieb der Rechenzentren vorantreiben, die Automatisierung der Systeme wird noch mehr zunehmen, so dass
            sie in einem Störungsfall proaktiv reagieren können. Die Rechenleistung verbessert sich immer weiter, das
            Datenaufkommen steigt und Algorithmen werden immer ausgereifter. Die Weiterentwicklung von Technologien wird
            Machine Learning und Deep Learning weiteren Auftrieb geben. So sollen digitale Assistenten wie Cortana und
            Siri künftig Verhaltensmuster analysieren, verstehen und Entscheidungen daraus ableiten können.      Das hat
            auch Auswirkungen auf Enterprise Search. Professionelle Enterprise Search Software ist nicht mehr nur auf
            das reine Suchen und Finden von Daten beschränkt. Durch zahlreiche semantische Funktionen, linguistische
            Features und Verfahren wie Natural Language Processing (NLP), das Sprache verarbeitet und daraus
            Erkenntnisse ableitet, ist die Software in der Lage, den Nutzer zu „verstehen“ und aus unstrukturierten UND
            strukturierten Daten Informationen zu gewinnen und in Kontext zu setzen. IoT, Big Data oder Deep Learning
            werden die Funktionalitäten von Enterprise Search Software maßgeblich beeinflussen. Was wird 2017 aus
            unserer Sicht besonders wichtig? Wir haben bei den Kollegen aus der Vorstandsetage, dem Marketing und der
            Entwicklung nachgefragt: Franz Kögl, Vorstand IntraFind Software AG: „Tiefes Textverständnis ist für mich
            das entscheidende Thema für 2017. Anwender wollen die Daten nicht einfach nur präsentiert bekommen, sondern
            wollen deutlich mehr aus den bestehenden Informationen herausholen. Auch wird sich die Art und Weise, wie
            gesucht wird, mit natürlichsprachlicher Suche deutlich verändern. Enterprise Search Engines müssen sich zu
            Insight Engines entwickeln, die dem Anwender wichtige Erkenntnisse liefert. Eine professionelle Enterprise
            Search Software wird ohne Verfahren wie Natural Language Processing und Deep Learning nicht mehr auskommen.“
            Dr. Christoph Goller, Head of Research IntraFind Software AG: „Machine Learning Verfahren wie Neuronale
            Netze und auch Deep Learning werden 2017 weiterhin eine große Rolle bei uns spielen. Der jetzige Hype um
            diese Themen amüsiert mich wirklich ... wir machen das schon seit über 10 Jahren und nutzen die Verfahren in
            unseren Produkten und Lösungen Schön zu sehen, dass unser &quot;Tägliches Brot&quot; im Mainstream
            angekommen ist.“ Sonja Bellaire, Marketing Managerin bei IntraFind Software AG: „IT betrifft uns alle.
            Software muss daher so einfach wie möglich zu bedienen und zu verstehen sein. Darstellungsmöglichkeiten wie
            der Knowledge Graph bieten hierfür eine prima Möglichkeit.“ Breno Faria, Head of Development, IntraFind
            Software AG: „Technologien rund um Roboter, die Arbeitsschritte übernehmen oder Chatbots, die automatisch
            antworten, werden sich 2017 erheblich weiterentwickeln. Das spielt auch der Enterprise Search Software in
            die Hände. Künftig weiß die Software anhand meines Kalenders, dass ich mich gerade auf dem Weg in ein
            Meeting befinde und liefert mir alle Dokumente, die ich dafür brauche.“             Der Autor Christiane
            Stagge studierte Geschichte und Politikwissenschaft und ist seit 2006 als IT-Redakteurin tätig. Seit 2016
            unterstützt sie die IntraFind Software AG in der Unternehmenskommunikation.   
        </body>
    </Document>
    <Document>
        <url>http://www.intrafind.de/blog/tipps-tricks-fuer-ihr-erfolgreiches-enterprise-search-projekt</url>
        <id>D5F3DEB560A17366912679F83632179C</id>
        <title>Tipps &amp; Tricks für Ihr erfolgreiches Enterprise Search-Projekt</title>
        <body>Tipps &amp; Tricks für Ihr erfolgreiches Enterprise Search-Projekt   Aller Anfang muss nicht schwer sein -
            kann aber ...   Im Verlauf von 14 Jahren Praxiserfahrung - vom Erstgespräch bei einem Interessenten bis hin
            zum Ausbau einer Suchlösung im produktiven Betrieb - war ich mit einer Vielzahl unterschiedlicher Projekte
            konfrontiert: angefangen von rein IT-getriebenen Themen bis hin zu Projekten, die eine Fachabteilung ohne
            Einbindung der IT-Abteilung in Eigeninitiative umgesetzt hatte.   Das Buying Center eines Enterprise
            Search-Projektes setzt sich aber sinnvollerweise aus beiden &quot;Lagern&quot; zusammen: Die IT allein
            vernachlässigt meist die Wünsche und Bedürfnisse der Benutzer, die Fachabteilung bekommt oft hinterher
            Probleme, wenn die IT zu spät an Bord geholt und nicht von Anfang an eingebunden wurde. Auch haben wir es
            bei unseren Kunden mit einer großen Bandbreite an Expertise für Search und Content Analytics zu tun:
            Fachabteilungen, die wild mit Buzzwords wie „Semantische Suche“ oder „Semantische Netze“ um sich werfen und
            bei Nachfrage nicht sagen können, was damit im konkreten Fall gemeint ist, bis hin zu Experten, die mit mir
            auf Expertenniveau die Details der Funktionsweise unserer Linguistik diskutieren.   Use Cases   Das bringt
            uns zu einem ersten wichtigen Kriterium für ein erfolgreiches Suchprojekt: Gute Planung im Vorfeld. Den
            Aspekt, ein vernünftiges Projekt mit professionellem Projektmanagement aufzusetzen, sich um das
            Hardware-Sizing oder um Authentifizierungsfragen / Single Sign On-Themen für rechtebasierte Suche zu
            kümmern, lassen wir jetzt außen vor. Es geht mir hier um die oftmals sträflich vernachlässigten Use Cases
            bzw. eine präzise Anforderungsanalyse.   Ich höre oft Sätze wie „Wir brauchen eine interne Google-Suche&quot;
            - und das war´s dann mit den Anforderungen. Dabei wäre es so einfach, mit normalen Benutzern und &quot;Wissensarbeitern&quot;
            zu sprechen, welche Fragen sie in Ihrem täglichen Arbeitsumfeld zu beantworten haben, wie sie heute nach
            welchen Informationen suchen und wie genau dieser Prozess verbessert werden kann. Vielleicht stellen Sie im
            Gespräch mit Ihren SAP-Usern fest, dass es für Ihre Kollegen nicht sinnvoll ist, SAP-Inhalte aus dem
            Rechnungswesen in einer Enterprise Search auffindbar zu machen, weil die &quot;Heavy-SAP-User&quot; nie im
            Leben auf die Idee kämen, ihre Rechnungen nicht mit SAP-Bordmitteln zu suchen. Diese Experten kennen ihre
            Buchungskreise und die oftmals verschlungenen Pfade zu den gesuchten Dokumenten - und schon haben Sie Ihr
            Suchprojekt erheblich vereinfacht.   Benutzer frühzeitig zu involvieren und zu befragen, hat den Vorteil,
            später im produktiven Betrieb auch einen Vergleich anstellen zu können, was sich konkret verbessert hat. Die
            Investition in ein Enterprise Search-Projekt muss wie bei allen Projekten irgendwann gerechtfertigt werden.
            Suchprojekte haben - wie im vorherigen Blogbeitrag bereits dargestellt - oftmals das Problem eines schlecht
            messbaren ROIs. Anhand der konkreten Anforderungen der Benutzer kann dann aber später belastbar die
            qualitative Verbesserung dargestellt werden.   Think big - start small   Ein wichtiger Tipp: Unterschätzen
            Sie niemals die Komplexität eines Suchprojektes! Eine Lösung für die Suche im Dateisystem ist schnell
            installiert und konfiguriert. Je mehr Datenquellen aber in die unternehmensweite Suche mit eingebunden
            werden, desto komplexer wird die technische Seite des Projektes. Die verschiedenen Datensilos müssen
            angebunden werden - der indizierungs- und suchseitige Zugriff auf die Inhalte muss schnell, rechtegeprüft
            und updatefähig erfolgen und dementsprechend konzipiert werden. Je mehr Quellen, desto mehr Vorarbeit. Wir
            unterscheiden hier zwischen einfachen Quellen, die &quot;out-of-the-box&quot; angebunden werden, dann
            Repositories wie Lotus Notes oder Microsoft Exchange, bei denen die Konnektoren konfiguriert werden müssen,
            und Datentöpfen wie SAP NetWeaver-Portalen, bei denen oftmals die Konnektoren an die kundenspezifische
            Installation des Datentopfs angepasst werden müssen.   Haben Sie alle für die Recherche Ihrer Mitarbeiter
            relevanten Datenquellen auf dem Schirm? Vergessen Sie z.B. keine sinnvollen externen Quellen wie z.B.
            Fachinformationsanbieter oder Social Media-Inhalte - konzipieren Sie sich aber nicht &quot;zu Tode&quot;.
            Eine schrittweise Einführung - beginnend mit den einfach anzubindenden Datenquellen - versetzt Sie in nur
            kurzer Zeit in die Lage, eine Suchlösung produktiv zu nutzen. Eine sukzessive Erweiterung um weitere
            Datenquellen rundet dann die Lösung ab. &quot;Der Geschmack kommt beim Essen&quot; trifft hier voll und ganz
            zu. Um auch hier wieder ein Buzzword zu verwenden: Realisieren Sie diesen &quot;Quick Win&quot;, holen Sie
            Ihre User an Bord und bauen Sie dann die Lösung durch die Anbindung weiterer Datenquellen aus. Think big -
            but start small.   Nicht nur Quantität zählt   Doch nicht nur die Hinzunahme weiterer Datentöpfe, sondern
            auch das Ausrollen von z.B. &quot;Spezial-Suchapplikationen&quot; ist eine weitere sinnvolle
            Ausbaumöglichkeit: Die Enterprise Search-Lösung ist auch in der Lage, die vorhandene Suche in Applikationen
            wie dem Produktdatenmanagement-System qualitativ aufzuwerten oder zu ersetzen oder kann für
            Compliance-Zwecke eingesetzt werden. Gerade eben realisieren wir für die Ingenieurs- und
            Entwicklungsbereiche eines langjährigen Kunden eine für diese Usergruppe maßgeschneiderte Suchlösung - mit
            einer an die speziellen Rechercheszenarien dieser Benutzer angepassten Suchoberfläche. Das
            Indizierungsbackend ist dabei dasselbe, das auch die unternehmensweite Suche bedient. Dieses zentrales
            Indizierungs- und Such-Backend fungiert als zentrale Wissensdatenbank, mit der unterschiedliche
            Benutzergruppen im Unternehmen ganz einfach individuell optimierte Such-Sichten für ihre jeweilige tägliche
            Arbeit nutzen können.   Sehen Sie Suche mal aus diesem etwas abstrakteren Blickwinkel: Indizierungs- und
            Such-Backend sind leistungsfähige Infrastrukturkomponenten, die unterschiedlichste Aufgaben erledigen. Von
            der Such-/Trefferseite im Intranet über einen Point of Knowledge für Ihren Vertriebsmitarbeiter, wo er aus
            verschiedenen Systemen alle Informationen zu seinem Kunden in einem Dashboard präsentiert bekommt bis hin zu
            einer Lösung für die Analyse der Bewertungen Ihres Unternehmens, Ihres Managements oder Ihrer Produkte in
            sozialen Medien durch Sentiment-Analyse. Bei dem anfangs genannten Kundenbeispiel binden wir zu ausgewählten
            internen Inhalten auch externe Patent- und Literaturdatenbanken an. Der User sieht in der Suchmaske sogar,
            ob ein bestimmtes Fachbuch oder eine Studie bereits bestellt wurden und wer im Unternehmen das Medium gerade
            nutzt.   Diese vielfältigen Einsatzmöglichkeiten können nicht vorab im Rahmen einer
            Lasten-/Pflichtenheftdefinition umfänglich konzipiert und umgesetzt werden. Ein derartiges Unterfangen würde
            vermutlich scheitern. Die schrittweise Umsetzung ist hier der Königsweg. Was wichtig dabei ist: Die
            Iterationen nach einer Initial-Installation sind kleine, überschaubare Projekte.   Last, but not least  
            Erfolgreiche Suchprojekte zeichnet zudem aus, dass der Produktivstart eines neuen Suchdienstes intern
            vermarktet werden muss. Unsere Kunden sind hier sehr kreativ: Angefangen bei Anzeigen im Intranet, Beiträgen
            in internen Newslettern und Videos bis hin zu Preisausschreiben, bei denen die User bestimmte
            Suchfunktionalitäten ausprobieren müssen, bietet sich eine breite Palette an möglichen Maßnahmen.   Einige
            unserer Kunden haben auch pro Abteilung einen &quot;Peer-User&quot; benannt, der als direkter
            Ansprechpartner für anfängliche Fragen zur Verfügung steht. Benutzer sind oft überrascht, welche komplexen
            Suchanfragen doch ganz einfach von der Maschine &quot;beantwortet&quot; werden können. Ein direkter
            Ansprechpartner - zusätzlich zum generellen IT-Support - kann hier nützlich sein, damit die User die
            Möglichkeiten und Potentiale der neuen Lösung schnell &quot;hands-on&quot; kennenlernen. Auf der Tonspur
            kommt meist doch mehr rüber als im besten Erklärvideo.   Ein besonderes Anliegen ist mir noch der Hinweis
            auf die Notwendigkeit, den produktiven Betrieb nicht nur aus der Sicht eines IT-Administrators zu
            betrachten, sondern auch die Rolle eines zentralen &quot;Search Competence Centers&quot; zu definieren. Eine
            Person reicht hier schon aus: Jemand, bei dem alles zum Thema Suche und Content-Analyse zusammenläuft und
            der sich auch um die fachliche Administration kümmert. Der sich beispielsweise die Suchanfragen anschaut,
            die zu keinen Treffern führten und dann ggf. Synonyme einpflegt, damit die User bei der Suche nach dem
            &quot;Führerschein&quot; auch das erwartete Dokument finden, in dem leider nur der Begriff &quot;Fahrerlaubnis&quot;
            steht. Diese zentrale Stelle hilft, dass das &quot;Such-Rad&quot; nicht in jeder Abteilung neu erfunden
            werden muss, sondern dass es einen erfahrenen Ansprechpartner gibt, der die Themenhoheit innehat.   Was
            neben diesen grundsätzlichen Themen noch wichtig ist, um ein Suchprojekt erfolgreich zu machen, lesen Sie im
            nächsten Beitrag.   Der Autor Franz Kögl ist Mitgründer und -inhaber der Firma IntraFind Software AG und
            verfügt über mehr als 15 Jahre Erfahrung im Enterprise Search und Content Analytics Bereich. Mehr über Franz
            Kögl finden Sie in unserem Management Profil  
        </body>
    </Document>
    <Document>
        <url>http://www.intrafind.de/blog/von-der-enterprise-search-zur-insight-engine</url>
        <id>2C76225C22FA01F141DCE13A9EA29452</id>
        <title>Von der Enterprise Search zur Insight Engine</title>
        <body>Von der Enterprise Search zur Insight Engine Suchmaschinen als Treiber fürs Wissensmanagement Ob
            Kundenadressen, Sensordaten oder Produktfactsheets – auf den Unternehmensrechnern sammeln sich immer mehr
            Daten an. Die Digitalisierung macht es möglich: Durch Smartphones, verbesserte Webtechnologien oder Social
            Media kommunizieren Firmen heutzutage viel häufiger mit Kunden, externen Mitarbeitern oder Lieferanten,  als
            es noch vor einigen Jahren der Fall war. Die dabei gewonnen  Informationen sind eine wahre Goldgrube – und
            das gilt nicht nur für Kundendaten. Auch innerhalb eines Unternehmens sammelt sich Wissen an, das in Form
            von Dateien, Worddokumenten, PDF-Files oder Tabellen auf den einzelnen Rechnern gespeichert ist, ohne dass
            die Kollegen der anderen Abteilungen davon wissen und deren Potenzial wirklich ausschöpfen.     IoT als
            Treiber vom Datenwachstum Besonders das Internet der Dinge (IoT) wird das Datenwachstum vorantreiben. Wenn
            Sensoren miteinander kommunizieren und Daten miteinander austauschen, wird das die Produktion nachhaltig
            verändern.  Laut einer Studie von Gartner („1000 Data and Analytics Predictions Through 2020“) stammt in
            vier Jahren ein Viertel der Daten aus IoT-Anwendungen. Die Masse an Daten, die täglich über Wearables wie
            Smartwatches,  Schrittzähler und Pulsmesser an die Server gesendet werden, bergen schon heute ein enormes
            Potenzial für Marketingabteilungen von Sportartikelherstellern. Wer fundierte Geschäftsentscheidungen
            treffen will und wissen möchte, was Kunden wirklich wollen, muss die Daten in seinem Unternehmen
            analysieren. Dabei gilt es, möglichst viele und unterschiedliche Quellen anzuzapfen, um Daten und
            Informationen sinnvoll miteinander zu verknüpfen und in Relation zu setzen. Datenwachstum kurbelt Wettbewerb
            an Der Markt für Anwendungen rund um das Thema Datenanalyse boomt. Gartner prophezeit, dass 2018 rund 50
            Prozent der Gewinne vom Datengeschäft verursacht werden. Bis 2020 werden etwa 40 Prozent der Investments,
            die Unternehmen tätigen, die Bereiche Analytics und Business Intelligence betreffen. Bis 2018 werden sich
            außerdem 75 Prozent der Technologieunternehmen auf das Thema Datengenerierung und strategie-orientierte
            Analysieren fokussieren und entsprechende Produkte dazu anbieten. Das kurbelt auch den Wettbewerb an: Laut
            Gartner werden 2018 über die Hälfte der großen Konzerne nicht nur mit ihren Produkten miteinander in
            Wettbewerb treten, sondern auch um das Thema Datengewinnung  miteinander konkurrieren.   Suchmaschinen für
            strukturierte und unstrukturierte Daten Wenn es darum geht, Daten aufzuspüren und diese sinnvoll
            auszuwerten, stoßen Dokumentenmanagementsysteme oft an ihre Grenzen. Meist sind die Daten über verschiedene
            Systeme, Wikis oder Verzeichnisse verstreut und lagern sowohl auf Festplatten als auch in der Cloud.
            Unternehmen brauchen daher eigene Softwarelösungen, um die ständig wachsende Anzahl an strukturierten und
            unstrukturierten Datenmengen verarbeiten zu können und die aus den Daten gewonnenen Erkenntnisse für sich zu
            nutzen. Doch zuvor muss man die Daten, die man braucht, erst einmal finden. Nahezu jede Applikation verfügt
            mittlerweile über eine Suchfunktion, doch die Suche beschränkt sich hier meist nur auf die Bereiche
            innerhalb der Applikation oder des Dokuments. Um Daten unternehmensweit aufzuspüren, helfen spezielle
            Suchmaschinen, sogenannte Enterprise Search Anwendungen.  Über ein einziges Interface kann der Mitarbeiter
            die relevanten Informationen aufspüren,  die er für seine Arbeit braucht. Im Unterschied zu den bekannten
            Web-Suchmaschinen wie Google durchsuchen Enterprise Search Anwendung nicht nur eine Quelle, wie zum Beispiel
            das Web. Professionelle Unternehmenssuchmaschinen verfügen über Schnittstellen zu Unternehmensprogrammen wie
            E-Mail-Anwendungen, Content Management Systemen, Dokumentenmanagementsystemen oder Netzlaufwerken. Egal, ob
            der Content in der Cloud oder auf den Laufwerken gespeichert ist oder ob die Daten strukturiert oder
            unstrukturiert vorliegen: Die Suchmaschine holt sich die Informationen über Standardkonnektoren aus den
            einzelnen Applikationen und listet diese unter Berücksichtigung der Quelle auf. Der Nutzer bekommt so eine
            360-Grad-Ansicht auf alle wichtigen Informationen und weiß, ob die Information beispielsweise aus dem
            CRM-System, dem Mailprogramm oder aus einem Textdokument entstammt.  Die integrierte Rechteverwaltung
            gewährleistet, dass die Nutzer nur die Daten in der Trefferliste angezeigt bekommen, auf die sie befugt
            sind, zuzugreifen.  Suche als Grundlage für Analytics Professionelle Enterprise Search Anwendungen können
            Daten mittlerweile nicht nur aufspüren, sondern dem Nutzer auch bei der Analyse helfen.  Im Gegensatz zu
            einfachen Suchmaschinen stellen professionelle Enterprise Search Lösungen Zusammenhänge her, wie zum
            Beispiel zwischen Kundennummer und den dazugehörigen Dokumenten, die für den Kunden hinterlegt worden sind.
            Auf diese Weise kann der Nutzer sich schnell einen Überblick über ein bestimmtes Thema verschaffen und zum
            richtigen Zeitpunkt die richtige Entscheidung für sein Unternehmen, sein Produkt oder seine Marke treffen. 
            Die intelligente Suche mit Content Analytics Funktionen bietet für Unternehmen einen nicht zu
            unterschätzenden Mehrwert. Gartner prophezeit, dass schon Ende 2017 rund 25 Prozent der Mitarbeiter bis zu
            fünfmal am Tag mit Suchtechnologien in Berührung kommen werden. Die Suche wird Bestandteil von Business
            Intelligence und vom Geschäft mit den Daten: Bis 2018, so Gartner, werden sich Enterprise Search Anwendungen
            in die nächste Generation weiterentwickeln und eine Komponente von Business Intelligence und Analytics
            Plattformen sein. Die Suche wird damit zur Grundlage für Analytics Anwendungen. Von Enterprise Search zu
            Insight Engine Gartner spricht in diesem Zusammenhang von einer Entwicklung von „Enterprise Search“ zu
            „Insight Engine“. Die Insight Engine erlaubt es Unternehmen, eine einheitliche Sicht auf verschiedene Arten
            von Informationen zu bekommen, egal, ob sie aus der Cloud stammen oder von Third-Party-Applikationen oder
            aus den Netzwerken. Das erlaubt es, Daten aus ERP und CRM-Systemen gleichermaßen aufzuzeigen und zu
            kombinieren  ohne sie jedoch durcheinanderzubringen. Die Ursprungsquelle bleibt im Suchindex dabei immer
            erhalten. Gartner empfiehlt den Anbietern von Enterprise Search Software, die Anwendungen „natural“, „total“
            und „proactive“ zu gestalten. Durch Google Now, Yelp, Waze und Siri hat sich die natürlichsprachige Suche
            insbesondere auf den Smartphones durchgesetzt. Nutzer erwarten von ihrer Enterprise Search Anwendungen, dass
            sie diese Funktionalitäten ebenso unterstützt. Die semantische Suche wird sich neben der keywordbasierten
            Suche immer weiter durchsetzen. In zwei Jahren werden laut Gartner fast ein Drittel der Suchanfragen mit
            Fragewörtern wie „Was“, „Wer“, „Wie“ oder „Wann“ beginnen.  Mit „total“ appelliert Gartner an die Anbieter,
            möglichst viele Datenquellen anzubinden, damit der Anwender ein ganzheitliches Bild vom Sachverhalt
            vermittelt bekommt und Zusammenhänge schneller erkennt. Darüber hinaus soll der Enterprise Search Anbieter
            seine Nutzer „proaktiv“ über relevante und verwandte Themen auf dem Laufenden halten. Eine Suche wird dann
            nicht mehr notwendig sein. Der Nutzer bekommt alle Informationen, die er für seine Arbeit braucht, direkt
            auf sein Gerät per Push geschickt.   Der Autor Franz Kögl ist Mitgründer und -inhaber der Firma IntraFind
            Software AG und verfügt über mehr als 15 Jahre Erfahrung im Enterprise Search und Content Analytics Bereich.
            Mehr über Franz Kögl finden Sie in unserem Management Profil  
        </body>
    </Document>
    <Document>
        <url>http://www.intrafind.de/blog/hardware-war-gestern-sehen-wir-schon-immer-so</url>
        <id>F83B60E49363106D3F4A153ABDA6017C</id>
        <title>Hardware war gestern? Stimmt, das sehen wir schon immer so.</title>
        <body>Hardware war gestern? Stimmt, das sehen wir schon immer so. Google kündigt Google Search Appliance (GSA)
            ab.   Google hat seinen Gold-Partnern kürzlich mitgeteilt, sein seit 2002 auf dem Markt befindliches
            Hardware- Software Bundle, Google Search Appliance, ab 2017 nicht mehr verkaufen zu wollen. Als Hintergründe
            wurden genannt, dass all die Komponenten und Funktionalitäten, die Google derzeit seinen B2C- Anwendern über
            www.google.com bereitstellt, nicht mehr in eine einzelne Hardware integrierbar sind. Die Anforderungen der
            Anwender sind gewachsen und jeder erwartet, dass die Möglichkeiten, die die klassische Internetsuche bietet,
            auch in einer unternehmensweiten Suche mit der GSA verfügbar sind. Google möchte dieses Problem mit einer
            Cloud-basierten Anwendung für Unternehmen weltweit lösen. Was hat es mit der Search-Cloud von Google auf
            sich? Unsere Idee dazu sieht folgendermaßen aus:   Gerade für die Entwicklung der natürlichsprachigen Suche,
            wie wir sie aktuell von Apple‘s Siri kennen, ist es Google sehr von Nutzen, massenhaft Suchmuster bzw.
            Suchverhalten zu speichern, zu analysieren und auszuwerten, um letztendlich daraus „Intelligentes“ und
            „Verkaufbares“ ableiten zu können. Um diese Entwicklung voranzutreiben, benötigt Google ein enormes
            weltweites Informationspaket aus sämtlichen Unternehmensumfeldern.   Im klassischen B2C Anwenderbereich kann
            Google wie auch Facebook auf dieses beachtliche Wissen zurückgreifen, welches sich aus Suchanfragen,
            Inhalten auf Webseiten, Darstellung und Nutzung der Trefferlisten, Akzeptanz des organischen Rankings,
            Klickverhalten oder Lesefluss zusammensetzt. Es ergeben sich Abermillionen Profile, die in der Masse mit
            statistischer Verfahren ausgewertet werden. Das Ergebnis sind beispielsweise Anwendungen wie Google
            Translate, die mit der Markteinführung und Marktakzeptanz ganze Unternehmen verschwinden ließen. Vielen
            Anwendern ist diese Entwicklung unheimlich, da für nur sehr wenige einsehbar und kalkulierbar.   Doch ist
            jedes Unternehmen, welches heute die GSA integriert hat, bereit sein Unternehmenswissen, seine
            Unternehmensinformationen in der Cloud zu speichern und damit bereit das Risiko einzugehen, dass im
            worst-case Suchanfragen, Suchverhalten von Mitarbeitern für statistische Auswertungen genutzt werden
            könnten?   Um noch kurz auf die Hardware-Frage einzugehen. Viele Suchanbieter für eine unternehmensweite
            Suche setzten und setzen heute noch auf den Google-Trend und investierten in starre Hardware-Lösungen, die
            dem Kunden nur ganz begrenzt Einblick in Verfahren bieten. Wir von IntraFind sind auch der Meinung, dass
            eine Blackbox keine Lösung ist, da Skalierbarkeit und Performance damit ihre Grenzen haben. Und wie wir
            bereits von unseren Kunden wissen, ein hohes Maß an Individualprogrammierungen und Workarounds benötigen.
            Also, warum sollte man eine von Google als zukunftslos betrachtete Lösung - nämlich eine Search Appliance -
            durch eine Kopie ersetzen?   Wir interessieren uns für Ihre Haltung zu diesem Thema. 3 kurze Fragen haben
            wir vorbereitet &gt;&gt;   Der Autor Anke Mittelstädt ist seit 2012 bei IntraFind beschäftigt und
            verantwortet den Bereich Marketing und Public Relations. Sie blickt derzeit auf über 15 Jahre IT-Marketing
            Erfahrung zurück. In der Zeit vor IntraFind arbeitete Anke Mittelstädt als Manager Marketing EMEA &amp; APAC
            bei Iron Mountain Digital. Weitere 5 Jahre verbrachte sie bei dem Storage-Hersteller NetApp. In ihrer Rolle
            als Enterprise Marketing Manager erarbeitete sie erfolgreiche Marketing-Strategien und -Kampagnen und trug
            so zum Unternehmenserfolg bei. Die Diplom Betriebswirtin startete ihre berufliche Laufbahn im Marketing und
            Vertrieb der Infineon Technologies AG. Anke Mittelstädt lebt in München.
        </body>
    </Document>
    <Document>
        <url>http://www.intrafind.de/blog/sie-wollen-google-nicht-in-die-cloud-folgen-muessen-sie-auch-nicht</url>
        <id>C418F51693A1A5290F8CF51BD628D131</id>
        <title>Sie wollen Google nicht in die Cloud folgen? Müssen Sie auch nicht.</title>
        <body>Sie wollen Google nicht in die Cloud folgen? Müssen Sie auch nicht. IntraFind bietet Ihnen mit dem
            iFinder5 elastic eine interessante Alternative zur Google Search Appliance (GSA). Wenn eine Verlängerung der
            GSA in Ihrem Unternehmen ansteht oder Sie kurzfristig auf eine zukunftssichere und stabile Lösung wechseln
            möchten, ist der folgende Blogbeitrag genau das Richtige für Sie. Viele Kunden investierten in eine GSA
            aufgrund des Markennamens Google - und nicht, weil eine Appliance gefordert war. Aus Sicht von Google stellt
            sich die Situation folgendermaßen dar: Mit der Abkündigung wird die letzte verbliebene Nicht-Cloud-Lösung
            aus dem Portfolio genommen. Eine Weiterentwicklung des Produkts oder auch der Support werden künftig sicher
            nur noch halbherzig verfolgt. Die GSA kam überwiegend für die Suche auf Webseiten, in Online Shops und in
            Intranets zum Einsatz - ein Umfeld, in dem eine rechtegeprüfte Suche oft nicht zwingend durchgängig
            erforderlich ist. Falls Sie Ihre bestehende Lösung ablösen möchten, dann hat IntraFind mit dem iFinder als
            Virtual Appliance eine zeitgemäße Alternative parat. Installation und Konfiguration nehmen nicht mehr Zeit
            in Anspruch, die Administration ist übersichtlich und zeitsparend und eine bestehende Content
            Feed-Architektur für die Datenanlieferung an die GSA kann 1:1 beibehalten werden. Das garantieren wir Ihnen.
            Wir als deutsches Unternehmen stehen für Datensicherheit und Datenschutz. Der iFinder telefoniert nicht nach
            Hause und liefert auch deutlich mehr Suchfunktionalität als die GSA. Wir als deutsches Unternehmen liefern
            Ihnen die besten Ergebnisse oben in der Trefferliste, weil wir viel in unsere Entwicklung investiert haben,
            um insbesondere deutschsprachige Dokumente optimal verarbeiten zu können. Dies gilt auch für weitere 32
            Hauptwirtschaftssprachen. Wir zwingen unsere Kunden nicht, in die Cloud zu gehen. Der iFinder kann sowohl
            bei Ihnen in Ihrer eigenen IT-Infrastruktur oder auch in der Cloud betrieben werden. Wir richten uns nach
            Ihnen - und nicht umgekehrt: Sie müssen nicht gemäß den Vorstellungen des Anbieters handeln. Unlimitierte
            Skalierbarkeit, Lastverteilung und Real-Time-Indexierung garantieren, dass der iFinder immer mit Ihren
            Anforderungen mitwachsen kann. Wir verwenden Elasticsearch als Basis unserer modernen Such- und Indexlösung.
            Eine Kombination aus Hard- und Software macht noch lange keine gute, moderne Suche. Suche muss sicher sein,
            Suche muss einfach administrierbar sein, darauf verlassen sich über 1000 iFinder Kunden seit vielen Jahren.
            Suche muss aber auch sexy sein... Unsere Autovervollständigung in Kombination mit semantischen Technologien
            und dem Wissen um die Komplexität der deutschen Sprache begeistert durch eingebaute Tippfehlerkorrektur und
            sinntragende Erweiterungsvorschläge. Damit liefern wir schon hier den Kontext zu einer Suchanfrage aus den
            Dokumenteninhalten und helfen so, schnell das eine gesuchte Dokument zu finden oder unterstützen beim
            Stöbern in unbekannten Inhalten. Und dabei sind alle Informationen rechtegeprüft im Kontext des angemeldeten
            Benutzers. Bei der Suche in Shops werden z.B. alle gesetzten Suchfilter auch auf das Autocomplete
            angewendet. Sie sehen: rundherum eine echte und dazu noch preiswertere Alternative zu Ihrer GSA! Unser
            Angebot Schauen Sie sich den iFinder im Rahmen einer Websession oder eines aufgezeichneten Webcasts an. Wir
            besprechen Ihre individuelle Situation und erstellen für Sie ein maßgeschneidertes, individuelles Angebot.
            Und wenn wir wie so oft schon die GSA abgelöst haben, kümmern wir uns auch um die kostenfreie Entsorgung
            Ihres Elektroschrotts. Wir freuen uns auf Ihre Anfrage. Webcast vom 12. Mai 2016 (Aufzeichnung) Mehr
            Informationen zu diesem Thema gibt es in diesem Webcast: &quot;Sie suchen Ersatz für Ihre Google Search
            Appliance?&quot;   Der Autor Franz Kögl ist Mitgründer und -inhaber der Firma IntraFind Software AG und
            verfügt über mehr als 15 Jahre Erfahrung im Enterprise Search und Content Analytics Bereich. Mehr über Franz
            Kögl finden Sie in unserem Management Profil  
        </body>
    </Document>
    <Document>
        <url>
            http://www.intrafind.de/blog/suche-big-data-eventual-consistency-warum-suche-in-extrem-grossen-datenmengen-nicht-mehr-zwingend-konsistent-sein-kann
        </url>
        <id>AF178F1BB5E9771497F36929903EF57E</id>
        <title>Suche Big Data = Eventual Consistency. Teil 1 - Warum Suche in extrem großen Datenmengen nicht mehr
            zwingend konsistent sein kann
        </title>
        <body>Suche + Big Data = Eventual Consistency. Teil 1 - Warum Suche in extrem großen Datenmengen nicht mehr
            zwingend konsistent sein kann „640 kB sollten eigentlich genug für jeden sein.“ Viele von uns kennen
            vermutlich diesen Spruch, den Bill Gates angeblich im Jahr 1981 gemacht haben soll. Heute misst man den
            Arbeitsspeicher eines Rechners in Gigabytes und würde man sich zu der Aussage hinreißen lassen, dass z.B. 8
            Gigabyte Hauptspeicher doch sicherlich für alle Zeiten ausreichend sein sollten, könnte man sich sicher
            sein, dass auch diese Behauptung in einigen Jahren skurril wirken würde. Die weltweite Datenmenge wächst,
            unaufhaltsam und immer schneller. Der digitale Fußabdruck, den jeder von uns Jahr für Jahr hinterlässt, wird
            immer größer. Die gesamte Datenmenge, die die Menschheit vom Anbeginn der Geschichtsschreibung bis in das
            Jahr 1986 erzeugt hat, beträgt etwa 2.6 Exabytes (= 300.000x die größte Bibliothek dieser Welt). Im Jahr
            2016 entsteht jeden Tag eine neue Datenmenge dieser Größe. Diese Entwicklung ist exponentiell, das digitale
            Datenvolumen verdoppelt sich etwa alle zwei Jahre - mit Konsequenzen für die Verarbeitung dieser Datenflut.
            War es früher noch möglich, die in einem Unternehmen entstehenden Daten mit einem einzigen Rechner zu
            verarbeiten, ist dies heute in vielen Fällen nicht mehr denkbar. Zwar gilt (mit gewissen Einschränkungen)
            noch immer das Mooresche Gesetz, demzufolge sich die Komplexität integrierter Schaltkreise alle 24 Monate
            verdoppelt. Allerdings steigt die effektive Geschwindigkeit, mit der Daten verarbeitet werden können, nicht
            linear mit. Eine Revolution stellt die Solid State Disc da, die durch ihre extrem niedrigen Zugriffszeiten
            Verfahren ermöglicht, die lange Zeit nicht praktikabel waren. Doch gibt es Grenzen für die
            Leistungsfähigkeit eines einzelnen Rechners – physikalisch oder finanziell (vertikale Skalierung war schon
            immer teuer und wenig kosteneffektiv). Damit bleibt irgendwann nur noch die horizontale Skalierung als
            Lösungsansatz übrig: statt eines sehr teuren monolithischen Hochleistungsrechners setzt man auf einen
            Verbund vieler günstiger Rechner. In Summe erhält man so erheblich mehr Rechenleistung für das gleiche Geld.
            Wir folgern: Eine Big Data-fähige Technologie ist zwingend verteilt. Unser verteiltes System ist Big
            Data-fähig, kostengünstig und schnell. Damit ist doch eigentlich alles geklärt… oder etwa doch nicht? Dazu
            muss man wissen, dass für verteilte Systeme folgendes Gesetz gilt: CAP Theorem Ein verteiltes System kann
            maximal zwei der folgenden drei Eigenschaften aufweisen: (C) Consistent/Konsistent: Jeder Knoten sieht zu
            jedem Zeitpunkt die gleichen Daten. (A) Available/Verfügbar: Jede Anfrage an das System wird zeitnah
            beantwortet. (P) Partition tolerant/Partitionstolerant: Das System arbeitet auch bei Knoten- oder
            Kommunikationsausfällen weiter. Anschaulich wird dies anhand eines Beispiels: Über zwei Reisebüros A, B wird
            das letzte Ticket für einen Flug angeboten. Die beiden Standorte koordinieren den Verkauf über eine
            Telefonleitung, um sicherzustellen, dass der Platz nicht doppelt vergeben wird. Büro A wird nun von einem
            Kunden 1 betreten, der die Reise buchen möchte. Der Verkäufer aus A ruft kurz in Büro B an und informiert
            darüber, dass die Reise nun verkauft wird. Der Kunde verlässt A mit seinem exklusiven Ticket. Nun betritt
            ein Kunde 2 Büro B, möchte ebenfalls die Reise buchen, wird aber vom Verkäufer dort darüber informiert, dass
            der Flug ausgebucht ist. In diesem Szenario hat alles geklappt: Die Kunden mussten nicht warten (Bild 2 und
            4), und die Reise wurde exakt 1x verkauft (Bild 3). Aber es gab ja auch keinen Netzwerkausfall! Nun
            betrachten wir, was passiert, wenn die Telefonleitung ausgefallen ist: Kunde 1 möchte die Reise buchen, aber
            der Verkäufer von A kann Büro B nicht erreichen. Er hat nun drei Möglichkeiten: 1. Warten, bis die
            Telefonleitung wieder steht. Damit ist das System nicht mehr verfügbar. 2. Das Ticket nicht verkaufen. Das
            wäre eine Form der Konsistenzverletzung, denn tatsächlich existiert ja ein „verkaufsfähiges“ Ticket. 3. Das
            Ticket verkaufen und hoffen, dass es nicht gleichzeitig in Büro B verkauft wird (was ein eindeutiger Bruch
            der Konsistenz wäre). Dass das CAP-Theorem für eine verteilte Architektur immanent ist, bedeutet aber nicht,
            dass cleveres Vorgehen die Auswirkung in der Praxis nicht doch verbessern kann. So kann man in unserem
            Beispielcluster das Verhalten deutlich verbessern, wenn Reisebüro A das Ticket auch dann verkaufen darf,
            wenn B nicht erreichbar ist, wohingegen B in diesem Fall warten oder den Verkauf verweigern muss (A wird
            also eine Art privilegierter Knoten).Wir haben die konsistente Verfügbarkeit trotz komplettem
            Netzwerksausfall von 0% auf 50% gesteigert! Dennoch gilt folgende Aussage: Eine Big Data-fähige (und damit
            verteilte) Technologie (z.B. unternehmensweite Suche oder Content Analytics) muss zwangsläufig eine der
            Eigenschaften konsistent, verfügbar, partitionstolerant aufgeben. Was bedeutet das für eine Suchmaschine
            bzw. für Enterprise Search? Oder anders formuliert: Worauf können wir verzichten, und welche Auswirkungen
            hat dies in der Praxis? Antworten darauf liefert Ihnen in Kürze unser nächster Blogartikel. Der Autor Jörg
            Viechtbauer ist seit Oktober 2012 als Software Architekt bei der IntraFind Software AG beschäftigt. Nach
            seinem Informatikstudium an der RWTH Aachen war Jörg Viechtbauer in der Softwareentwicklung namhafter
            deutscher Softwareunternehmen tätig und sammelte umfassende Praxiserfahrung in der Konzeption und
            Implementierung von Enterprise Search-Lösungen. Aktuell gilt sein Interesse besonders der Entwicklung von
            Suchlösungen, Services &amp; Plugins auf Basis von Elasticsearch.
        </body>
    </Document>
    <Document>
        <url>http://www.intrafind.de/blog/die-suche-nach-der-perfekten-suche-im-intranet</url>
        <id>C4959084B8952EBF4F87ACFCA7B810FD</id>
        <title>Die Suche nach der perfekten Suche im Intranet</title>
        <body>Die Suche nach der perfekten Suche im Intranet   In meiner langjährigen Tätigkeit als IT-Berater, der sich
            mit Technologien und Lösungen für Wissens-Arbeiter oder neudeutsch &quot;Knowledge Worker&quot; beschäftigt,
            musste ich immer wieder feststellen, dass das Thema „Suche im Unternehmen“ - über alle Branchen und
            Unternehmensgrößen hinweg - eher stiefmütterlich behandelt wird.   Ich kenne wenige Unternehmen, in denen
            „Suche“ ein klar definierter Teil der Unternehmens- und IT-Strategie ist.   Diese Tatsache ist für mich
            verwunderlich, besonders in Anbetracht dessen, dass ich mich beruflich lange Zeit in den Themenfeldern
            Collaboration, Knowledge Management und Dokumentenmanagement bewegte. Hierfür kommen in der Regel in
            Unternehmen sehr heterogene Systeme zum Einsatz, die durchaus nicht einfach miteinander gekoppelt werden
            können. Aber gerade im Umfeld von Systemen, die Daten verwalten und für Anwender bereitstellen, sind
            Recherchen Alltag und müssen im Unternehmensalltag häufig über verschiedenste Systeme gemacht werden, um ein
            vollständiges Bild zu einem Sachverhalt oder Antworten auf aktuelle Fragestellungen zu erhalten.   Natürlich
            bieten die genannten Systeme inzwischen in der Regel eine Möglichkeit zum Auffinden von Inhalten an, aber
            eben meist nur in Form einer applikationsspezifischen Suche innerhalb des Systems selbst.   Als Folge daraus
            wird eine Recherche über mehrere Systeme hinweg zu einem zeit- und nervenraubenden Akt. Dieser resultiert
            oft auch noch darin, dass der geplagte Mitarbeiter die gefundenen Ergebnisse aus den verschiedenen Systemen
            - mit allen Medienbrüchen behaftet - häufig noch manuell zu einem Ergebnis zusammenfassen muss. Dies ist
            übrigens einer der Hauptgründe, warum sich so viele Dateien in unzähligen Kopien auf lokalen Festplatten,
            Benutzerlaufwerken, in E-Mail-Ordnern etc. wiederfinden.   Die geschilderte Problematik war Motivation genug
            für mich, Ihnen mit dieser neuen Blogreihe das Thema „Suche“ näher zu bringen und den einen oder anderen
            Aspekt genauer zu beleuchten, der Augenmerk verdient, wenn man sich mit dem Thema „Suchen und Finden von
            Informationen“ auseinander setzt.   Das Für und Wider einer unternehmensweiten Suche   Wenn ich mit meinen
            Kunden diskutiere, warum Suchprojekte nur zögerlich in Angriff genommen werden, höre ich die
            unterschiedlichsten Gründe.   Die Ansicht, dass die Mitarbeiter „eine Suche brauchen könnten&quot;, teilen
            fast alle.   Oft steht die Befürchtung im Raum, dass die Einführung einer Unternehmenssuche eine
            anspruchsvolle Aufgabe sei, vergleichbar mit der Einführung eines Intranet-Portals.   Und in vielen
            Unternehmen ist den verantwortlichen Entscheidern der echte Mehrwert einer hochwertigen Unternehmenssuche
            nicht wirklich klar. Der &quot;Return of Investment (ROI)&quot; lässt sich schwer vorab beziffern und damit
            wird die Investition in die Anschaffung einer Enterprise Search Software sowie in die Projekte gescheut.  
            Und hier nähern wir uns aus meiner Sicht dem entscheidenden Punkt.   Für mich steht außer Frage, dass mit
            Suche eine Menge Geld eingespart und verdient werden kann. Sehen Sie sich beispielsweise nur die
            kommerziellen, frei zugänglichen Medienportale und Handelsplattformen im Internet an: die guten basieren
            ausnahmslos alle auf Suche.   Im Unternehmensumfeld kommt es hingegen auf den konkreten Anwendungsfall oder
            neudeutsch &quot;Use Case&quot; an, für den eine ROI-Betrachtung gemacht wird - und der ist nicht immer
            sofort ersichtlich. Ich werde Ihnen aber im Verlauf dieser Blogreihe immer wieder Anregungen für mögliche
            Use Cases für Suche in Ihrem Unternehmen geben.   Das Bild des EINEN Intranet-Portals als „single point of
            information“, das viele im Kopf haben und auf die Suche übertragen, stellt eine gute Ausgangsbasis dar.  
            Denn wenn eine unternehmensweite Suche in dem EINEN Suchportal mündet oder in das Intranet-Portal des
            Unternehmens integriert wird, findet sich der Benutzer bestätigt, dass er über das Portal alles findet, was
            er für sein (berufliches) Leben braucht. Er wird es als wichtiges Recherchewerkzeug nutzen und dies führt zu
            einer nachhaltigen Anwenderakzeptanz.   Natürlich hat eine unternehmensweite Suche über die Portal-Suche
            hinaus weitaus mehr Facetten, die in vielen Fällen sogar für Benutzer völlig transparent sind und daher die
            Argumentation für die Einführung einer unternehmensweiten Suche stützen. Es gibt unzählige Möglichkeiten,
            Suche im Unternehmen zu platzieren und sie in Prozesse oder Anwendungen zu integrieren. Ein Kunde aus der
            Medien- und Verlagsbranche beispielsweise nutzt Suche als automatisierte Redaktionsunterstützung in einem
            Content-Management-System: durch das Auffinden von inhaltlich ähnlichen Artikeln zum geschriebenen Text wird
            verhindert, dass Artikel zu demselben Sachverhalt doppelt erstellt und veröffentlicht werden.   Die Vorteile
            von Suche im Unternehmen sind vielfältig: wertvolle Zeitersparnis, mehr Komfort und Effizienz für die
            Benutzer, aber auch eine höhere Qualität und Nutzbarkeit neu erstellter sowie historischer
            Unternehmensdaten. Entscheidend ist, dass mit einer guten Suchmaschine all diese Anwendungsszenarien
            realisiert werden können, da die Suche überall auf die gleiche Art und Weise funktioniert.   Im nächsten
            Beitrag erfahren Sie, was es bei der Planung und Umsetzung von Enterprise Search-Projekten zu beachten gibt.
              Der Autor Patrick Baldi verfügt über mehr als 20 Jahre Erfahrung als Berater in der IT-Branche. Lange
            Jahre war er im Consulting und technischen Vertrieb der Microsoft Deutschland GmbH tätig, wo er
            schwerpunktmäßig Kunden rund um die Themen Architekturen und Anwendungen im Bereich Collaboration, Intranet,
            Knowledge Management und Suche betreute. &quot;Meine Leidenschaft für das Thema Suche wuchs aus der
            Erkenntnis, dass das Potential und die Möglichkeiten von Suche im Unternehmensumfeld immer noch weitgehend
            verkannt werden.&quot;
        </body>
    </Document>
    <Document>
        <url>http://www.intrafind.de/blog/language-identification-and-language-chunking-en</url>
        <id>0E080E5C8D42FE6F2DBE6FF25165C977</id>
        <title>Language Identification and Language Chunking</title>
        <body>Language Identification and Language Chunking   Identifying the language of a given text is a crucial
            preprocessing step for almost all text analysis methods. It is considered as a solved problem since more
            than 20 years. Available solutions build on the simple observation that for all languages typical letter
            sequences (letter n-grams) exist, that occur significantly more frequent in this language than in other
            languages. Even the frequency of individual letters differs significantly for European languages. The letter
            “e” is the most frequent single letter in most European languages, but for Polish the most frequent letter
            is “a”.   Figure 1: All letter n-grams of length 1 (unigram) to 4 (tetra gram) for &quot;TEXT&quot; with
            &quot;_&quot; as marker for start and end of text Tika: Cavnar &amp; Trenkle Approach All existing solutions
            for language identification build some kind of letter n-gram model or profile for every language, which is
            then compared with the letter n-gram profile of the text that is analyzed. A very simple approach was
            described in [C&amp;T94] and is still used by the Apache Tika language identifier. The profiles generated in
            their approach simply consist of the most frequent letter n-grams sorted according to their frequency and
            the comparison of text and language profiles is based on a heuristic measure which compares the rank of
            letter n-grams in both profiles:   Figure 2: Apache Tika / C&amp;T94 Approach   As stated already in the
            beginning, language identification is considered a solved problem. For text that is longer than 100
            characters, there are methods that identify the correct language with an accuracy of 99%. This means that we
            might still have ten thousand incorrectly classified documents in a 1 million documents collection. So there
            is definitely still room for improvement. Furthermore, accuracy of established approaches drops considerably
            if we consider shorter text chunks like tweets or search engine queries.   Since there is an increasing need
            for accurate language identification for short text, research activity in the field has increased during the
            last 5 years. At IntraFind we recently also decided to take a closer look at language identification. I
            think we got some interesting findings.   First we tested the simple Apache Tika implementation on the LIGA
            benchmark (language identifier benchmark on twitter data, see [T&amp;P11]). We got an accuracy of 97.7%,
            much better than the 93% reported by [T&amp;P11] with the Tika implementation. In contrast to [T&amp;P11] we
            used our own training data set which is much bigger than the data set used by [T&amp;P11] (they used cross
            validation on the benchmark itself). Our result is even slightly better than the result achieved by [T&amp;P11]
            with their own very sophisticated new approach. This shows that using big sets of training data is
            important.   Google: Naïve Bayes Established approaches for language identification such as the Naïve Bayes
            approaches used by the current Google implementations (e.g. the Chromium Language Detector used in the
            Chrome Browser) use a fixed n-gram length. Usually an n-gram length of 4 is used for European languages and
            a length of 2 is used for Chinese or Japanese. For a comparison see [McC11].   Latest Developments Latest
            developments that try to improve language identification accuracy consider longer n-grams. This is motivated
            by the fact, that languages are not only characterized by typical short letter n-grams. Humans recognize
            languages because they know frequent words and those words are usually longer than 4 letters.   We tested a
            language identifier based simply on counting the number of matching words for each language using our
            morphological lexica and achieved an accuracy of 99.4% on the LIGA benchmark. This is better than the best
            result reported on that benchmark (as far as we know) and it confirms the idea that a fixed short n-gram
            length is incompatible with further improvements in language identification. However, language
            identification is considered as a preprocessing step which should be very fast. Lexicon lookup is not a
            practical solution, especially since good morphological lexica might not be available for all languages.  
            Markov Chain Standard Naïve Bayes n-gram approaches do not allow to consider variable-length n-grams in a
            straightforward way. We therefore decided to use a Markov Chain model [Dun94], which actually allows to use
            variable-length n-grams by using a back-off approach for estimating letter n-gram probabilities. With our
            current implementation we achieve an accuracy of 99.2% on the LIGA benchmark, which is identical to the best
            published result that we know.   Furthermore our Java-based implementation is very fast. On a current
            notebook processor with one thread we achieve a throughput of 5.5 MB/sec (counting each character as one
            byte), which is approximately 5 times faster than the Apache Tika implementation and comparable to the
            performance of the fastest existing language identifier (Google’s Chromium language detector).   Language
            identification is not the only goal we wanted to achieve with our new language identifier. It can be used to
            automatically identify chunks with the same language within a given text and since we have a high accuracy
            for short text we are even able to identify short chunks (see figure 3).   German Tweet: “Ich mag den Song
            la vie en rose sehr gerne.” Chunk 1: Ich mag den Song language: de Chunk 2: la vie en rose language: fr
            Chunk 3: sehr gerne language: de Fig 3: Language Chunking Example     Bibliography:   &quot;Gram-Based Text
            Categorization&quot;, William B. Cavnar , John M. Trenkle, In Proceedings of SDAIR-94, 3rd Annual Symposium
            on Document Analysis and Information Retrieval, 1994 [T&amp;P11] “Graph-based n-gram language identification
            on short texts.”, Erik Tromp and Mykola Pechenizkiy, In Proceedings of Benelearn, The Hague, Netherlands,
            2011 [Dun94] “Statistical Identification of Language”, Ted Dunning, Technical Report New Mexico State
            University, 1994 [McC11] “Accuracy and performance of Google&apos;s Compact Language Detector”, Michael
            McCandless Blog: http://blog.mikemccandless.com/2011/10/accuracy-and-performance-of-googles.html Der Autor
            Dr. Christoph Goller verfügt über 15 Jahre Erfahrung im Enterprise Search Markt. Er promovierte in
            Computerwissenschaften an der Technischen Universität in München und arbeitete im Anschluss in einigen
            wissenschaftlichen Projekten zu Themenbereich Künstliche Intelligenz, Machine Learning sowie Neuronale
            Netze.   Seit 2002 leitet er den Bereich Forschung bei IntraFind und verantwortet IntraFind&apos;s
            Kern-Suche- und Content Analyse Technologien. Zwischen 2003 und 2007 arbeitete Christoph aktiv als Committer
            in Apache&apos;s Open Source Projekt Lucene.    
        </body>
    </Document>
    <Document>
        <url>
            http://www.intrafind.de/blog/suche-big-data-eventual-consistency-teil-2-warum-suche-in-extrem-grossen-datenmengen-nicht-mehr-zwingend-konsistent-sein-kann
        </url>
        <id>8EA44AAE6B11844BB5A0B5FF75B6DD46</id>
        <title>Suche Big Data = Eventual Consistency Teil 2 - Warum Suche in extrem großen Datenmengen nicht mehr
            zwingend konsistent sein kann
        </title>
        <body>Suche + Big Data = Eventual Consistency Teil 2 - Warum Suche in extrem großen Datenmengen nicht mehr
            zwingend konsistent sein kann Fortsetzung Blogbeitrag von Jörg Viechtbauer vom 19.05.2015 Der erste Teil des
            letzten Blogbeitrages endete mit der Feststellung: Eine Big Data-fähige (und damit verteilte) Technologie
            muss zwangsläufig eine der Eigenschaften konsistent, verfügbar, partitionstolerant aufgeben. Teil 2 soll nun
            Antworten auf folgende Fragen liefern: Was bedeutet das für eine Suchmaschine? Oder anders formuliert:
            Worauf können wir verzichten, und welche Auswirkungen hat dies in der Praxis? Die Partitionstoleranz ist
            kaum verhandelbar, denn sie wird umso wichtiger, je höher man skalieren möchte. In einem Verbund von 300
            Rechnern ist zu erwarten, dass im Schnitt jede Woche mindestens eine Maschine ausfällt. Wie wichtig ist die
            Verfügbarkeit? 2005 wurde bei Google die Trefferliste experimentell auf 30 Einträge vergrößert – mit dem
            Ergebnis, dass der Traffic um 20% einbrach. Als Ursache wurde der Anstieg der Suchzeit von 400 auf 900
            Millisekunden identifiziert. Einmal mehr gilt: Zeit ist Geld! Selbst eine geringfügige Verschlechterung der
            Verfügbarkeit hätte Google Milliarden gekostet. Somit sind Abstriche bei der Verfügbarkeit nicht akzeptabel.
            Bleibt also unabwendbar nur noch die Konsistenz als verzichtbare Eigenschaft. Verlust der Konsistenz – das
            klingt beunruhigender als es ist, betrifft die Inkonsistenz doch lediglich die während des Netzwerkausfalls
            aufgelaufenen Operationen und somit nur einen Bruchteil der Gesamtdatenmenge. Was bedeutet der Verlust von
            Konsistenz für Suche / Enterprise Search? Bei einer Suchmaschine kann sich dies z.B. so bemerkbar machen,
            dass neu indizierte Dokumente erst zeitverzögert gefunden werden können oder – wenn das System
            beispielsweise durch Replikation hochverfügbar (und damit noch verteilter) gemacht wird – temporär
            alternierende Ergebnislisten (mal wird das neue Dokument gefunden und dann wieder nicht). Vielleicht sind
            auch die Facettenwerte nicht exakt oder das Dokument wird zunächst nicht präzise gerankt… die konkreten
            Auswirkungen hängen von der Implementierung ab, sind bei einer Suchmaschine aber in den allermeisten Fällen
            kaum wahrnehmbar und stellen daher keine Einschränkung der Funktionalität dar. Oder haben Sie schon einmal
            bemerkt, dass die Google Suche inkonsistent ist? Und wenn ja, hat es Sie gestört? Wenn die unmittelbare
            Konsistenz schon nicht vollständig garantiert werden kann, gibt es in den meisten Architekturen doch eine
            gewisse Annäherung daran, die mit dem englischen Begriff eventual consistency bezeichnet wird. Ein System
            heißt eventual consistent, wenn es garantiert, dass es zu einem konsistenten Zustand konvergiert, d.h.
            irgendwann einmal konsistent wird. Auch hier kann man keine pauschalen Aussagen darüber treffen, was dies
            konkret bedeutet und wie lange es dauern wird, bis dieser Zustand erreicht ist. In der Regel wird das System
            nach Wiederherstellung der Konnektivität die Konsistenz überprüfen und gegebenenfalls mit einer Art
            Reparatur beginnen. Sollen in unserem Beispiel die Reisebüros verfügbar und partitionstolerant sein,
            verkaufen die Verkäufer das Ticket auch dann, wenn die Leitung zusammengebrochen ist, versuchen danach aber
            jede Minute, den Kollegen zu erreichen. Sobald dies gelingt, wird der Konsistenzzustand überprüft: Wurde das
            Ticket nicht oder nur einmal verkauft, ist alles in Ordnung. Wurde das Ticket hingegen fälschlicherweise
            doppelt verkauft, erfolgt nun die Wiederherstellung eines gültigen Zustands. Ein Kunde wird über den Fehler
            unterrichtet, erhält eine Rückerstattung und als Kompensation z.B. eine gute Flasche Wein. FAZIT Die
            digitale Datenflut wächst exponentiell und schneller als sich die Rechenleistung eines einzelnen Rechners
            entwickelt. Eine Big Data-fähige Architektur muss daher zwangsläufig horizontal skalieren, Somit unterliegt
            sie dem CAP-Theorem, demzufolge ein verteiltes System nicht gleichzeitig sowohl konsistent, verfügbar als
            auch partitionstolerant sein kann. Bei einer Suchmaschine wird die Konsistenz – als der am wenigsten ins
            Gewicht fallende Faktor – durch eine schwächeren Konsistenzgarantie ersetzt: der eventual consistency. Diese
            gewährleistet, dass das System – so schon nicht unmittelbar – doch irgendwann einmal konsistent sein wird.
            Der Autor Jörg Viechtbauer ist seit Oktober 2012 als Software Architekt bei der IntraFind Software AG
            beschäftigt. Nach seinem Informatikstudium an der RWTH Aachen war Jörg Viechtbauer in der
            Softwareentwicklung namhafter deutscher Softwareunternehmen tätig und sammelte umfassende Praxiserfahrung in
            der Konzeption und Implementierung von Enterprise Search-Lösungen. Aktuell gilt sein Interesse besonders der
            Entwicklung von Suchlösungen, Services &amp; Plugins auf Basis von Elasticsearch.
        </body>
    </Document>
    <Document>
        <url>http://www.intrafind.de/blog/tagging-mehrwerte-durch-metadaten</url>
        <id>6689BF90707AA359E2137AEED3FCC1C0</id>
        <title>Tagging – Mehrwerte durch Metadaten</title>
        <body>Tagging – Mehrwerte durch Metadaten   Metadaten sind per Definition Daten, welche andere Daten
            beschreiben. Beispiele hierfür sind Informationen wie Autor, Erstellungsdatum, Ort der Erstellung eines
            Dokuments oder auch Textbausteine, die besonders relevant sind. Bei IntraFind unterscheiden wir strukturelle
            Metadaten von semantischen Metadaten - nicht nur weil sie unterschiedlich entstehen, sondern auch weil sie
            unterschiedlich genutzt werden können.     Strukturelle Metadaten entspringen direkt der Struktur eines
            Dokuments (oder liegen bereits strukturiert vor). Beispiele für strukturelle Metadaten sind Dateityp oder
            Erstellungsdatum eines Dokuments.   Semantische Metadaten werden über mehr oder weniger aufwändige Text
            Analytics-Verfahren aus dem Inhalt eines Dokumentes gewonnen. Beispiele hierfür sind die in einem Text
            erwähnten Namen von Personen oder Orten oder die in einem Bild erkannten Gesichter. Semantische Metadaten
            ermöglichen einen inhaltlichen Überblick zu einzelnen Dokumenten ohne umfangreiche Detailbetrachtungen.
            Darüber hinaus können solche Metadaten im Kontext einer Suche dokumentenübergreifend kombiniert werden und
            somit einen wichtigen Einblick in die Unternehmensdaten gewähren.   Probleme bei der Erzeugung semantischer
            Metadaten Semantische Metadaten entstehen dort, wo redaktionelle Arbeit geleistet wird – beispielsweise in
            der Redaktion einer Zeitung. Üblicherweise sind die Redakteure als Inhaltsproduzenten auch dafür
            verantwortlich, die Inhalte mit semantischen Metadaten zu versehen; beispielsweise indem sie eine
            Ressorteinordnung („Politik“, „Sport“, &quot;Wirtschaft&quot;) vornehmen oder besonders relevante
            Schlagwörter („Eurokrise“, &quot;US-Wahlkampf&quot;) ihres Textes dem Inhalt als sogenannte Tags hinzufügen.
            Die durch die manuelle und individuelle Verschlagwortung erzeugte Heterogenität erschwert eine
            dokumentenübergreifende Verwertung der Metadaten (autorenübergreifende Verlinkungen von Texten sind nicht
            mehr möglich).   Dieses Problem ist in der Bibliothekswissenschaft unter dem englischen Fachbegriff
            inter-indexer consistency bekannt und der unvermeidlichen Subjektivität der Interpretation des Einzelnen
            geschuldet. Dies war einer der Gründe, warum unser Kunde ZEIT Online die Unterstützung von IntraFind gesucht
            hatte und bei der Verschlagwortung von Nachrichten seit Jahren erfolgreich auf unser Produkt Tagging Service
            setzt. Durch die automatische Verschlagwortung kann eine homogene Schlagwortlandschaft gewährleistet werden.
            [Pflugfelder und Drongowski 2012]   Eine Meta-Frage stellt sich Ein Anwendungsszenario von Metadaten wurde
            schon erläutert – Metadaten, besonders semantische, ermöglichen eine automatische Verlinkung von Dokumenten
            in einem Datenbestand. Die dadurch erzeugten Informationsnetze eignen sich hervorragend als Navigationshilfe
            und bringen im Fall eines über das Internet frei zugänglichen Informationsbestandes, intelligent
            konsolidiert, einen enormen Vorteil für dessen Suchmaschinenoptimierung (SEO).   Bei der Verwendung von
            Metadaten in der Suche sollte zunächst folgende Frage beantwortet werden: Welche Arten/Kategorien von
            Fragestellungen haben Benutzer einer Suchmaschine und wie können Metadaten sie bei der Abdeckung ihres
            Informationsbedarfes am besten unterstützen? Dazu ein nicht ganz ernst gemeintes Zitat des ehemaligen
            Verteidigungsministers der USA, Donald Rumsfeld [Rumsfeld 2012]:   &quot;There are known knowns; there are
            things we know we know. We also know there are known unknowns; that is to say we know there are some things
            we do not know. But there are also unknown unknowns – there are things we do not know we don&apos;t know.&quot;
              Rhetorisch ist noch viel Luft nach oben, aber rein formal ist das Zitat schlüssig (obwohl unvollständig:
            eine Kategorie, die der unknown knowns, wurde nicht behandelt).   Um das Potenzial von Metadaten zu
            illustrieren, konzentrieren wir uns auf zwei Fragenkategorien - die gezielte Suche (known knowns) und die
            explorative Suche (unknown unknowns).   Die gezielte Suche (known knowns) Einer gezielten Suche liegen
            Fragestellungen wie diese zugrunde: „Wo ist denn die Präsentation, die Herr Müller letzte Woche geschickt
            hat, und die auch an Herrn Meyer ging?“.   Die Wissenslandkarte in unserem Enterprise Search Produkt iFinder
            stellt eine Benutzeroberfläche für gezielte Suchen dar. Dabei wird nicht wie üblich über die manuelle
            iterative Verfeinerung der Suchanfrage (query) die Treffermenge weiter eingeschränkt, bis man zum
            erwünschten Treffer gelangt. Stattdessen wird über eine grobe Suchanfrage (im Extremfall die &quot;*-Suche&quot;,
            welche alle Dokumente eines Datenbestandes als Treffer liefert) zunächst eine große Treffermenge
            bereitgestellt, die garantiert den gewünschten Treffer beinhaltet. Anschließend wird durch das Anklicken von
            auf Metadaten basierenden Filterelementen (Facetten) die Treffermenge so lange eingeschränkt, bis der
            gewünschte Treffer gefunden wird.   Im obigen Beispiel würde der Benutzer folgendermaßen vorgehen:   der
            Suchfilter &quot;Dateityp&quot; wird auf den Wert &quot;Präsentation&quot; begrenzt, der Suchfilter &quot;Absender&quot;
            wird auf den Wert &quot;Herr Müller&quot; festgelegt, der Suchfilter &quot;Änderungsdatum&quot; wird auf den
            Zeitrahmen &quot;letzte Woche&quot; eingeschränkt und der Suchfilter &quot;CC-Adressat&quot; wird auf den
            Wert &quot;Herr Meyer&quot; limitiert. Bei jedem Schritt verändert sich die gezeigte Treffermenge und sobald
            das gesuchte Dokument erscheint, kann der Recherchevorgang erfolgreich beendet werden.   Die gezielte Suche
            bringt man als Benutzer nicht zwangsläufig in Verbindung mit einer Suchmaschine – schließlich ist diese Art
            von Suche bei Websuchmaschinen nur bedingt durchführbar. Google und Co. sammeln für einzelne Webseiten nur
            sehr wenige vom Benutzer sichtbare und verwertbare Metadaten. Außerdem sind solche Suchmaschinen nicht
            vollständig – sie indizieren nicht das gesamte Internet. Die geschilderten Suchaufträge stehen in der Regel
            dann an, wenn man in den Daten des eigenen Rechners oder im Dateisystem der Firma ein bestimmtes Dokument
            sucht. Dabei ist man auf die eigene Organisationsdisziplin und die der Kollegen angewiesen. Solche Suchen
            beanspruchen viel Zeit und enden oft ohne Erfolg. Im iFinder lässt sich dieses Anwendungsbeispiel als Teil
            der Suche abdecken.   Die explorative Suche (unknown unknowns) Die explorative Suche ist eine sehr
            aufwändige Art von Suche. Der Informationsbedarf ist dabei nicht einfach zu verbalisieren oder lässt sich
            durch das Finden eines einzelnen Dokumentes nicht abdecken. Beispiele für Fragestellungen, welche zu einer
            explorativen Suche leiten würden, sind: „Worum ging es denn im Projekt XYZ?“, „Was hat Herr Müller in dieser
            Firma gemacht?“, „Welche sozialen Netzwerke zwischen Personen und Firmen lassen sich aus einem
            beschlagnahmten Datenbestand ableiten?“.   Die Antworten zu derartigen Fragen stehen selten konsolidiert in
            einem Dokument zur Verfügung, sondern müssen aus unterschiedlichen Dokumenten und Quellen mühsam
            zusammengetragen werden. Durch die Kombination von semantischen Metadaten und Suche kann sich ein Benutzer
            sehr schnell einen Überblick zu solchen Fragestellungen verschaffen. Eine Möglichkeit ist die Bereitstellung
            von Tag Clouds, die dem Benutzer auf einen Blick besonders relevante Begriffe zur Treffermenge seiner
            Suchanfrage liefern.   Explorative Suchen fallen beispielsweise bei der Einarbeitung in neue Themen, der
            Einarbeitung neuer Kollegen oder im Rahmen von Ermittlungen bzw. im Anwendungsbeispiel E-Discovery an.
            Semantische Metadaten sind eine Voraussetzung für hochwertige Suchergebnisse im Rahmen einer explorativen
            Suche und durch den Wegfall sonst sehr aufwändiger Recherchearbeiten ein Garant für einen schnellen ROI!  
            Zusammenfassung Durch den Einsatz des IntraFind Tagging Service lassen sich zusätzlich zu den strukturellen
            Metadaten automatisch hochwertige semantische Metadaten ohne subjektiven Bias erzeugen, den einzelnen
            Dokumenten zuordnen und als Filterelemente in der Suche verwenden. Über die Wissenslandkarten im iFinder
            kann der Benutzer sowohl strukturelle als auch semantische Metadaten verwenden, um gezielte Suchbedürfnisse
            zu befriedigen.   Durch die Gewinnung semantischer Metadaten bereitet der Tagging Service den Weg zur
            explorativen Suche. Metadaten gewähren einen detaillierteren Einblick in einen Informationsbestand,
            optimieren Rechercheprozesse und schaffen kürzere und unkompliziertere Wege zur gewünschten Information.    
            Bibliographie: [Pflugfelder und Drongowski 2012] – Bernhard Pflugfelder und Ron Drongowski, Semantische
            Suche @ ZEIT Online, KnowTech -- 14. Kongress zum Wissensmanagement in Unternehmen und Organisationen &quot;Neue
            Horizonte für das Unternehmenswissen -- Social Media, Collaboration, Mobility&quot;. M. Bentele, N. Gronau,
            P. Schütt, M. Weber (Hrsg.). Stuttgart, 2012 [Rumsfeld 2012] - Donald Henry Rumsfeld in einem News Briefing
            des US-Verteidigungsministeriums am 12. Februar 2002 -- https://de.wikiquote.org/wiki/Donald_Rumsfeld (Abruf
            am 22.07.2015) Der Autor Breno Faria, Head of Development, ist seit 2012 für die IntraFind Software AG
            tätig. Seit den späten 2000er Jahren beschäftigt er sich intensiv mit den Themen Content Analytics und
            Information Retrieval. 2015 übernahm er die Rolle des Entwicklungsleiters bei IntraFind.   Im Rahmen von
            Veranstaltungen, z.B. Berlin Buzzwords 2014 oder &quot;IntraFind Enterprise Search Day 2015&quot;, referiert
            er regelmäßig über neue Technologien oder präsentiert innovative Lösungen aus IntraFind Kundenprojekten.
        </body>
    </Document>
    <Document>
        <url>
            http://www.intrafind.de/blog/cognitive-computing-alter-wein-in-neuen-schlaeuchen-ein-plaedoyer-fuer-den-boykott-staendig-neuer-it-buzzwords
        </url>
        <id>F8904B988805A5A270A68854B2F745F6</id>
        <title>&quot;Cognitive Computing&quot; - alter Wein in neuen Schläuchen? Ein Plädoyer für den Boykott ständig
            neuer IT-Buzzwords.
        </title>
        <body>&quot;Cognitive Computing&quot; - alter Wein in neuen Schläuchen? Ein Plädoyer für den Boykott ständig
            neuer IT-Buzzwords.   Eine Bitte an alle IBM Buzzword-Marketeers:   Auf der diesjährigen Knowtech wurde
            unter anderem in einer Blog-Parade das Thema &quot;Zukunft des Wissensmanagements: Was ändert sich mit
            Cognitive Computing?&quot; diskutiert. Mir geht das ständige Generieren neuer Hype-Begriffe in der IT
            ziemlich auf die Nerven.   Ich bin auch davon überzeugt, dass wir mit großen Datenmengen und Verfahren aus
            den Bereichen Artificial Intelligence (AI), Machine Learning, Statistical Analysis und Text Mining in den
            nächsten Jahren sehr nützliche und neue Anwendungen realisieren können. Diese Verfahren in die Praxis zu
            bringen, daran arbeiten wir bei IntraFind schon seit über 10 Jahren. Aber ich sehe eher eine Evolution an
            Stelle der mit immer neuen Hype-Begriffen propagierten Revolutionen, die mit ihren übertriebenen
            Versprechungen nur unrealistische Erwartungen bei Unternehmen und Anwendern wecken.   Die von IBM bei Watson
            eingesetzten Verfahren sind keineswegs so revolutionär wie oft behauptet. Als Erklärung des Begriffs
            „Cognitive Computing“ finde ich mit Google als ersten deutschsprachigen Treffer z.B. folgende Textpassagen:
            &quot;Der hier entscheidende technologische Aspekt ist, dass Watson nicht wie bisher mit Nullen und Einsen
            Ergebnisse nach dem Prinzip &quot;wahr/falsch&quot; beziehungsweise &quot;Transaktion durchgeführt&quot;
            oder &quot;fehlgeschlagen&quot; ausführt. Vielmehr wird, zwar immer noch mit Hilfe des dualen Systems, auf
            Grundlage teilweise unsicherer Informationen, Annahmen und auch Spekulationen basierend auf
            Wahrscheinlichkeiten nach Antworten auf komplexe, in natürlicher Sprache gestellte Fragen gesucht.&quot;  
            Genau darum geht es doch seit über 50 Jahren in allen Arbeiten im Bereich AI! Watson ist ein tolles
            AI-System mit beeindruckender Leistung für Jeopardy. Das meiste, was jetzt von IBM unter dem Namen Watson
            beworben wird, hat aber mit Watson nichts zu tun.    Also bitte, liebe IBM, nicht schon wieder einen neuen
            Hype um Watson mit dem sinnlosen Begriff &quot;Cognitive Computing&quot;. Mich nervt schon das viele Gerede
            von &quot;Big Data&quot; ...   Der Autor Dr. Christoph Goller verfügt über 15 Jahre Erfahrung im Enterprise
            Search Markt. Er promovierte in Computerwissenschaften an der Technischen Universität in München und
            arbeitete im Anschluss in einigen wissenschaftlichen Projekten zu Themenbereich Künstliche Intelligenz,
            Machine Learning sowie Neuronale Netze.   Seit 2002 leitet er den Bereich Forschung bei IntraFind und
            verantwortet IntraFind&apos;s Kern-Suche- und Content Analyse Technologien. Zwischen 2003 und 2007 arbeitete
            Christoph aktiv als Committer in Apache&apos;s Open Source Projekt Lucene.    
        </body>
    </Document>
    <Document>
        <url>http://www.intrafind.de/blog/the-difference-between-stemming-and-lemmatization-en</url>
        <id>970CE4EC31A4B97B528D153411B896BB</id>
        <title>The difference between stemming and lemmatization</title>
        <body>The difference between stemming and lemmatization   &quot;Stemming&quot; as well as &quot;Lemmatization&quot;
            are commonly used buzzwords in the field of Information Retrieval (IR), particularly in the development of
            powerful search engines.   The inverted index Just as a quick reminder: The basis of a search engine is an
            index, called Inverted Index, a data structure which consists of a list of all unique words (index terms)
            occurring in any document of a document silo. Additionally, for each unique term a list of documents, in
            which the term appears, is saved.   As an example, let&apos;s assume that we have to index two documents:
            First document (doc1): In my house lives a mouse. Second document (doc2): Mice are living in my houses.  
            The (lowercased) inverted index will look like this: index term documents in doc1, doc2 my doc1, doc2 house
            doc1 lives doc1 a doc1 mouse doc1 mice doc2 are doc2 living doc2 houses doc2   Now imagine the search query
            &quot;mouse&quot; (on the above index). It will end up only in the first document as a search result,
            although document two is also an expected result candidate, as it contains &quot;mice&quot;, the plural of
            mouse.   A search engine of high quality must be able to handle those linguistical variations of index and
            search terms. Consequently, some kind of term normalization is indispensable. Stemming and lemmatization are
            both natural language processing techniques to make sure that different word variants (inflectional and
            derivational word forms) are not left out.   So what exactly is the difference between these two methods?
            What are the advantages and disadvantages and which one should be preferred?   Stemming vs. lemmatization  
            Stemming is a procedure to strip inflectional and derivational suffixes from index and search terms with the
            aim to merge different word forms into one canonical form, called stem or root. The most common stemmer is
            the Porter Stemmer (a Porter stemmer implementation is also provided by Lucene library), which works by
            heuristically (rule based) identifying word suffixes and then chopping them off.   By contrast,
            lemmatization means reducing an inflectional or derivationally related word form to its baseform (dictionary
            form) by applying a lookup in a word lexicon. More exactly, the mentioned word lexicon is a dictionary which
            covers a complete morphological analysis for each word of a specific language.   Advantages of a stemmer are
            that there are freely available implementations and that there is no need of lexicons, which have to be
            maintained. However, the quality of stemming often is bad.   Just have a look at some examples produced by
            Porter Stemmer: Whereas for example &quot;organization&quot; as well as &quot;organs&quot; are both stemmed
            to &quot;organ&quot; (over-stemming - stemmer is cutting off too much with the result that words of
            different meaning are reduced to the same root), the following two terms of same origin &quot;absorption&quot;
            and &quot;absorbing&quot; are stemmed to &quot;absorpt&quot; and &quot;absorb&quot; (under-stemming). There
            are many other examples where stemming algorithms fail, especially words of irregular inflection (foot
            [singl.] - feet [pl.], go [inf.] - went [past tense], ...). The problem concerning a search index is obvious
            - query &quot;organization&quot; will correctly match documents containing &quot;organization&quot; or
            &quot;organizations&quot;, but will also erroneously merge documents containing &quot;organs&quot;.   By
            contrast, if an efficient and sufficiently complete lexicon exists, a lemmatizer will mostly output correct
            baseforms. Thus, a search engine based on a lemmatization normalization component compared to a stemming
            component significantly benefits and provides much more accurate search results.   High recall and precision
            for enterprise search   In order to guarantee high recall and precision of iFinder&apos;s search results,
            IntraFind developed its own linguistic module called LISA, including a lemmatizer of high quality. LISA
            offers a complete morphological analysis by using complex prepared dictionaries of currently 15 European
            languages (Croatian, Czech, Dutch, English, French, German, Greek, Hungarian, Italian, Polish, Portuguese,
            Russian, Slovakian, Slovenian, Spanish).   Apart from lemmatization LISA is also a word decomposer, which is
            capable of splitting compound words in their individual word fragments. Imagine for example an English
            search query like &quot;toothpaste&quot;. Thanks to LISA you will be able to find documents, which contain
            of course &quot;toothpaste&quot;, as well as &quot;paste&quot; and &quot;tooth&quot;.   As German is a
            language much more complex than English, the following example is more exciting: The German search query
            &quot;Glückskeks&quot; decomposed by LISA results in two word parts - &quot;glück&quot; and &quot;keks&quot;.
            As you can see, LISA even succeeds in recognizing the semantic useless part of the word (&quot;s&quot;),
            called epenthesis (in German &quot;Fugenelement&quot;) and therefore the query will provide result documents
            containing &quot;glückskeks&quot;, &quot;glück&quot; and &quot;keks&quot;.   For enterprise search, using
            high performance linguistics like LISA means that the user gets highly relevant search results and no
            information is lost.   Der Autor Ursula Seisenberger studierte Computerlinguistik an der Universität in
            München (Center for Information and Language Processing).   Seit 2013 arbeitet Ursula als Software
            Architektin bei IntraFind und fokussiert sich auf Textanalyseverfahren.
        </body>
    </Document>
    <Document>
        <url>http://www.intrafind.de/unternehmen/glossar/tag</url>
        <id>9DC11F4C8DD2C7CE298CF95A70ECC9EF</id>
        <title>Tag</title>
        <body>Tag   Ein Merkmal, das einen Inhalt, Bild, Text beschreibt. zurück  </body>
    </Document>
    <Document>
        <url>http://www.intrafind.de/Abteilungsuebergreifender_Nutzen</url>
        <id>3029DEA6D376BA5CA763BDBE592625FC</id>
        <title>Nutzenargumente für Abteilungen</title>
        <body>Abteilungsübergreifender Nutzen IntraFind setzt mit dem Facelift der Benutzeroberfläche neue Standards.
            Als Mitarbeiter profitieren Sie schon beim Start der webbasierten Oberfläche - mit dem Dashboard. Lesen Sie
            aktuellste Nachrichten von Ihrem Unternehmen, erfahren Sie Neues aus Ihrem Blog. Das Widget-basierte
            Dashboard gibt Ihnen viel Raum für kleine personalisierte Alltagshelfer. Teilen Sie Ihre Suchergebnisse mit
            Ihren Kollegen, lassen Sie sich aktiv Informieren, wenn neue Informationen zu einem Thema abgelegt wurden
            oder speichern Sie Ihre Suchanfragen bzw. Ihren Rechercheprozess, so dass Sie ihn zu gegebener Zeit wieder
            aufnehmen können. IntraFind lässt Sie auch mit den Suchergebnissen nicht allein. Schnell, einfach und
            komfortabel erkennen Sie Dateitypen auf einen Blick. Jeder Dateityp hat seine einzigartige Darstellung.
            PowerPoint-Präsentationen sind beispielsweise als einzelne Folien dargestellt und können direkt aufgerufen
            werden. Die universelle Suche über viele Datenquellen (z.B. Dateisystem, Microsoft Exchange, Lotus Notes,
            Microsoft SharePoint, Confluence) hinweg ermöglicht einen umfassenden und zielgerichteten Einblick in Ihr
            gewünschtes Informationsmaterial.   &gt;&gt; zurück zu den Unternehmensbereichen
        </body>
    </Document>
    <Document>
        <url>http://www.intrafind.de/unternehmen/glossar/lucene.net</url>
        <id>40599B10F9383C3915113516B3EFDBB5</id>
        <title>Lucene.NET</title>
        <body>Lucene.NET   Die Open Source-Technologie Lucene.NET ist ein Apache Projekt auf der Basis des .NET
            Frameworks, geschrieben in C#.   IntraFind nutzt Lucene als Basis für die Enterprise Search-Lösung iFinder
            und bietet eine leistungsfähige unternehmensweite Suchmaschine, die alle vorhandenen Applikationen in den
            Suchprozess mit integrieren kann. zurück  
        </body>
    </Document>
    <Document>
        <url>http://www.intrafind.de/company/glossary/cross-validation</url>
        <id>B87E6E2C8AC3916DF76D166462694C16</id>
        <title>Cross-validation</title>
        <body>Cross-validation   A way to measure a classification model&apos;s quality when a test corpus is not
            available. In an nfold cross-validation setup, the training data is split into n disjoint sets. n different
            models are then trained on n-1 sets and tested on the (always different) remaining set. The computed quality
            statistics for each of the n rounds are then averaged and used as an estimate for the actual quality.  
        </body>
    </Document>
    <Document>
        <url>http://www.intrafind.de/blog/die-datenschutz-grundverordnung-anforderungen-fuer-insight-engines</url>
        <id>38DF7C68BAD6B5DB0D83E5F33B6AD6A2</id>
        <title>Die Datenschutz Grundverordnung – Anforderungen für Insight Engines</title>
        <body>Die Datenschutz Grundverordnung – Anforderungen für Insight Engines Die Datenschutz Grundverordnung –
            Anforderungen für Insight Engines Ab Mai 2018 tritt die neue DSGVO im Rahmen der europaweiten
            Datenschutzreform in Kraft. Sie betrifft alle Unternehmen und Dienste, die im Rechtsraum der Europäischen
            Union angeboten werden. Insbesondere sind aber solche Unternehmen im Fokus der DSGVO, deren Firmensitz nicht
            im Gebiet der EU liegt und daher die bereits vorhandene und eher strengere Datenschutzanforderung nicht
            berücksichtigt haben. Die DSGVO garantiert dem Bürger neue weitreichende Rechte. Insbesondere das „Recht auf
            Vergessen“ wurde neu eingeführt. Auf Anforderung muss das Unternehmen alle personenbezogenen Daten
            unwiderruflich löschen. Darunter fallen tatsächlich alle Daten, die im Zusammenhang mit einem Benutzer
            angefallen sind, etwa besuchte Webseiten oder aber herkömmliche personenbezogene Daten, wie Adresse und
            Telefonnummern. Besteht die eigentliche Zielgruppe der DSGVO eher aus Social Media Diensten, so trifft sie
            dennoch alle Unternehmen gleichermaßen. Schaut man in die einschlägigen Foren, so schießen bereits diverse
            „neue“ Lösungen in den Markt, die sich mit den Herausforderungen der DSGVO beschäftigen. Allen ist gemein,
            dass die Lösungen hinreichend unspezifisch sind. Woran liegt das? Das Recht auf Vergessen Bleiben wir bei
            der Anforderung des „Recht auf Vergessen“. Werden im Rahmen eines B2C-Geschäfts Daten des Kunden/Käufers
            aufgenommen, so fallen diese Daten aller Voraussicht nach nicht unter das Recht auf Vergessen, da andere
            Regelungen dieses Recht einschränken. Hier seien zum Beispiel das BGB, das Handelsrecht und die daraus
            resultierenden Aufbewahrungsfristen genannt. Sind im selben Unternehmen allerdings Newsletter, Chats und
            Foren im Einsatz, dann greift wiederum das Recht auf Vergessen. Auf Anforderung sind diese Informationen zu
            löschen. Unternehmen, die nicht einen extrem perfekten Prozess befolgen, werden sich hier intensiver mit der
            DSGVO beschäftigen müssen. In der DSGVO sind viele Einschränkungen und Ausnahmemöglichkeiten genannt. Ist
            das Löschen zum Beispiel wirtschaftlich nicht möglich, technisch nicht umsatzbar, dann gibt es eine
            Freikarte. Gleichzeitig gilt aber auch der Grundsatz: Sollen heißt Müssen, wenn Du kannst. Also ist das
            Berufen auf die o.g. Ausnahmen eher keine gute Strategie. Wie der iFinder5 elastic bei der Recherche hilft
            Damit man weiß, was alles gelöscht werden muss, benötigt man einen Überblick über die gespeicherten Daten
            eines Anwenders. Bestandssysteme mit Benutzerverwaltung sollten da alle Anforderungen erfüllen können.
            Speziell wird es, wenn außerhalb der Bestandssysteme Daten verteilt werden, etwa eine eingehende
            Interessentenanfrage via Email-Kopie zur weiteren Klärung mit den Details des Anwenders weitergeleitet wird.
            In diesem Fall hilft die eingesetzte Enterprise Search Anwendung wie der iFinder5 elastic. Als Insight
            Engine ist der iFinder5 elastic die zentrale Anwendung für die unternehmensweite Suche in strukturierten und
            unstrukturierten Daten. Über ein zentrales, einfach zu bedienendes User Interface kann der Anwender nach
            entsprechenden Namen der Personen suchen. Der iFinder5 elastic verfügt über zahlreiche Schnittstellen, über
            die sämtliche Quellen wie Email-Programme und Webportale angebunden werden können. Der Anwender muss bei der
            Suche nicht zwingend wissen, in welcher Datenquelle die Information liegt. Mit dem iFinder5 elastic können
            auch „Ausreißer“ wie etwa die weitergeleitete Email mit den Details schnell und zuverlässig identifiziert
            und dann auch entsprechend aus dem Email-System entfernt werden. Der Autor Ralf Klinkhammer ist seit fast 30
            Jahren im Bereich des Enterprise Information Managements, national und international, tätig. Ralf
            verantwortet seit 2014 das Produkt Management der IntraFind Software AG.  
        </body>
    </Document>
    <Document>
        <url>
            http://www.intrafind.de/blog/verborgene-schaetze-finden-bestehende-probleme-aufzeigen-mehrwert-schaffen-durch-fileshareanalyse
        </url>
        <id>2BE71F4F2261BF84DCBC4B2F428245E5</id>
        <title>Verborgene Schätze finden, bestehende Probleme aufzeigen: Mehrwert schaffen durch Fileshareanalyse
        </title>
        <body>Verborgene Schätze finden, bestehende Probleme aufzeigen: Mehrwert schaffen durch Fileshareanalyse
            Verborgene Schätze finden, bestehende Probleme aufzeigen: Mehrwert schaffen durch Fileshareanalyse Ob
            Handbücher, Datenblätter oder Präsentationen – in vielen Unternehmen sammeln sich auf den Servern immer mehr
            Daten an. Nicht immer ist die Qualität der Metadaten dieser Dokumente wünschenswert und oft gibt es auch
            Brüche bei den Zugriffsrechten auf die Daten. Die Realität in vielen Unternehmen, besonders bei der Nutzung
            von Fileshares, ist die, dass oftmals Dubletten oder veraltete Dokumente den Fileshare „zumüllen“. In der
            Regel müssen Mitarbeiter jedoch immer genau wissen, wo ein bestimmtes Dokument gespeichert ist und wie es
            heißt. Wird die Datei jedoch an einem anderen Ort gespeichert oder nicht korrekt bezeichnet oder mit
            entsprechenden Metadaten versehen, wird das Dokument unter Umständen gar nicht gefunden. Dadurch geht
            wertvolles Wissen verloren oder das Rad wird zu oft zweimal erfunden, weil dem Mitarbeiter zum Beispiel
            nicht bewusst ist, dass es eine Information zu einem bestimmten Thema bereits gibt. Eine zusätzliche
            Herausforderung ergibt sich dann, wenn die Daten auf eine andere IT-Architektur migriert werden müssen.
            Oftmals steht der IT-Administrator vor der Herausforderung, nur die Daten zu migrieren, die auch wirklich
            benötigt werden. Dubletten sollten hierbei entfernt werden, so dass Dateien nicht unnötig doppelt umgezogen
            werden. Das bestehende Rechte- und Rollenkonzept muss dabei jedoch genau betrachtet und auf der
            Zielplattform eingehalten werden. Die Fileshareanalyse Um herauszufinden, welche Daten migriert werden
            sollen und welche gelöscht werden können, muss die IT-Abteilung jedoch erst einmal wissen, welche Daten es
            gibt und wo sich diese befinden. Durch eine tiefgehende Analyse der Fileshares können alle Daten erfasst
            werden. Dadurch können vor allem drei wichtige Erkenntnisse gewonnen werden: Das Erkennen von Brüchen im
            Rechtekonzept: Durch Ausschneiden und Einfügen von Ordnern mit vielen angehängten Dateien – ein durchaus
            gängiges Mittel bei der Migration von Abteilungsdaten – kommt es oft zu Rechtebrüchen im Fileshare. Das
            Problem, das sich daraus ergibt, ist, dass Ordner, auf denen Berechtigungen fehlen, im Fileexplorer nicht
            angezeigt werden. Dokumente, die aber unterhalb der Ordner liegen, besitzen oftmals wieder die Rechte des
            Benutzers. Ein Problem bei Migrationsprojekten oder auch bei einem Enterprise-Search-Projekt ist, dass nun
            die Suchmaschine diese Dokumente richtig anhand der bestehenden Dateirechten in der Trefferliste anzeigt,
            obwohl der Ordner gegebenenfalls nicht im Fileexplorer zu sehen ist. Die Sicht auf Metadaten. Hier lässt
            sich unter anderem erkennen, wie oft auf die Daten zugegriffen wird und welche Dateiformate wie häufig in
            welchen Bereichen vorkommen. Auch das Alter der Inhalte kann analysiert werden. Die inhaltliche Analyse.
            Hier geht es darum, mit Content Analytics Verfahren die wesentlichsten Inhalte zu extrahieren. In der
            Analyse werden auch die Verzeichnispfade auf Top-Themen und wichtige Metadaten hin analysiert, innerhalb der
            Dokumente werden wichtige Schlagworte erkannt, sowie Informationen über Produkte, Mitarbeiter bzw.
            Personennamen, Orte oder Firmennamen gesammelt und in der Analyse-Plattform zur Anzeige gebracht. Der Nutzen
            Durch die beschriebene Detail-Analyse der Filesharedaten kann ein Securitymanager Vertrauen schaffen, indem
            die Daten sauber migriert werden und die Sicherheitsanforderungen, wie integrierte Rechtekonzepte,
            eingehalten werden. Ebenso wichtig ist zudem der Aspekt der inhaltlichen Analyse. Welche Dokumente müssten
            gemäß gesetzlicher oder unternehmensspezifischer Löschfristen und Compliance-Richtlinien aus den
            Speichersystemen entfernt werden? Welche Dokumente gilt es gesondert zu betrachten, weil es hierfür bereits
            definierte Workflows oder Zielsystem im Unternehmen gibt? Das Unternehmen profitiert von der
            Fileshareanalyse außerdem in folgenden Punkten: Mehr Sicherheit: Durch das Erkennen von fehlenden
            Rechtekonzepten können diese nachträglich ergänzt werden. Bessere Qualität der Daten: Durch das Aufdecken
            von Dubletten können diese bereinigt werden. Daten, die lange nicht mehr verwendet oder veraltet sind,
            können bequem in einem gesonderten Prozess gelöscht werden. Mehr Wissen: Dokumente werden mit Metadaten
            versehen, sodass der Nutzer weiß, welche Inhalte sich in den Daten befinden. Verborgenes Wissen kann so
            aufgedeckt und einem großen, berechtigten Nutzerkreis zur Verfügung gestellt werden. Weniger Kosten: Durch
            die Datenbereinigung lassen sich nicht zuletzt Speicherplatz und damit Kosten für weitere Server sparen. Bei
            einer Migration verringert sich gegebenenfalls das zu migrierende Datenvolumen aufgrund der gewonnen
            Erkenntnisse drastisch. Fileshareanalyse mit iFinder5 elastic und Kibana Sind alle zu analysierenden
            Datenquellen erfasst, werden sie im nächsten Schritt mit dem iFinder5 elastic indexiert und recherchierbar
            gemacht. Der iFinder5 elastic ist eine Enterprise Search Lösung für die unternehmensweite Suche in
            strukturierten und unstrukturierten Daten. Kibana ist eine Visualisierungsplattform von Elasticsearch, die
            die Suchoberfläche des iFinder um flexible Reportingfunktionen ergänzt. Darüber hinaus verfügt der iFinder5
            elastic über zahlreiche Content Analytics Funktionen. Bereits beim Indexieren extrahiert der iFinder aus dem
            Dokumenten die Metadaten wie zum Beispiel Autor, Datum oder Titel und stellt diese später in der
            Trefferliste zur Ansicht bereit. Tauchen im Dokument Schlagworte, Produkte, Personen, Organisationen oder
            Orte auf, können diese außerdem mit dem Tagging Service ebenfalls automatisch erkannt werden. Diese Angaben
            werden dann als zusätzliche Metadaten im Index gespeichert und sind so für den Analysten recherchierbar.    
            Suchanfragen können einfach und schnell gefiltert werden. So lässt sich zum Beispiel bei einer Suche nach
            dem Begriff „confidential“ über die angebotenen Suchfilter Dateityp „PowerPoint“ und Autor „IntraFind“ sowie
            aus der Zeitleiste der „letzte Monat“ aus einem großen Datenbestand, eine überschaubare Menge an Inhalten
            filtern. Mit drei Klicks werden dadurch alle PowerPoint Präsentationen, in denen der Begriff „confidential“
            vorkommt, des vergangenen Monats von IntraFind gefunden. In einer weiteren Ansicht der Analyseplattform
            lassen sich schnell die gewonnenen Metadaten anhand moderner, auf Kibana basierenden Dashboards selbständig
            in Reports zusammenfassen. Reports sind einfach und schnell neu definiert und wiederum für eine Gruppe von
            Analysten wiederverwertbar. So unterstützt der iFinder5 elastic die Fileshare-Analyse:
            Autovervollständigung: mit Tippfehlerkorrektur und „Meinten-Sie“-Vorschläge Preview: Vorschaufunktion für
            mehr als 600 Dateiformate und Hervorhebung der Trefferstelle in der Großpreview der
            iFinder-Benutzeroberfläche. Metadatenerzeugung: Über das automatische Verschlagworten als
            Standardfunktionalität des iFinder können fehlende Metadaten automatisch oder in einem qualitätsgesicherten
            Prozess aus den unstrukturierten Volltexten erzeugt und die Inhalte damit angereichert werden. Das System
            erzeugt folgende Metadaten: Top-Schlagworte, allgemeine Eigennamen von Personen, Unternehmen oder auch
            firmenspezifische Entitäten wie Produktnamen oder Abteilungsbezeichnungen sowie Themenzugehörigkeiten. Es
            können dann darauf aufbauend auch Relationen zwischen den Entitäten erkannt und angereichert werden.
            Dublettenprüfung: Filterung nach Dokumenten mit gleichem Inhalt beziehungsweise nach identischen Dokumenten
            Ähnliche Dokumente finden: Auf Basis der Schlagworte und Top-Terme eines gefundenen Dokuments werden im
            Bestand inhaltlich ähnliche Dokumente gefunden. Speicherung von kompletten Suchanfragen, Unterstützung von
            Kollaborations-Funktionalitäten Rollenkonzepte: der iFinder nutzt Rechte und Rollenkonzepte, mit denen sich
            unterschiedliche Ansichten auf die Inhalte der Dokumente steuern lassen. Ein Mitarbeiter aus der
            Rechtsabteilung könnte so beispielsweise auch unabhängig der Dokumentenrechte Inhalte der Dokumente sehen.
            Fachadministratoren könnte man nur Zugriff auf ihre „eigenen Daten“ gewähren und für IT Administratoren wäre
            es durchaus denkbar, dass sie die Metadaten aller Dokumente sehen, die das System erfasst. Der iFinder5
            basiert auf der Technologie Elasticsearch und kennt keinerlei Limitierung bezüglich der zu erfassenden
            Datenmengen. Mehrere Milliarden Datensätze stellen für den iFinder keinerlei Hürde dar. Selbstverständlich
            ist eine inhaltliche Analyse der Daten auch in jedem anderem Quellsystem sinnvoll. Auch über Datenquellen
            hinweg ist die Analyse in vielen Anwendungsszenarien ein wichtiges Werkzeug dafür, um Sicherheit über die
            Dateninhalte und Datenstruktur zu gewinnen. Erfahren Sie hier mehr über die Content Analytics Suite for
            Artificial Intelligence. Fragen Sie uns nach erfolgreichen Projektbeispielen bei unseren Kunden!   Der Autor
            Manuel Brunner ist Experte für das Thema Suchtechnologien. Seit 2008 ist er für die IntraFind Software AG
            tätig und leitete u.a. viele Jahre lang das Team Professional Services, bevor er in seiner neuen Rolle die
            Bereiche Partner Management &amp; Business Development übernahm. Manuel Brunner is an expert in search
            technologies. He has been working at IntraFind Software AG since 2008 and has managed the team Professional
            Services for many years. In his new role he took over the divisions Partner Management &amp; Business
            Development.
        </body>
    </Document>
    <Document>
        <url>http://www.intrafind.de/services/schulungen</url>
        <id>36AD57AF43B7E41446CAF37EDB1211C1</id>
        <title>Schulungsportfolio der IntraFind Software AG</title>
        <body>IntraFind Schulungen – Know-how für Ihr Unternehmen und Ihre Mitarbeiter IntraFind Experten stellen ein
            umfangreiches Schulungsangebot in den Bereichen Projektmanagement, Suchtechnologien auf Basis von
            Elasticsearch und Verfahren der Textanalyse bereit und vermitteln den Teilnehmern kompaktes Wissen. Lernen
            Sie durch den Besuch unserer Schulungen, Enterprise Search zur Produktivitätssteigerung in Ihrem Unternehmen
            nutzbar zu machen: wir verschaffen Ihnen einen Wissensvorsprung und unterstützen Sie dabei, Ihre Suchlösung
            optimal zu konfigurieren, zu verwalten und einzusetzen. Die Schulungen finden bei IntraFind in München oder
            flexibel im Rahmen eines Inhouse-Workshops in Ihrem Unternehmen statt. Das Schulungsportfolio: Erfolgreich
            Enterprise Search-Projekte umsetzen - aber wie? Verfahren der Textanalyse - Mehrwert aus Textinformationen
            generieren Expertenwissen rund um iFinder5 elastic Enterprise Search
        </body>
    </Document>
    <Document>
        <url>http://www.intrafind.de/products</url>
        <id>B6481A6810DAAB7A8DF77E2BE94F8FAE</id>
        <title>IntraFind Products</title>
        <body>IntraFind - your partner for enterprise search and content analytics. iFinder5 elastic Search &amp; Find
            is Intrafind&apos;s DNA: Our product iFinder5 elastic is your search engine for your company. You benefit
            from unified information access to all data sources you have. Provide your colleagues an easy to use
            interface and combine existing insights with innvovative thoughts.  Find your information now Tagging
            Service You would like to see how all documents correlating to a certain term or topic? No problem. The fast
            IntraFind Tagging Service supports your search engine or application to get most out of your information.
            Tag now TopicFinder Learn how our TopicFinder your content is getting classified in order to help you to get
            more insights in big data repositories.  Classify now Elasticsearch Plugins &amp; Services You wish to
            enhance your elasticsearch search engine? IntraFind is providing you a complete service stack to maximise
            your search experience.  Search Plugins &amp; Services Autocomplete Shorten the website search of your
            customers visiting your website. Our Autocomplete Service identifies correct words, provides highlighting,
            synomyms and search results upfront. Your customers get what they are looking for ver fast. Autocomplete now
        </body>
    </Document>
    <Document>
        <url>http://www.intrafind.de/produkte/ifinder5-elastic/services_plugins</url>
        <id>9A6E22FCD5FC08C05CD71F95645714E3</id>
        <title>iFinder5 elastic Services &amp; Plugins</title>
        <body>Services &amp; Plugins für Elasticsearch Die wachsenden Mengen vor allem unstrukturierter Daten stellen
            Unternehmen vor neue Herausforderungen im Umgang mit Dokumenten und Informationen. IntraFind erweitert mit
            seinen Services &amp; Plugins die führenden Open Source- Suchserver Elasticsearch und bietet Unternehmen
            Einfachheit und maximale Flexibilität für den Aufbau hochperformanter und skalierbarer Suchanwendungen,
            besonders im Big Data-Umfeld. Daraus ergeben sich wichtige Vorteile: Mit den IntraFind Services &amp;
            Plugins werden Elasticsearch schneller und einfacher nutzbar. Projektlaufzeiten und Kosten reduzieren sich
            durch die einfach und schnell integrierbaren IntraFind Services &amp; Plugins deutlich. Zudem ergänzt das
            Baukastenprinzip optimal den Funktionsumfang, welcher für die Umsetzung einer Suchanwendung erforderlich
            ist. Services und Plugins (z.B. für die Dokumentenkonvertierung, Treffervorschau, Autovervollständigung,
            Linguistik, Verschlagwortung mit Metadaten sowie für das Einbinden von externen Ressourcen wie Thesauri)
            sorgen für eine hohe Qualität der Suchergebnisse und damit für Benutzerfreundlichkeit. Die IntraFind
            Services und Plugins stehen als Cloud-Dienst (SaaS) oder als On-Premise-Lösung zur Verfügung. Zudem sind sie
            alleinstehend oder flexibel miteinander kombiniert einsetzbar. Folgende Eigenschaften zeichnen die IntraFind
            Services aus: Benutzer können die Services in geringer Zeit und mit geringen Vorkenntnissen implementieren
            und nutzen Alle Services sind schlank, auf Performance optimiert und einfach integrierbar Webschnittstelle
            im XML-, JSON-, Hessian- und SOAP-Format Verfügbarkeit von Load-Balancing und Ausfallsicherheit Minimale
            Installationsvoraussetzungen Gute Abstraktion der Suchtechnologie von der Implementierung Migration auf
            zukünftige Technologien ohne clientseitige Änderung möglich Beispielhafte Anordnung der IntraFind Services
            &amp; Plugins auf Basis von Elasticsearch IntraFind Services Search &amp; Index Service Extract Service
            Preview &amp; Extract Service Autocomplete Service Tagging Service Query Expansion Service SimFinder Service
            Summarizer Service IntraFind Plugins Linguistik Plugin
        </body>
    </Document>
    <Document>
        <url>http://www.intrafind.de/company/glossary/open-source</url>
        <id>B0123BEFD48AB4B6D723F50DEF623A7A</id>
        <title>Open Source</title>
        <body>Open Source   IntraFind uses the open source search engine Lucene.  </body>
    </Document>
    <Document>
        <url>http://www.intrafind.de/produkte/elasticsearch/plugins_services/Preview_Service</url>
        <id>17EB673685BEFCA479F77723DE535B0C</id>
        <title>Elasticsearch Preview &amp; Extract Service</title>
        <body>Preview &amp; Extract Service Der Preview &amp; Extract Service generiert aus über 600 Dateiformaten
            Vorschaubilder der Originaldokumente. Das erhaltene Bild kann im Anschluss in verschiedene Systeme (CMS,
            DMS, Trefferlisten etc.) eingebunden werden. Der Preview &amp; Extract Service ist ein optimaler Bestandteil
            jeder Suchlösung und unterstützt den Anwender im schnelleren Umgang mit Dokumenten.   Technische
            Spezifikation: Betriebssysteme Windows Server 2008 R2 oder höher sowie Linux Installationsvoraussetzungen
            Java 7 Kombinierbare Dienste: Der Preview &amp; Extract Service kann sowohl alleine als auch in Verbindung
            mit dem Search &amp; Index Service eingesetzt werden. Alle weiteren Services und Plugins können in einer
            Pipeline orthogonal kombiniert werden Dokumentation: SOA-Begleitdokumentation Beispielimplementierung in
            Java wird bereitgestellt Demo-Verzeichnis mit einfachen Click&amp;Play-Nutzungsbeispielen Für wen ist der
            Service interessant: Unternehmen, die eine visuelle Vorschau der Dokumente benötigen ohne die entsprechende
            Anwendung oder das Programm lokal installiert zu haben Embedding Partner Elasticsearch Entwickler Solr
            Entwickler DEMO-ZUGANG ANFORDERN &gt;&gt;
        </body>
    </Document>
    <Document>
        <url>http://www.intrafind.de/blog/suche-im-pdm</url>
        <id>B53E3EE7ED7FE99F6A2E673A5C7F25A7</id>
        <title>5 Gründe für eine Suche in PDM Systemen</title>
        <body>5 Gründe für eine Suche in PDM Systemen 5 Gründe, warum sich eine Suche im Produktdatenmanagement lohnt
            Produktdaten und produktrelevante Informationen entstehen in fast allen Bereichen eines Unternehmens und
            werden von fast allen Abteilungen benötigt. Die Werkstatt braucht Aussagen, welche Schrauben in einer
            Baugruppe verwendet werden dürfen, in der PR-Abteilung eines Autobauers ist eine Anfrage eines Kunden zum
            Thema gesundheitsgefährdende Stoffe in den verwendeten Kunststoffen oder im Lederbezug des Lenkrads zu
            beantworten. Oftmals fügen Mitarbeiter im Laufe des Entstehungszyklus eines Produkts weitere Daten hinzu.
            Der hohe Grad an Vernetzung der Produktdaten aufgrund steigender Produktkomplexität und die Forderung nach
            zuverlässiger Bereitstellung von Daten über den gesamten Lebenszyklus hinweg, stellen Verantwortlichen im
            Bereich Produktdatenmanagement vor große Herausforderungen. Nicht nur der Informationsbedarf nimmt zu, die
            Entwicklungszyklen werden immer kürzer und Unternehmen sind gezwungen, schnelle Antworten auf komplexe
            Fragestellungen zu finden. Wie kann sichergestellt werden, dass im Bedarfsfall die benötigten Daten
            vollständig und in adäquat aufbereiteter Form beim Adressaten landen? Gibt es überhaupt den EINEN Ort, an
            dem sich alle benötigten Informationen befinden? Ist sichergestellt, dass wirklich alle produktrelevanten
            Informationen dort zusammen kommen? Wer entscheidet, was produktrelevant ist? Das Marketing benötigt z.B.
            zum Thema Lenkrad andere Informationen als die Produktionsplanung. Oberste Priorität kommt der
            Bereitstellung von Normen, Versuchsberichten, Änderungsmitteilungen und den technischen Dokumentationen zu.
            Idealerweise sind mindestens diese Informationen in einem PDM System in aktueller Form vorhanden. Zusätzlich
            gibt es in jedem Unternehmen weitere, häufig sehr hilfreiche Information z.B. aus Gesprächsprotokollen,
            Informationen aus Krisensitzungen, Beschwerde- und Belobigungsschreiben, Serviceberichten oder Kommentaren
            zu Reparaturanleitungen. WIKIS Einträge, E-Mails oder Diskussionsforen sind ebenfalls durchaus hilfreiche
            und wichtige Informationsquellen, um die eine oder andere Fragestellung schneller und umfassend beantworten
            zu können. Einbindung aller relevanten Datenquellen PDM Systeme lassen eine wesentliche Funktion vermissen:
            Es existiert meist kein Ranking. Bei der Suche muss der User unter Umständen große Listen durchsuchen und
            jedes Ergebnis einzeln prüfen. Hierdurch wird wertvolle Arbeitszeit verschwendet. Manche Systeme bieten
            überhaupt keine intelligenten Filter und Selektionsmechanismen an, was die Suche nicht gerade erleichtert.
            Selten kann man sich darauf verlassen, dass angezeigte Informationen wirklich vollständig und aktuell sind,
            die Anwender können sich eigentlich nie so 100-prozentig sicher sein, ob sie auch wirklich alle relevanten
            Informationen gefunden haben. Das Ganze ist dann noch dazu zeitraubend und zeigt auf, dass
            Informationsgewinnung ein echter Kostenfaktor ist. In der Praxis ist nicht automatisch gewährleistet, dass
            wichtige Informationen, die die Produkte betreffen, automatisch auch im PDM System landen. Oft sind z.B. die
            Prozesse für die Zulieferung überhaupt nicht genau definiert und man läuft als PDM Verantwortlicher
            sozusagen den Daten hinterher.   Werden dann tatsächlich mal schnell irgendwelche Informationen benötigt,
            geht die Suche erst richtig los. Müssen dann noch andere Datenquellen mit unterschiedlichen
            Abfragemechanismen in die Suche mit einbezogen werden, wird es schwierig.  Wie bekommt man nun die
            Informationen aus den unterschiedlichen Quellen zusammen, natürlich unter Einhaltung der individuell
            geregelten, manchmal sehr komplexen Zugriffsberechtigungen? Die Lösung liegt im Aufsetzen einer
            übergreifenden Suche, an die sowohl das PDM System, als auch die diversen anderen Datenquellen angeschlossen
            werden. Dadurch wird sichergestellt, dass die Anwender auf Informationen aus verschiedenen Datenquellen
            zugreifen können, egal ob z.B. Intranet, Filesystem oder einer  Bibliotheksapplikation. Einfache Bedienung
            auch für fachfremde Kollegen Ein weiterer Vorteil: Mitarbeitern, die nicht den ganzen Tag mit PDM Systemen
            zu tun haben, kann man über eine intuitiv bedienbare Oberfläche mit durchgängiger, einheitlicher
            Bediensystematik mehr Suchkomfort bieten. Das komplizierte Bedienen eines PDM Systems wird somit umgangen,
            Berührungsängste gehen verloren, die Zahl der User und die Nutzung des Systems steigen sprunghaft an. Auch
            die Einarbeitung neuer Kollegen kann dadurch  deutlich vereinfacht werden. Nicht zuletzt lassen sich durch
            die Integration der Suche in das PDM System neue Nutzerkreise erschließen, die nicht mehr auf Spezialisten
            angewiesen sind, sondern durchaus selbst in der Lage sind, Recherchen durchzuführen. Natürlich lässt sich so
            ein Suchsystem auch genau andersrum nutzen, nämlich zum Auffinden von Spezialisten. In der Praxis zeigt
            sich, dass die PDM Systeme durch die Hinzunahme einer komfortablen Suche eine ganz neue Akzeptanz erfahren.
            Das liegt daran, dass das System nun einfacher zu bedienen ist.   Vollständige Trefferliste nach
            Relevanzkriterien Nutzer erhalten eine übersichtliche Trefferliste und können davon ausgehen, dass diese
            auch vollständig ist und sie können über diese angezeigten Treffer direkt auf ihre gefundenen Dokumente und
            Informationen zugreifen. Wichtig ist dabei natürlich die Einhaltung der individuellen Zugriffsrechte. Das
            funktioniert übrigens auch mit der Autovervollständigung bei der Eingabe eins Suchworts. Auch das ist
            rechtegeprüft! Die Trefferliste wird nach Relevanzkriterien gebildet, die über ein PDM System überhaupt
            nicht zu Verfügung gestellt werden könnte. Intelligente Filtermechanismen als Facetten und Suchfilter
            erleichtern dem User nochmals das weitere schnelle Eingrenzen. Die Suche funktioniert natürlich auch über
            mehrsprachigen Content hinweg.   Linguistische Funktionalitäten wie Kompositazerlegung Das Herzstück einer
            guten Suchmaschine ist die linguistische Aufbereitung des Index. Dadurch lassen sich qualitativ hochwertige
            und vollständige Suchergebnisse erzielen. Suchen Mitarbeiter beispielsweise nach der Beschreibung einer
            Ölablassschraube, wollen sie  vielleicht auch die Dokumente schnell finden, die Informationen über die
            „Abdichtung an der Ölwanne mit einer Schraube“ enthalten. Zusätzliche Mehrwerte ergeben sich durch die
            Einbeziehung von branchen- und firmenspezifischen Ausdrücken und Synonymen. Man kann also mit einem Klick
            Suchanfragen erweitern, Vorschläge dafür liefert zum Beispiel ein firmen- oder branchenspezifischer
            Thesaurus - oder im einfachsten Fall eine selbst erstellte Liste mit Akronymen über häufige firmeninterne
            Abkürzungen.   Mehr Ergebnisse dank semantischer Suche Die semantische Suche schlägt weitere Suchterme vor,
            die in engem Zusammenhang mit ihrem eigentlichen Suchwort stehen. Dadurch lässt sich ebenfalls die
            Trefferliste schnell minimieren. Die gefundenen Dokumente lassen sich als Favoriten kennzeichnen und in
            einem separaten Bereich speichern.   Eine Wissenslandkarte ermöglicht zudem eine Suche, ohne dafür Suchwort
            eingeben zu müssen. Hier werden Metadaten dynamisch gefiltert und die potentielle Treffermenge eingegrenzt.
              Fazit Durch eine professionelle Enterprise Search Lösung kann die Suche in PDM-Systemen mit einfachen
            Mitteln verbessert und effizienter gestaltet werden. Informations- und Wissensschätze bleiben dadurch nicht
            in irgendwelchen Ordnern oder Laufwerken verborgen, sondern können gezielt gefunden und genutzt werden. Dank
            der vielfältigen Anbindungsmöglichkeiten an unterschiedliche Systeme findet der Mitarbeiter auch die Daten,
            die nicht direkt im PDM-System hinterlegt sind – seien es Einträge aus Wikis, Mails oder anderen Systemen.
            Die Autocomplete-Funktion macht es möglich, dass der Nutzer auch dann die richtigen Begriffe findet, wenn er
            sich vertippt. Die integrierte Rechteverwaltung gewährleistet zudem, dass Abteilungen nur auf die Dokumente
            zugreifen können, die sie berechtigt sind zu sehen.   Der Autor Nach dem BWL Studium war Rutger Lörch in
            verschiedenen Positionen bei namhaften Hardware- und Softwareanbietern wie Nixdorf, Digital und Oracle
            tätig. Seit 2008 unterstützt er beim Suchspezialisten IntraFind den Vertrieb. „Das Thema Enterprise Search
            ist deshalb so faszinierend, weil die Anforderungen der jeweiligen Interessenten sehr individuell sind. Ein
            sehr abwechslungsreiches Betätigungsfeld voller spannender Herausforderungen.“
        </body>
    </Document>
    <Document>
        <url>http://www.intrafind.de/company/glossary/redundancy</url>
        <id>A5ED529680D8D6F357DE943AE3E0F394</id>
        <title>Redundancy</title>
        <body>Redundancy   Redundancy of, for example, information is when the content of the information is available
            several times.  
        </body>
    </Document>
    <Document>
        <url>http://www.intrafind.de/news</url>
        <id>43BCC0F0832DBB6B8AB1AF544F29047E</id>
        <title>Neuigkeiten der IntraFind Software AG</title>
        <body>IntraFind News Auf dieser Seite finden Sie aktuelle Neuigkeiten der IntraFind Software AG. Presse-Material
            stellen wir Ihnen auf Anfrage gerne zur Verfügung.
        </body>
    </Document>
    <Document>
        <url>
            http://www.intrafind.de/unternehmen/news/google-verabschiedet-sich-laut-us-portal-von-google-search-appliance
        </url>
        <id>9922D9886255BF2FBB367E4ADE965345</id>
        <title>09/02/2016 - GOOGLE verabschiedet sich laut US-Portal von Google Search Appliance</title>
        <body>GOOGLE verabschiedet sich laut US-Portal von Google Search Appliance München, 09.02.2016 – Fortune.com
            veröffentlichte kürzlich, dass Google den Verkauf und den Support des Produktes Google Search Appliance
            (GSA) bis 2018 einstellen wird. Das Portal beruft sich dabei auf verschiedene Quellen von Google-Partnern.
            Google selbst äußerte sich bisher nicht zu dieser Information. Das seit 2002 auf dem Markt befindliche und
            vom Analystenhaus Gartner als Challenger eingestufte Produkt wird aktuell auch von zahlreichen deutschen
            Unternehmen eingesetzt. Kunden und Partner der GSA - Zeit für Veränderung IntraFind, einer der führenden
            europäischen Softwarehersteller und seit dem Jahr 2000 mit dem Enterprise Search Produkt iFinder auf dem
            Markt, bietet für Interessenten, Kunden und Partner der Google Search Appliance eine interessante und
            nahtlos integrierbare Softwarelösung sowie kompetente Beratung für mögliche Migrations- oder Ablöseprojekte.
            Lernen Sie unser Produkt iFinder5 elastic kennen. Sie nutzen in Ihrem Unternehmen derzeit die Google Search
            Appliance (GSA) und haben Interesse an einer Alternative? Kontaktieren Sie uns. Sie sind Google Partner und
            auf der Suche nach einer neuen Enterprise Search-Lösung für Ihr Portfolio? Wir freuen uns auf Ihre
            Nachricht. Über die IntraFind Software AG IntraFind entwickelt Produkte und Lösungen für das effiziente
            Suchen, Finden, Analysieren von Informationen unter Berücksichtigung aller Datenquellen eines Unternehmens.
            Volltextsuche und die komplette Bandbreite an Textanalyseverfahren bilden die Grundlage für optimale
            Rechercheergebnisse. Das IntraFind-Lösungsspektrum reicht von der Suche in einer Applikation, Enterprise
            Search und Metadatenmanagement bis hin zu spezialisierten, suchbasierten Anwendungen und
            Textanalyselösungen. IntraFind betreut seine Kunden umfassend – beginnend mit einer Bedarfsanalyse bietet
            IntraFind Beratung, Konzeption und Umsetzung sowie Unterstützung im laufenden Betrieb an. Die IntraFind
            Software AG wurde im Jahr 2000 gegründet und hat ihren Firmensitz in München. Namhafte Kunden von IntraFind
            sind: AUDI AG, Robert Bosch GmbH, MAN Truck &amp; Bus AG, Voith GmbH, ZEIT ONLINE GmbH. Für weitere
            Informationen wenden Sie sich bitte an IntraFind Software AG Sonja Bellaire Landsberger Straße 368 D-80687
            München Email: sonja.bellaire@intrafind.de Internet: http://www.intrafind.de
        </body>
    </Document>
    <Document>
        <url>http://www.intrafind.de/unternehmen/news/bmw-group-entscheidet-sich-fuer-intrafind</url>
        <id>90D00B0DE280C037F0037F358EE6D885</id>
        <title>08/03/2017 -BMW Group entscheidet sich für IntraFind</title>
        <body>BMW Group entscheidet sich für IntraFind München, den 08.03.2017 – Nach einem erfolgreichen Proof of
            Concept hat sich die BMW Group für den iFinder5 elastic als unternehmensweites Suchprodukt entschieden. Der
            iFinder wird die bestehende Suche ablösen und den Mitarbeitern einen universellen Zugriff auf Millionen von
            Dokumenten ermöglichen.  
        </body>
    </Document>
    <Document>
        <url>http://www.intrafind.de/unternehmen/news/ifinder5-elastic-weitaus-mehr-als-suchen-und-finden</url>
        <id>534AEBDFFA79C1897EAF9D249BF80880</id>
        <title>22/01/2016 - iFinder5 elastic – weitaus mehr als Suchen und Finden</title>
        <body>iFinder5 elastic – weitaus mehr als Suchen und Finden IntraFind startet Technologie-Roadshow durch
            Deutschland für Kunden, Interessenten und Partner. München, 22.01.2016 – IntraFind veranstaltet aktuell eine
            deutschlandweite Roadshow und präsentiert Kunden, Interessenten und Partnern die neue Produktgeneration
            iFinder5 elastic. Den Auftakt der Deutschlandtour bildete ein halbtägiges Business Brunch am 21. Januar in
            Hamburg. Die Teilnehmer begeisterten sich vor allem für die vielfältigen Einsatzszenarien der Suchlösung.
            Anhand praxisnaher Kurzvorträge und einer Live Demo des iFinder5 elastic erläuterten IntraFind Vorstand
            Franz Kögl, Partner Manager Manuel Brunner und Produkt Manager Ralf Klinkhammer wie Unternehmen durch den
            Einsatz einer intelligenten Suche die Produktivität ihrer Mitarbeiter steigern können. Besonderes Augenmerk
            galt auch den unstrukturierten Daten im Unternehmen, deren Erschließung und Analyse IntraFind durch
            leistungsfähige Content Analytics-Werkzeuge unterstützt. Große Datenbestände, die wertvolle Informationen
            beinhalten, werden dadurch für Mitarbeiter zugänglich, die Datenqualität verbessert und der „Blinde Fleck“
            in unstrukturierten Daten minimiert. Die Vorstellung verschiedenster Anwendungsbeispiele für Suche im
            Unternehmen verdeutlichte den Teilnehmern die zentrale Botschaft der drei IntraFind Experten: „Suche ist
            weitaus mehr als der Funktionsumfang einer klassischen Volltextsuche oder Suchen und Finden.“ Den Abschluss
            der Veranstaltung bildeten Praxistipps zur sinnvollen Planung und Realisierung von Enterprise
            Search-Projekten sowie ein gemeinsames Mittagessen aller Teilnehmer. IntraFind setzt seine Roadshow im
            Februar mit Veranstaltungen in Frankfurt (25.02.) und Düsseldorf (26.02.) fort, die Teilnahme ist
            kostenfrei. Lernen Sie unser Produkt iFinder5 elastic kennen. Vom iFinder5 elastic profitieren einzelne
            Anwender bis hin zu ganzen Teams - erfahren Sie mehr. Registrieren Sie sich hier zum Business Brunch in
            Frankfurt (25.02.2016) und Düsseldorf (26.02.2016). Über die IntraFind Software AG IntraFind entwickelt
            Produkte und Lösungen für das effiziente Suchen, Finden, Analysieren von Informationen unter
            Berücksichtigung aller Datenquellen eines Unternehmens. Volltextsuche und die komplette Bandbreite an
            Textanalyseverfahren bilden die Grundlage für optimale Rechercheergebnisse. Das IntraFind-Lösungsspektrum
            reicht von der Suche in einer Applikation, Enterprise Search und Metadatenmanagement bis hin zu
            spezialisierten, suchbasierten Anwendungen und Textanalyselösungen. IntraFind betreut seine Kunden umfassend
            – beginnend mit einer Bedarfsanalyse bietet IntraFind Beratung, Konzeption und Umsetzung sowie Unterstützung
            im laufenden Betrieb an. Die IntraFind Software AG wurde im Jahr 2000 gegründet und hat ihren Firmensitz in
            München. Namhafte Kunden von IntraFind sind: AUDI AG, Robert Bosch GmbH, MAN Truck &amp; Bus AG, Voith GmbH,
            ZEIT ONLINE GmbH. Für weitere Informationen wenden Sie sich bitte an IntraFind Software AG Sonja Bellaire
            Landsberger Straße 368 D-80687 München Email: sonja.bellaire@intrafind.de Internet: http://www.intrafind.de
             
        </body>
    </Document>
    <Document>
        <url>http://www.intrafind.de/unternehmen/news/scherdel-nutzt-ifinder5-elastic-fuer-den-digitalen-arbeitsplatz
        </url>
        <id>21FED68026C467959723A57049BF44C4</id>
        <title>21/07/2017 - Scherdel nutzt iFinder5 elastic für den Digitalen Arbeitsplatz</title>
        <body>Scherdel nutzt iFinder5 elastic für den Digitalen Arbeitsplatz München, den 21. Juli 2017 – Die
            Martkredwitzer Scherdel Gruppe, ein international agierender Hersteller und Lieferant von Metallprodukten
            und Lösungen für den Anlagenbau verschiedenster Branchen, setzt für den neuen „Digitalen Arbeitsplatz“ auf
            Suchtechnologien der IntraFind.  Im ersten Schritt sollen Mitarbeiter die für sie wichtigen Reports und
            Berichte innerhalb eines Portals noch schneller und effizienter finden. Für die Suche setzt Scherdel den
            iFinder5 elastic ein. Mittelfristig werden weitere Anwendungsbereiche erschlossen. Die Suchfunktionen werden
            hier von einem weiteren Partner der Scherdel Gruppe voll in das neu entstehende Portal integriert. Vor allem
            die zahlreichen Linguistik-Funktionen im iFinder5 elastic waren ausschlaggebend dafür, dass sich Scherdel
            für eine Suchlösung von IntraFind entschieden hat. Dank intelligenter Autokorrektur und Kompositazerlegung
            muss z.B. der Mitarbeiter aus dem Controlling den exakten Namen des jeweiligen Reports nicht zwingend
            kennen. Der entsprechende Begriff wird automatisch erkannt und vorgeschlagen. Der iFinder5 elastic ist somit
            für die Scherdel Gruppe einen wesentliche Komponente des neuen Digitalen Arbeitsplatzes.   Über IntraFind
            Software AG IntraFind entwickelt Produkte und Lösungen für das effiziente Suchen, Finden, Analysieren von
            strukturierten und unstrukturierten Informationen unter Berücksichtigung aller verfügbaren Datenquellen
            eines Unternehmens. Volltextsuche und die komplette Bandbreite an Textanalyse- und Machine
            Learning-Verfahren, Natural Language Processing, kombiniert mit den Möglichkeiten von Graphdatenbanken für
            Big Data Analytics, bilden hierbei den Schwerpunkt. Namhafte Kunden sind: AUDI AG, BMW AG, Bundeswehr, IHK
            Berlin, Robert Bosch GmbH und Rohde &amp; Schwarz GmbH &amp; Co. KG. Mehr Informationen: www.intrafind.de.
            Für weitere Informationen wenden Sie sich bitte an   IntraFind Software AG Christiane Stagge Landsberger
            Straße 368 D-80687 München Email: christiane.stagge@intrafind.de Internet: http://www.intrafind.de  
        </body>
    </Document>
    <Document>
        <url>
            http://www.intrafind.de/news/webcast-intrafind-und-intelligent-views-informieren-ueber-wissensmanagement-projekte
        </url>
        <id>734B85BBF049AC2ED1DE19D2C3772A0F</id>
        <title>14/04/2015 - Webcast: IntraFind und intelligent views informieren über Wissensmanagement-Projekte</title>
        <body>Webcast: IntraFind und intelligent views informieren über Wissensmanagement-Projekte   München, 14.04.2015
            – IntraFind, führender Anbieter von Lösungen für das effiziente Suchen, Finden und Analysieren von
            Informationen aus verschiedensten Datenquellen, veranstaltet am 28. April 2015 um 10 Uhr einen gemeinsamen
            Webcast mit seinem Partner intelligent views GmbH aus Darmstadt.   Im Webcast erläutern die Moderatoren
            Peter Dommermuth, Senior Sales Manager, intelligent views, und Jörg Issel, Principal Solution Manger,
            IntraFind, mit welchen Herausforderungen Unternehmen bei der Planung und Umsetzung von
            Wissensmanagement-Projekten konfrontiert sind und zeigen anschließend Lösungsansätze aus dem Projektalltag
            auf.   Im Fokus des Webcasts stehen folgende Aspekte und Fragestellungen:   Knowledge Management-Projekt /
            Enterprise Search-Lösung / Intranetsuche / Wissensportal - es gibt zahlreiche Bezeichnungen dafür ... Kennen
            Sie Ihre Wissensmanagement-Projekte im Unternehmen? Unterschiedliche Motive, aber ein gemeinsames Ziel -
            welche Stakeholder haben Einfluss auf ein Wissensmanagement-Projekt? Zusammenführung zweier Welten:
            Kombination von Enterprise Search und semantischer Graphdatenbank zur Erschließung unstrukturierter und
            strukturierter Informationen Praxisbeispiel: Präventionsforum , die internationale Wissensplattform
            europäischer Unfallversicherungen Im Anschluss besteht für die Teilnehmer die Möglichkeit, Fragen zu
            stellen. Die Teilnahme an der Veranstaltung ist kostenfrei.   In einem Folgewebcast am Donnerstag, 11. Juni
            2015, widmen sich Peter Dommermuth und Jörg Issel dem Thema &quot;Wie Wissensmanagement-Projekte in der
            Industrie wirklich funktionieren&quot; und stellen Best Practice-Beispiele aus aktuellen Kundenprojekten
            vor.   Mehr Informationen zum Serviceportfolio der IntraFind Software AG im Bereich Projektmanagement
            Kombination aus Suche und Semantik im Kundenprojekt Präventionsforum :
            https://www.praeventionsforum-plus.info/ Wissensmanagement durch Enterprise Search in Produktdaten,
            Terminologie, Fachliteratur und Intranetseiten: Kundenreferenz MTU Aero Engines lesen Zum Folgewebcast
            &quot;Wissensmanagement-Projekte: Best Practice-Beispiele&quot; am 11. Juni 2015, 10 Uhr, anmelden.   Über
            die IntraFind Software AG IntraFind entwickelt Produkte und Lösungen für das effiziente Suchen, Finden,
            Analysieren von Informationen unter Berücksichtigung aller Datenquellen eines Unternehmens. Volltextsuche
            und die komplette Bandbreite an Textanalyseverfahren bilden die Grundlage für optimale Rechercheergebnisse.
            Das IntraFind-Lösungsspektrum reicht von der Suche in einer Applikation, Enterprise Search und
            Metadatenmanagement bis hin zu spezialisierten, suchbasierten Anwendungen und Textanalyselösungen. IntraFind
            betreut seine Kunden umfassend – beginnend mit einer Bedarfsanalyse bietet IntraFind Beratung, Konzeption
            und Umsetzung sowie Unterstützung im laufenden Betrieb an. Die IntraFind Software AG wurde im Jahr 2000
            gegründet und hat ihren Firmensitz in München. Namhafte Kunden von IntraFind sind: AUDI AG, Robert Bosch
            GmbH, ZEIT ONLINE GmbH.   Für weitere Informationen wenden Sie sich bitte an   IntraFind Software AG Sonja
            Bellaire Landsberger Straße 368 D-80687 München Email: sonja.bellaire@intrafind.de Internet:
            http://www.intrafind.de  
        </body>
    </Document>
    <Document>
        <url>
            http://www.intrafind.de/unternehmen/news/deutsche-rentenversicherung-nutzt-ifinder5-elastic-fuer-unternehmensweite-suche
        </url>
        <id>9EBDEDFC79240E7A88585D838F37C903</id>
        <title>20/01/2017 - Deutsche Rentenversicherung nutzt iFinder5 elastic für unternehmensweite Suche</title>
        <body>Deutsche Rentenversicherung nutzt iFinder5 elastic für unternehmensweite Suche München, den 20.01.2017 –
            Die Deutsche Rentenversicherung setzt für ihre unternehmensweite Suche ab sofort den iFinder5 elastic ein.
            Ob E-Mails, Tarifverträge, Adressen oder Urteile von Sozialgerichten -  bis zu 24.000 Anwender sollen mit
            der Enterprise Search Lösung noch besser die Dokumente finden, die sie für ihre tägliche Arbeit brauchen,
            beispielsweise um rechtliche Sachverhalte zu prüfen und Informationen zu recherchieren.   Die NOW IT GmbH
            stellt dabei als IT-Dienstleister der Deutschen Rentenversicherung die erforderliche Hardware und Software
            zur Verfügung und wird die Enterprise Search Lösung zusammen mit der IntraFind Software AG auf sechs
            Mandanten ausrollen. Dazu gehören die Deutsche Rentenversicherung, Deutsche Rentenversicherung Nord,
            Deutsche Rentenversicherung Braunschweig-Hannover, Deutsche Rentenversicherung Mitteldeutschland, Deutsche
            Rentenversicherung Rheinland und Deutsche Rentenversicherung Westfalen. Die mandantenspezifische und
            rechtegeprüfte Suche ist für die Deutsche Rentenversicherung ganz entscheidend, da Mitarbeiter hier täglich
            mit sensiblen Daten arbeiten. Mandantenfähig bedeutet, dass auf demselben Server beziehungsweise
            Softwaresystem mehrere Kunden arbeiten können, ohne dass sie Einblick in die jeweiligen anderen Ordner und
            Daten haben. So werden bei der Suche in der Trefferliste nur die Dokumente angezeigt, auf die die
            Mitarbeiter berechtigt sind, zuzugreifen. Jeder Mitarbeiter bekommt nur seine personalisierten
            Suchergebnisse angezeigt und kann nur seine Daten sehen und ändern. Der iFinder5 elastic unterstützt
            außerdem weitere Features wie Autocomplete mit Tippfehlerkorrektur und die Mehrwortsuche. Sucht der Nutzer
            beispielsweise nach „Gesetz“, findet er auch Dokumente, in denen das Wort „Sozialversicherungsgesetz“
            vorkommt. Da Mitarbeiter bei der Deutschen Rentenversicherung besonders häufig nach Gesetztexten suchen
            müssen, ist der iFinder5 elastic ebenfalls ideal, da er auch Zahlen, Paragrafen und das Auffinden von
            Rechtsnormen unterstützt. Mit seinen über hundert Konnektoren kann der iFinder5 elastic außerdem an
            verschiedene Quellen angebunden werden und ist dadurch in der Lage, das Web, Intranet, Filesystem, E-Mails
            wie Lotus Domino, Confluence und z.B. IBM Connections zu durchsuchen.      Über IntraFind Software AG
            IntraFind entwickelt Produkte und Lösungen für das effiziente Suchen, Finden, Analysieren von strukturierten
            und unstrukturierten Informationen unter Berücksichtigung aller verfügbaren Datenquellen eines Unternehmens.
            Volltextsuche und die komplette Bandbreite an Textanalyse- und Machine Learning-Verfahren, Natural Language
            Processing, kombiniert mit den Möglichkeiten von Graphdatenbanken für Big Data Analytics, bilden hierbei den
            Schwerpunkt. Namhafte Kunden sind: AUDI AG, Bundeswehr, IHK Berlin, Robert Bosch GmbH und Rohde &amp;
            Schwarz GmbH &amp; Co. KG. Mehr Informationen: www.intrafind.de. Für weitere Informationen wenden Sie sich
            bitte an IntraFind Software AG Christiane Stagge Landsberger Straße 368 D-80687 München Email:
            christiane.stagge@intrafind.de Internet: http://www.intrafind.de
        </body>
    </Document>
    <Document>
        <url>http://www.intrafind.de/unternehmen/referenzen</url>
        <id>8D4C571012D4361DA9E3A7D8888626D9</id>
        <title>Kundenreferenzen</title>
        <body>ERFOLGREICHE UNTERNEHMEN SETZEN BEREITS AUF LÖSUNGEN VON INTRAFIND. IntraFind zählt seit dem Jahr 2000
            mehr als 1.000 Kunden aller Branchen und Unternehmensgrößen. Unsere Technologien unterstützen namhafte
            DAX-Unternehmen, erfolgreiche Hidden Champions ebenso wie innovative Unternehmen aus dem Mittelstand.
            Überzeugen Sie sich selbst. Branche     Automobilindustrie     Fertigungsindustrie     Banken &amp;
            Versicherung     Chemie &amp; Pharma     Dienstleistung     Energiewirtschaft     Human Resources
                Lebensmittelhersteller     Logistik &amp; Transport     Medien &amp; Verlage     Öffentliche
            Auftraggeber Filter zurücksetzen AUDI setzt auf IntraFind im PartnerNet Die AUDI AG löste mit der
            Integration des iFinder die bereits existierende Suche im Händlerportal AUDI PartnerNet ab. Unabhängiges
            Marktforschungsinstitut bescheinigte hohe Zufriedenheit unter den Händlern mit dem Portal und nannte
            explizit die Suchfunktionalitäten als herausragend. &quot;IntraFind steht für technologisch hochwertige
            Produkte, Zuverlässigkeit und Kundennähe.&quot;   Thomas Müller, AUDI AG Referenz ansehen Diakonie
            Deutschland nutzt iFinder5 elastic für die unternehmensweite Suche in NetApp Fileservices Das Evangelische
            Werk für Diakonie und Entwicklung (EWDE) nutzt NetApp Fileservices für die Speicherung von Dokumenten und
            Informationen. Mit Hilfe der über den fpolicy-Service angebundenen und von NetApp zertifizierten IntraFind
            Suchlösung iFinder5 elastic können diese Filesysteme nun in Echtzeit durchsucht werden. „Besonders die
            Linguistik-Funktionen im iFinder5 elastic haben uns begeistert. Bei der Suche bekommen wir alle relevanten
            Ergebnisse angezeigt und übersehen nichts mehr.“ Danijel Slankovic Teamleiter IT Betrieb, Evangelisches Werk
            für Diakonie und Entwicklung (EWDE) Referenz ansehen DEUTSCHE POST führt workflowbasiertes Ideenmanagement
            ein Die Deutsche Post implementiert mit IntraFind ein effizientes Ideenmanagement. Integrierte Workflows
            steuern schon kurz nach Versand der Idee die Zuordnung zum richtigen Experten. Diese Vorgehensweise verkürzt
            massiv die Reaktionszeiten und verbessert die Zusammenarbeit. „Die Lösungen und Experten von IntraFind
            konnten unsere Anforderungen aus dem Stand heraus abdecken. Bereits im Proof of Concept wurde das Potential
            der Suchtechnologie für die Prozessoptimierung deutlich.“   Günter Raffel, Abteilungsleiter des
            Konzernideenmanagements der Deutschen Post Referenz ansehen IBC SOLAR findet nun unternehmensweit wichtige
            und relevante Information IBC Solar integriert die IntraFind Suchlösung als applikationsübergreifende Suche
            und erzielt eine deutliche Effizienzsteigerung in Recherchen. Mitarbeiter aller Standorte haben über die
            Suche Zugriff auf Dateisysteme, ECM-Systeme sowie auf Microsoft SharePoint. „IBC.search auf Basis des
            IntraFind iFinder liefert schnell und zuverlässig qualitativ hochwertige Treffer – ein unverzichtbares
            Recherche-Tool für unsere Mitarbeiter.“   Michael Seifert, Projektleiter IBC.search, IBC SOLAR AG Referenz
            ansehen Landesbank Berlin setzt auf IntraFind Durch die Integration des iFinder in das bestehende Intranet
            profitieren vor allem Mitarbeiter von der intuitiv zu bedienenden Suchoberfläche und den umfassenden
            Recherchemöglichkeiten. „IntraFind konnte uns vor allem durch die intuitive und einfache Bedienbarkeit des
            iFinder, die hohe Qualität der Wortanalyse und das beste Preis-Leistungs-Verhältnis aller
            getesteten Produkte überzeugen.“   Dieter Morgenthal Projektleiter „KonzernWeb-Suche“ Landesbank Berlin AG
            Referenz ansehen MTU AERO ENGINES integriert Wissensmanagement mit IntraFind MTU realisiert mit IntraFind
            und der Integration des iFinder eine Wissensmanagementplattform. Angebunden werden zahlreiche Datenquellen
            (PLM, Fachliteratur, Intranet auf Basis von TYPO3, Glossar) unter Berücksichtigung komplexer
            Rechtestrukturen. „Die neue Suchlösung liefert schnell, unkompliziert und zuverlässig die gewünschten
            Treffer – das Ergebnis: wertvolle Zeitersparnis und zufriedene Anwender dank IntraFind.“   Uwe Urra, MTU
            Aero Engines AG Referenz ansehen WOLTERS KLUWER integriert Suche in www.jurion.de Anwender des juristischen
            Wissensportals von Wolters Kluwer www.jurion.de suchen mit IntraFind. Umfassende Recherchemöglichkeiten,
            einfache Usability, die Darstellung von vernetzten Wissen sorgen für hohe Anwenderzufriedenheit. „Mit Hilfe
            von IntraFind konnten wir unsere Vision verwirklichen, einen neuen Weg juristischen Arbeitens zu schaffen.“
              Ralph Vonderstein, CIO, Wolters Kluwer Referenz ansehen ZF Friedrichshafen integriert IntraFind Suche in
            PDM-System ZF Friedrichshafen löste mit der Integration des iFinder die Komplexität der Suche im bestehenden
            PDM-System auf. Um den Mitarbeitern zusätzliche Informationen über die Suche zur Verfügung zu stellen,
            wurden weitere Datenquellen an die Suchlösung angebunden.   „Das Suchen mit PDM-Search ist eine wahre
            Freude! Genau solch eine Funktion hat uns noch gefehlt.&quot;    Dr. Georg Häberle, ZF Informatik Leiter des
            Funktionscenters PLM ZF Friedrichshafen AG Details anfragen AVITEA integriert intelligente Bewerbersuche in
            bestehendes HR-System avitea profitiert von der Integration der intelligenten Bewerbersuche von IntraFind.
            Lebensläufe und Jobprofile werden automatisch nach Kenntnissen und Fähigkeiten gescannt und mit den
            Anforderungen gematcht. „Mit der neuen Bewerbersuche ist es avitea möglich, innerhalb kürzester Zeit auf
            Kundenanfragen mit einer Liste optimaler Bewerberprofile zu reagieren.&quot;   Iris Utzel IT
            –Systemadministratorin avitea GmbH  Details anfragen IHK Berlin realisiert IntraFind Suche im
            Mitarbeiterportal Die IHK Berlin entschied sich die IntraFind Suchlösung iFinder5 elastic und integrierte
            diese in das neue Mitarbeiterportal. Die Suche sollte dabei rechtegeprüft sein, so dass jeder Mitarbeiter
            nur die für ihn freigegebenen Dokumente erhält. &quot;Schnelle Integration des iFinder in das Liferay Portal
            ermöglichte zügige Projektabwicklung und termingerechten Launch des Mitarbeiterportals.&quot;   IHK Berlin
            Details anfragen B. Braun setzt für Produktdatensuche auf IntraFind B.Braun integriert die iFinder
            Suchlösung in ihre Produktdatensuche. Selbst komplexe Begrifflichkeiten werden mittels intelligenter
            IntraFind Technologien und Verfahren indexiert und für die Suche bereitgestellt.  &quot;B. Braun verbesserte
            mit dem iFinder die applikationsbasierte Suche enorm und nutzt die Software um eigene Search based
            Applications zu entwickeln.&quot;   B.Braun Melsungen Details anfragen MAN konsolidiert mit IntraFind
            Informationen für ein Händlerportal Speziell für Händler entwickelte MAN ein umfangreiches After Sales
            Portal. Die in dieses Portal integrierte IntraFind Suchlösung iFinder konsolidiert aus verschiedenen
            Datenbanken und von Filesystemen Informationen und stellt diese den Informationssuchenden bereit.  &quot;MAN
            entschied sich gegen eine aufwendige  und kostenintensive Datenbankmigration und für IntraFind als
            einheitlichen, anwenderfreundlichen und performanten Zugang über zahlreiche Informationsquellen. Die
            Akzeptanz und Händlerzufriedenheit ist sehr groß.&quot;   MAN Details anfragen Robert Bosch GmbH realisiert
            Wissensmanagementlösung mit IntraFind Die Anbindung zahlreicher Datenquellen wie z.B. Datei-, ECM-Systeme
            und das Intranet ermöglichen den Mitarbeitern des Fachbereiches Automotive einen schnellen Zugang zu
            wichtigen Produktinformationen. &quot;Die Robert Bosch GmbH erreichte mit der neuen Suche und dem
            performanten Zugang zu Informationen eine hohe Kundenzufriedenheit.&quot;   Robert Bosch GmbH Details
            anfragen Fonds Finanz setzt bei Sitesearch auf iFinder5 elastic Fonds Finanz hat für die Suche auf der
            Webseite den iFinder5 elastic installiert. Die Suche unterstützt die Autocomplete-Funktion und toleriert
            Fehler - wie Buchstabendreher und Buchstabendoppler - innerhalb eines Wortes und schlägt sogar Synonyme vor,
            die in demselben Kontext stehen. „Im Prozess der internen Implementierung haben die Programmierer sowohl das
            Produkt als auch die Betreuung durch die IntraFind Software AG zu schätzen gelernt. Nach einer kurzen
            Umsetzungszeit konnte das Produkt gelauncht werden, welches den Usern nun ein echtes Sucherlebnis
            verschafft.&quot; Reimund Schneider Teamleiter Bildung &amp; Projekte in der Abteilung Marketing &amp;
            Kommunikation Fonds Finanz GmbH Details anfragen Harry-Brot setzt auf IntraFind Die Hamburger
            Traditionsbäckerei Harry-Brot GmbH entschied sich nach einer intensiven Evaluation für den Einsatz des
            Enterprise Search Produkts iFinder der IntraFind Software AG. Die überzeugenden Kriterien waren das optimale
            Preis-Leistungs-Verhältnis und die intuitive Bedienbarkeit der Software sowie die hohe Qualität der
            Suchergebnisse.   Details anfragen
        </body>
    </Document>
    <Document>
        <url>http://www.intrafind.de/kontaktformular-google-kunden</url>
        <id>8B5A8C4E242D99F86525EFC114080D26</id>
        <title>IntraFind Kontaktformular für Google Kunden</title>
        <body>IntraFind Kontaktformular für Google Kunden Sie nutzen in Ihrem Unternehmen derzeit die Google Search
            Appliance (GSA) und haben Interesse an einer Alternative? Kontaktieren Sie uns.
        </body>
    </Document>
</documents>